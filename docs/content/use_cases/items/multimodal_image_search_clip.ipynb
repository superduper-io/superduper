{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "238520e0",
   "metadata": {},
   "source": [
    "# Multimodal search with CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3590f0e",
   "metadata": {},
   "source": [
    "In this notebook we show-case SuperDuperDB's functionality for searching with multiple types of data over\n",
    "the same `VectorIndex`. This comes out very naturally, due to the fact that SuperDuperDB allows\n",
    "users and developers to add arbitrary models to SuperDuperDB, and (assuming they output vectors) use\n",
    "these models at search/ inference time, to vectorize diverse queries.\n",
    "\n",
    "To this end, we'll be using the [CLIP multimodal architecture](https://openai.com/research/clip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebe1497",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/openai/CLIP\n",
    "!pip install datasets\n",
    "!pip install superduperdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f94ae8",
   "metadata": {},
   "source": [
    "So let's start. \n",
    "\n",
    "SuperDuperDB supports MongoDB as a databackend. Correspondingly, we'll import the python MongoDB client `pymongo`\n",
    "and \"wrap\" our database to convert it to a SuperDuper `Datalayer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5ef986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from superduperdb import CFG\n",
    "from superduperdb.db.base.build import build_datalayer\n",
    "from superduperdb.db.mongodb.query import Collection\n",
    "\n",
    "# Uncomment one of the following lines to use a bespoke MongoDB deployment\n",
    "# For testing the default connection is to mongomock\n",
    "\n",
    "mongodb_uri = os.getenv(\"MONGODB_URI\", \"mongomock://test\")\n",
    "# mongodb_uri = \"mongodb://localhost:27017/documents\"\n",
    "# mongodb_uri = \"mongodb+srv://<username>:<password>@<atlas_cluster>/<database>\"\n",
    "\n",
    "# Super-Duper your Database!\n",
    "CFG.data_backend = mongodb_uri\n",
    "CFG.artifact_store = 'filesystem://./models'\n",
    "CFG.vector_search = mongodb_uri\n",
    "\n",
    "db = build_datalayer(CFG)\n",
    "\n",
    "collection = Collection(name='tiny-imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd6d6b0",
   "metadata": {},
   "source": [
    "In order to make this notebook easy to execute an play with, we'll use a sub-sample of the [Tiny-Imagenet\n",
    "dataset](https://paperswithcode.com/dataset/tiny-imagenet). \n",
    "\n",
    "Everything we are doing here generalizes to much larger datasets, with higher resolution images, without\n",
    "further ado. For such use-cases, however, it's advisable to use a machine with a GPU, otherwise they'll \n",
    "be some significant thumb twiddling to do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29386a5",
   "metadata": {},
   "source": [
    "To get the images into the database, we use the `Encoder`-`Document` framework. This allows\n",
    "us to save Python class instances as blobs in the `Datalayer`, but retrieve them as Python objects.\n",
    "This makes it far easier to integrate Python AI-models with the datalayer.\n",
    "\n",
    "To this end, SuperDuperDB contains pre-configured support for `PIL.Image` instances. It's also \n",
    "possible to create your own encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa0a06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduperdb.container.document import Document as D\n",
    "from superduperdb.ext.pillow.image import pil_image as i\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "dataset = load_dataset(\"zh-plus/tiny-imagenet\")['valid']\n",
    "dataset = [D({'image': i(r['image'])}) for r in dataset]\n",
    "dataset = random.sample(dataset, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c7e282",
   "metadata": {},
   "source": [
    "The wrapped python dictionaries may be inserted directly to the `Datalayer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32a91a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.execute(collection.insert_many(dataset, encoders=(i,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d37264",
   "metadata": {},
   "source": [
    "We can verify that the images are correctly stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7282a0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = db.execute(collection.find_one()).unpack()['image']\n",
    "display(x.resize((300, 300 * int(x.size[1] / x.size[0]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab27b50",
   "metadata": {},
   "source": [
    "We now can wrap the CLIP model, to ready it for multimodel search. It involves 2 components:\n",
    "\n",
    "- text-encoding\n",
    "- visual-encoding\n",
    "\n",
    "Once we have installed both parts, we will be able to search with both images and text for \n",
    "matching items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916792d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "from superduperdb.ext.vector.encoder import vector\n",
    "from superduperdb.ext.torch.model import TorchModel\n",
    "import torch\n",
    "\n",
    "model, preprocess = clip.load(\"RN50\", device='cpu')\n",
    "\n",
    "e = vector(shape=(1024,))\n",
    "\n",
    "text_model = TorchModel(\n",
    "    identifier='clip_text',\n",
    "    object=model,\n",
    "    preprocess=lambda x: clip.tokenize(x)[0],\n",
    "    forward_method='encode_text',\n",
    "    postprocess=lambda x: x.tolist(),\n",
    "    encoder=e,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f82a69-caa3-4783-a015-2034540edc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model.predict('This is a test', one=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbb08cc",
   "metadata": {},
   "source": [
    "Similar procedure with the visual part, which takes `PIL.Image` instances as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0f0cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_model = TorchModel(\n",
    "    identifier='clip_image',\n",
    "    preprocess=preprocess,\n",
    "    object=model.visual,\n",
    "    postprocess=lambda x: x.tolist(),\n",
    "    encoder=e,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3a7685",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_model.predict(x, one=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b716bcb2",
   "metadata": {},
   "source": [
    "Now let's create the index for searching by vector. We register both models with the index simultaneously,\n",
    "but specifying that it's the `visual_model` which will be responsible for creating the vectors in the database\n",
    "(`indexing_listener`). The `compatible_listener` specifies how one can use an alternative model to search \n",
    "the vectors. By using models which expect different types of index, we can implement multimodal search\n",
    "without further ado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e0302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduperdb.container.vector_index import VectorIndex\n",
    "from superduperdb.container.listener import Listener\n",
    "\n",
    "db.add(\n",
    "    VectorIndex(\n",
    "        'my-index',\n",
    "        indexing_listener=Listener(\n",
    "            model=visual_model,\n",
    "            key='image',\n",
    "            select=collection.find(),\n",
    "        ),\n",
    "        compatible_listener=Listener(\n",
    "            model=text_model,\n",
    "            key='text',\n",
    "            active=False,\n",
    "            select=None,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18971a6d",
   "metadata": {},
   "source": [
    "We can now demonstrate searching by text for images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab994b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "from IPython.display import display\n",
    "from superduperdb.container.document import Document as D\n",
    "\n",
    "out = db.execute(\n",
    "    collection.like(D({'text': 'mushroom'}), vector_index='my-index', n=3).find({})\n",
    ")\n",
    "for r in out:\n",
    "    x = r['image'].x\n",
    "    display(x.resize((300, 300 * int(x.size[1] / x.size[0]))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
