{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38c1a328-fd86-4c5f-bd54-b8664f433608",
   "metadata": {},
   "source": [
    "# Multimodal vector search - images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f8484d-2e35-472a-9b24-1a30ec1d144b",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "## Connect to superduper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d66021-ce62-4021-a2c5-158dee92b3bb",
   "metadata": {},
   "source": [
    ":::note\n",
    "Note that this is only relevant if you are running superduper in development mode.\n",
    "Otherwise refer to \"Configuring your production system\".\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb029a5e-fedf-4f07-8a31-d220cfbfbb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper import superduper\n",
    "\n",
    "db = superduper('mongomock:///test_db')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032c2e7b-3f54-4263-b778-0fef60596efb",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "## Get useful sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0828031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://superduperdb-public-demo.s3.amazonaws.com/images.zip && unzip images.zip\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "data = [f'images/{x}' for x in os.listdir('./images') if x.endswith(\".png\")][:200]\n",
    "data = [ Image.open(path) for path in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44a702b1-faf9-4edb-8a55-efc4add84a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{'img': d} for d in data[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296b245a-6254-4219-a8e9-4a6360cff5c3",
   "metadata": {},
   "source": [
    "## Build multimodal embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7b2646-8693-41dc-98e9-3c15b78ee68e",
   "metadata": {},
   "source": [
    "We define the output data type of a model as a vector for vector transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5375ff04-8b6d-40b0-a8b2-6aa124636871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: MongoDB>\n",
    "from superduper.components.vector_index import vector\n",
    "output_datatpye = vector(shape=(1024,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "489a19c2-c673-4bf9-9bea-f4bb279fc462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: SQL>\n",
    "from superduper.components.vector_index import sqlvector\n",
    "output_datatpye = sqlvector(shape=(1024,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40772155-8a01-4577-a9c9-7928fcd012f6",
   "metadata": {},
   "source": [
    "Then define two models, one for text embedding and one for image embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8751ede0-4b9f-4f92-b4ec-f6b0e0740c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install ../../plugins/torch\n",
    "import clip\n",
    "from superduper import vector\n",
    "from superduper_torch import TorchModel\n",
    "\n",
    "# Load the CLIP model and obtain the preprocessing function\n",
    "model, preprocess = clip.load(\"RN50\", device='cpu')\n",
    "\n",
    "# Create a TorchModel for text encoding\n",
    "compatible_model = TorchModel(\n",
    "    identifier='clip_text', # Unique identifier for the model\n",
    "    object=model, # CLIP model\n",
    "    preprocess=lambda x: clip.tokenize(x)[0],  # Model input preprocessing using CLIP \n",
    "    postprocess=lambda x: x.tolist(), # Convert the model output to a list\n",
    "    datatype=output_datatpye,  # Vector encoder with shape (1024,)\n",
    "    forward_method='encode_text', # Use the 'encode_text' method for forward pass \n",
    ")\n",
    "\n",
    "# Create a TorchModel for visual encoding\n",
    "embedding_model = TorchModel(\n",
    "    identifier='clip_image',  # Unique identifier for the model\n",
    "    object=model.visual,  # Visual part of the CLIP model    \n",
    "    preprocess=preprocess, # Visual preprocessing using CLIP\n",
    "    postprocess=lambda x: x.tolist(), # Convert the output to a list \n",
    "    datatype=output_datatpye, # Vector encoder with shape (1024,)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0119da-9cfd-4a60-8847-c3bfdf37697f",
   "metadata": {},
   "source": [
    "Because we use multimodal models, we define different keys to specify which model to use for embedding calculations in the vector_index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12e75fab-8504-4d17-a7d9-f98667a5d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexing_key = 'img' # we use img key for img embedding\n",
    "compatible_key = 'text' # we use text key for text embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b8b40d-3750-4d7b-aa60-62e07b734b04",
   "metadata": {},
   "source": [
    "## Create vector-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66ee3ff4-880e-477b-bbdf-5b8d89c56de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index_name = 'my-vector-index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cede653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper import VectorIndex, Listener\n",
    "\n",
    "vector_index = VectorIndex(\n",
    "    vector_index_name,\n",
    "    indexing_listener=Listener(\n",
    "        key=indexing_key,                 # the `Document` key `model` should ingest to create embedding\n",
    "        select=db['docs'].select(),       # a `Select` query telling which data to search over\n",
    "        model=embedding_model,            # a `_Predictor` how to convert data to embeddings\n",
    "        identifier='indexing-listener',\n",
    "    ),\n",
    "    compatible_listener=Listener(\n",
    "        key=compatible_key,               # the `Document` key `model` should ingest to create embedding\n",
    "        model=compatible_model,           # a `_Predictor` how to convert data to embeddings\n",
    "        select=None,\n",
    "        identifier='compatible-listener',\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2508c32b-1db2-4b45-a1d3-8b0e352d59a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper import Application\n",
    "\n",
    "application = Application(\n",
    "    'image-vector-search',\n",
    "    components=[vector_index],\n",
    ")\n",
    "\n",
    "db.apply(application)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339180f6-9855-4c5d-ae70-78040d72d170",
   "metadata": {},
   "source": [
    "## Add the data\n",
    "\n",
    "The order in which data is added is not important. *However* if your data requires a custom `Schema` in order to work, it's easier to add the `Application` first, and the data later. The advantage of this flexibility, is that once the `Application` is installed, it's waiting for incoming data, so that the `Application` is always up-to-date. This comes in particular handy with AI scenarios which need to respond to changing news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b5bc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper import Document\n",
    "\n",
    "table_or_collection = db['docs']\n",
    "\n",
    "ids = db.execute(table_or_collection.insert([Document(r) for r in data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a87f9d-581a-419a-81b8-a743250413e9",
   "metadata": {},
   "source": [
    "## Perform a vector search\n",
    "\n",
    "We can perform the vector searches using two types of data:\n",
    "\n",
    "- Text: By text description, we can find images similar to the text description.\n",
    "- Image: By using an image, we can find images similar to the provided image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce565823-4655-488c-8684-2240107fa30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Text>\n",
    "item = Document({compatible_key: \"Find a black dog\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8059626-dff8-4fe0-b872-97b8eb8b1b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=500x338>}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# <tab: Image>\n",
    "from IPython.display import display\n",
    "search_image = data[0]\n",
    "display(search_image)\n",
    "item = Document(search_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3ba07d-1124-4d94-a117-60d2e72581f7",
   "metadata": {},
   "source": [
    "Once we have this search target, we can execute a search as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a061de0b-2694-4b36-844c-7753a465360f",
   "metadata": {},
   "outputs": [],
   "source": [
    "select = db['docs'].like(item, vector_index=vector_index_name, n=5).select()\n",
    "results = list(db.execute(select))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6d9af9-a012-42bd-aad4-31b92d089caa",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2ecea5-3a58-457c-ac50-ddc742484f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "for result in results:\n",
    "    display(result[indexing_key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1390e3b-d942-4442-9071-ef8045d96847",
   "metadata": {},
   "source": [
    "## Create a `Template`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118bc2d4-5313-4584-8607-1eacfff09660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper import Template\n",
    "\n",
    "template = Template(\n",
    "    'image-vector-search',\n",
    "    template=application,\n",
    "    substitutions={'docs': 'table'},\n",
    ")\n",
    "\n",
    "template.export('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759db69c-c3e6-47e5-af6d-e089c1e8ad4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
