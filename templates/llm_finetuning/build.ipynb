{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d92f42b-7160-4965-a0ef-5e4ce46bd529",
   "metadata": {},
   "source": [
    "# Fine tune LLM on database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f8484d-2e35-472a-9b24-1a30ec1d144b",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "## Connect to superduper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d66021-ce62-4021-a2c5-158dee92b3bb",
   "metadata": {},
   "source": [
    ":::note\n",
    "Note that this is only relevant if you are running superduper in development mode.\n",
    "Otherwise refer to \"Configuring your production system\".\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b39819c-2bc3-49a3-b285-d13b6ccc1242",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "APPLY = True\n",
    "TABLE_NAME = 'sample_llm_finetuning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb029a5e-fedf-4f07-8a31-d220cfbfbb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper import superduper\n",
    "\n",
    "db = superduper('mongomock://test_db', force_apply=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032c2e7b-3f54-4263-b778-0fef60596efb",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "## Get LLM Finetuning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f5169e-ab2f-4eac-bd3f-30fd845f2a1b",
   "metadata": {},
   "source": [
    "The following are examples of training data in different formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b37c7dc-390a-428b-916a-09d191678cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from superduper.base.document import Document\n",
    "\n",
    "def getter():\n",
    "\n",
    "    dataset_name = \"timdettmers/openassistant-guanaco\"\n",
    "    dataset = load_dataset(dataset_name)\n",
    "    \n",
    "    train_dataset = dataset[\"train\"]\n",
    "    eval_dataset = dataset[\"test\"]\n",
    "    \n",
    "    train_documents = [{**example, \"_fold\": \"train\"} for example in train_dataset][:10]\n",
    "    eval_documents = [{**example, \"_fold\": \"valid\"} for example in eval_dataset][:5]\n",
    "    \n",
    "    data = train_documents + eval_documents\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef3a6a1-732c-4896-b00b-8624876533a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if APPLY:\n",
    "    data = getter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361a4705-e7d6-4244-9150-bfa8372f85ba",
   "metadata": {},
   "source": [
    "We can define different training parameters to handle this type of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c824212e-0c4f-4b93-b3fa-4d2a105fc655",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = None\n",
    "key = ('text')\n",
    "training_kwargs=dict(dataset_text_field=\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7a1ec0-bf28-4b59-8be1-e7bcfd4eeccc",
   "metadata": {},
   "source": [
    "Example input_text and output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb8c36c-97f8-40f4-8b8d-736d55352138",
   "metadata": {},
   "outputs": [],
   "source": [
    "if APPLY:\n",
    "    d = data[0]\n",
    "    input_text, output_text = d[\"text\"].rsplit(\"### Assistant: \", maxsplit=1)\n",
    "    input_text += \"### Assistant: \"\n",
    "    output_text = output_text.rsplit(\"### Human:\")[0]\n",
    "    print(\"Input: --------------\")\n",
    "    print(input_text)\n",
    "    print(\"Response: --------------\")\n",
    "    print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027bc0ba",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "## Insert simple data\n",
    "\n",
    "After turning on auto_schema, we can directly insert data, and superduper will automatically analyze the data type, and match the construction of the table and datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbdf015",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_or_collection = db[TABLE_NAME]\n",
    "\n",
    "if APPLY:\n",
    "    table_or_collection.insert(data).execute()\n",
    "\n",
    "select = table_or_collection.select()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a46a283-4fef-40d8-8df4-7023479ec2dd",
   "metadata": {},
   "source": [
    "## Select a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef0e1e-22ea-4af3-b914-1a3eed23a754",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "model_kwargs = dict()\n",
    "tokenizer_kwargs = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf339d2-6eaa-4a90-8718-6d6e6120c400",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "## Build A Trainable LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6c2662-ce55-4767-8e3d-ef3901fd31ee",
   "metadata": {},
   "source": [
    "**Create an LLM Trainer for training**\n",
    "\n",
    "The parameters of this LLM Trainer are basically the same as `transformers.TrainingArguments`, but some additional parameters have been added for easier training setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deed34b-189c-4662-8972-aed92718225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper_transformers import LLM, LLMTrainer\n",
    "\n",
    "trainer = LLMTrainer(\n",
    "    identifier=\"llm-finetune-trainer\",\n",
    "    output_dir=\"output/finetune\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,\n",
    "    save_total_limit=10,\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    max_seq_length=512,\n",
    "    key=key,\n",
    "    select=select,\n",
    "    transform=transform,\n",
    "    training_kwargs=training_kwargs,\n",
    "    use_lora=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a370f6c8-ce2f-443e-ae89-a4e95c4375a8",
   "metadata": {},
   "source": [
    "Create a trainable LLM model and add it to the database, then the training task will run automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400c1ebb-345e-4030-9f9e-96099f53664c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(\n",
    "    identifier=\"llm\",\n",
    "    model_name_or_path=model_name,\n",
    "    trainer=trainer,\n",
    "    model_kwargs=model_kwargs,\n",
    "    tokenizer_kwargs=tokenizer_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb0dd6a-2bc8-4288-9b99-49e1ab214965",
   "metadata": {},
   "outputs": [],
   "source": [
    "if APPLY:\n",
    "    db.apply(llm, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3440967-bd90-40d7-b78d-caab78f9472a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper import Template, Table, Schema, Application\n",
    "from superduper.components.dataset import RemoteData\n",
    "\n",
    "llm.trainer.use_lora = \"<var:use_lora>\"\n",
    "llm.trainer.num_train_epochs = \"<var:num_train_epochs>\"\n",
    "\n",
    "app = Application(identifier=\"llm\", components=[llm])\n",
    "\n",
    "t = Template(\n",
    "    'llm-finetune',\n",
    "    template=app,\n",
    "    substitutions={\n",
    "        TABLE_NAME: 'table_name',\n",
    "        model_name: 'model_name',\n",
    "    },\n",
    "    default_table=Table(\n",
    "        'sample_llm_finetuning',\n",
    "        schema=Schema(\n",
    "            'sample_llm_finetuning/schema',\n",
    "            fields={'x': 'str', 'y': 'int'},\n",
    "        ),\n",
    "        data=RemoteData(\n",
    "            'llm_finetuning',\n",
    "            getter=getter,\n",
    "        ),\n",
    "    ),\n",
    "    template_variables=['table_name', 'model_name', 'use_lora', 'num_train_epochs'],\n",
    "    types={\n",
    "        'collection': {\n",
    "            'type': 'str',\n",
    "            'default': 'dataset',\n",
    "        },\n",
    "        'model_name': {\n",
    "            'type': 'str',\n",
    "            'default': 'Qwen/Qwen2.5-0.5B',\n",
    "        },\n",
    "        'use_lora': {\n",
    "            'type': 'bool',\n",
    "            'default': True,\n",
    "        },\n",
    "        'num_train_epochs': {\n",
    "            'type': 'int',\n",
    "            'default': 3\n",
    "        },\n",
    "        'table_name': {\n",
    "            'type': 'str',\n",
    "            'default': 'sample_llm_finetuning',\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "t.export('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edd846d-aa81-456f-b2ea-fc2d230a41a2",
   "metadata": {},
   "source": [
    "## Load the trained model\n",
    "There are two methods to load a trained model:\n",
    "\n",
    "- **Load the model directly**: This will load the model with the best metrics (if the transformers' best model save strategy is set) or the last version of the model.\n",
    "- **Use a specified checkpoint**: This method downloads the specified checkpoint, then initializes the base model, and finally merges the checkpoint with the base model. This approach supports custom operations such as resetting flash_attentions, model quantization, etc., during initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2e1a0d-c760-4a01-b4bf-c6e83296ca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if APPLY:\n",
    "    llm = db.load(\"model\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b8933b-723d-481c-9d6e-dff14d256377",
   "metadata": {},
   "outputs": [],
   "source": [
    "if APPLY:\n",
    "    llm.predict(input_text, max_new_tokens=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
