{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1c66caf",
   "metadata": {},
   "source": [
    "# Transfer learning using Sentence Transformers and Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf342c7-86e5-42ce-a41d-87e6828a6979",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install superduperdb\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc151f6",
   "metadata": {},
   "source": [
    "In this example, we'll be demonstrating how to simply implement transfer learning using SuperDuperDB.\n",
    "You'll find related examples on vector-search and simple training examples using scikit-learn in the \n",
    "the notebooks directory of the project. Transfer learning leverages similar components, and may be used synergistically with vector-search. Vectors are, after all, simultaneously featurizations of \n",
    "data and may be used in downstream learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73ff7b9",
   "metadata": {},
   "source": [
    "Let's first connect to MongoDB via SuperDuperDB, you read explanations of how to do this in \n",
    "the docs, and in the `notebooks/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f8ef76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduperdb import superduper\n",
    "from superduperdb.db.mongodb.query import Collection\n",
    "import os\n",
    "\n",
    "# Uncomment one of the following lines to use a bespoke MongoDB deployment\n",
    "# For testing the default connection is to mongomock\n",
    "\n",
    "mongodb_uri = os.getenv(\"MONGODB_URI\",\"mongomock://test\")\n",
    "# mongodb_uri = \"mongodb://localhost:27017\"\n",
    "# mongodb_uri = \"mongodb://superduper:superduper@mongodb:27017/documents\"\n",
    "# mongodb_uri = \"mongodb://<user>:<pass>@<mongo_cluster>/<database>\"\n",
    "# mongodb_uri = \"mongodb+srv://<username>:<password>@<atlas_cluster>/<database>\"\n",
    "\n",
    "# Super-Duper your Database!\n",
    "from superduperdb import superduper\n",
    "db = superduper(mongodb_uri)\n",
    "\n",
    "collection = Collection('transfer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fede97",
   "metadata": {},
   "source": [
    "We'll use textual data labelled with sentiment, to test the functionality. Transfer learning \n",
    "can be used on any data which can be processed with SuperDuperDB models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb65106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from datasets import load_dataset\n",
    "\n",
    "from superduperdb.container.document import Document as D\n",
    "\n",
    "data = load_dataset(\"imdb\")\n",
    "\n",
    "N_DATAPOINTS = 500    # increase in order to improve quality\n",
    "\n",
    "train_data = [\n",
    "    D({'_fold': 'train', **data['train'][int(i)]}) \n",
    "    for i in numpy.random.permutation(len(data['train']))\n",
    "][:N_DATAPOINTS]\n",
    "\n",
    "valid_data = [\n",
    "    D({'_fold': 'valid', **data['test'][int(i)]}) \n",
    "    for i in numpy.random.permutation(len(data['test']))\n",
    "][:N_DATAPOINTS // 10]\n",
    "\n",
    "db.execute(collection.insert_many(train_data))\n",
    "\n",
    "r = db.execute(collection.find_one())\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a92214",
   "metadata": {},
   "source": [
    "Let's create a SuperDuperDB model based on a `sentence_transformers` model.\n",
    "You'll notice that we don't necessarily need a native SuperDuperDB integration to a model library \n",
    "in order to leverage its power with SuperDuperDB. For example, in this case, we just need \n",
    "to configure the `Model` wrapper to interoperate correctly with the `SentenceTransformer` class. After doing this, we can link the model to a collection, and daemonize the model using the `listen=True` keyword:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef91c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduperdb.container.model import Model\n",
    "import sentence_transformers\n",
    "\n",
    "from superduperdb.ext.numpy.array import array\n",
    "\n",
    "m = Model(\n",
    "    identifier='all-MiniLM-L6-v2',\n",
    "    object=sentence_transformers.SentenceTransformer('all-MiniLM-L6-v2'),\n",
    "    encoder=array('float32', shape=(384,)),\n",
    "    predict_method='encode',\n",
    "    batch_predict=True,\n",
    ")\n",
    "\n",
    "m.predict(\n",
    "    X='text',\n",
    "    db=db,\n",
    "    select=collection.find(),\n",
    "    listen=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fefc17",
   "metadata": {},
   "source": [
    "Now that we've created and added the model which computes features for the `\"text\"`, we can train a \n",
    "downstream model using Scikit-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2faeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model = superduper(\n",
    "    SVC(gamma='scale', class_weight='balanced', C=100, verbose=True),\n",
    "    postprocess=lambda x: int(x)\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X='text',\n",
    "    y='label',\n",
    "    db=db,\n",
    "    select=collection.find().featurize({'text': 'all-MiniLM-L6-v2'}),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e1f164",
   "metadata": {},
   "source": [
    "Now that the model has been trained, we can apply the model to the database, also daemonizing the model \n",
    "with `listen=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee16436",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(\n",
    "    X='text',\n",
    "    db=db,\n",
    "    select=collection.find().featurize({'text': 'all-MiniLM-L6-v2'}),\n",
    "    listen=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b156c1",
   "metadata": {},
   "source": [
    "To verify that this process has worked, we can sample a few records, to inspect the sanity of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76958a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = next(db.execute(collection.aggregate([{'$sample': {'size': 1}}])))\n",
    "print(r['text'][:100])\n",
    "print(r['_outputs']['text']['svc'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
