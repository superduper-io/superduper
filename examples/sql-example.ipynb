{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0e29fef",
   "metadata": {},
   "source": [
    "# End-2-end example using SQL databases\n",
    "\n",
    "SuperDuperDB allows users to connect to a MongoDB database, or any one of a range of SQL databases, i.e. from this selection:\n",
    "\n",
    "- MongoDB\n",
    "- PostgreSQL\n",
    "- SQLite\n",
    "- DuckDB\n",
    "- BigQuery\n",
    "- ClickHouse\n",
    "- DataFusion\n",
    "- Druid\n",
    "- Impala\n",
    "- MSSQL\n",
    "- MySQL\n",
    "- Oracle\n",
    "- pandas\n",
    "- Polars\n",
    "- PySpark\n",
    "- Snowflake\n",
    "- Trino\n",
    "\n",
    "In this example we show case how to implement multimodal vector-search with DuckDB.\n",
    "This is a simple extension of multimodal vector-search with MongoDB, which is \n",
    "just slightly easier to set-up (see [here](https://docs.superduperdb.com/docs/use_cases/items/multimodal_image_search_clip)).\n",
    "Everything we do here applies equally to any of the above supported SQL databases, as well as to tabular data formats on disk, such as `pandas`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1db9c6",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before working on this use-case, make sure that you've installed the software requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d752ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install superduperdb[demo]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfde8264",
   "metadata": {},
   "source": [
    "## Connect to datastore\n",
    "\n",
    "The first step in any `superduperdb` workflow is to connect to your datastore.\n",
    "In order to connect to a different datastore, add a different `URI`, e.g. `postgres://...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e7ef91-9eda-4fbd-b34f-b49b5411fc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from superduperdb import superduper\n",
    "\n",
    "os.makedirs('.superduperdb', exist_ok=True)\n",
    "db = superduper('duckdb://.superduperdb/test.ddb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8794451",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "Now, Once connected, add some data to the datastore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d2b073-38b0-4d29-aa65-e568f19e7852",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://superduperdb-public.s3.eu-west-1.amazonaws.com/coco_sample.zip\n",
    "!curl -O https://superduperdb-public.s3.eu-west-1.amazonaws.com/captions_tiny.json\n",
    "!unzip coco_sample.zip\n",
    "!mkdir -p data/coco\n",
    "!mv images_small data/coco/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5d36d3-7e74-4c87-92c2-ed1586330858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas\n",
    "import PIL.Image\n",
    "\n",
    "with open(captions_tiny.json') as f:\n",
    "    data = json.load(f)[:500]\n",
    "    \n",
    "data = pandas.DataFrame([\n",
    "    {\n",
    "        'image': r['image']['_content']['path'], \n",
    "         'captions': r['captions']\n",
    "    } for r in data   \n",
    "])\n",
    "data['id'] = pandas.Series(data.index).apply(str)\n",
    "images_df = data[['id', 'image']]\n",
    "\n",
    "images_df['image'] = images_df['image'].apply(PIL.Image.open)\n",
    "captions_df = data[['id', 'captions']].explode('captions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43cd7d2",
   "metadata": {},
   "source": [
    "## Define schema\n",
    "\n",
    "This use-case requires a table with images, and a table with text. \n",
    "SuperDuperDB extends standard SQL functionality, by allowing developers to define\n",
    "their own data-types via the `Encoder` abstraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9483f3-78c5-47df-9fa2-4cd070282791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduperdb.backends.ibis.query import Table\n",
    "from superduperdb.backends.ibis.field_types import dtype\n",
    "from superduperdb.ext.pillow import pil_image\n",
    "from superduperdb import Schema\n",
    "\n",
    "captions = Table(\n",
    "    'captions', \n",
    "    primary_id='id',\n",
    "    schema=Schema(\n",
    "        'captions-schema',\n",
    "        fields={'id': dtype(str), 'captions': dtype(str)},\n",
    "    )\n",
    ")\n",
    "\n",
    "images = Table(\n",
    "    'images', \n",
    "    primary_id='id',\n",
    "    schema=Schema(\n",
    "        'images-schema',\n",
    "        fields={'id': dtype(str), 'image': pil_image},\n",
    "    )\n",
    ")\n",
    "\n",
    "db.add(captions)\n",
    "db.add(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115b2c14",
   "metadata": {},
   "source": [
    "## Add data to the datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cce29b-dd04-47d2-bdfc-fe3780e06ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = db.execute(images.insert(images_df))\n",
    "_ = db.execute(captions.insert(captions_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def10282",
   "metadata": {},
   "source": [
    "## Build SuperDuperDB `Model` instances\n",
    "\n",
    "This use-case uses the `superduperdb.ext.torch` extension. \n",
    "Both models used, output `torch` tensors, which are encoded with `tensor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87ed43d-6f90-46c1-8851-6050ae21a051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "from superduperdb.ext.torch import TorchModel, tensor\n",
    "\n",
    "# Load the CLIP model\n",
    "model, preprocess = clip.load(\"RN50\", device='cpu')\n",
    "\n",
    "# Define a tensor type\n",
    "t = tensor(torch.float, shape=(1024,))\n",
    "\n",
    "# Create a TorchModel for text encoding\n",
    "text_model = TorchModel(\n",
    "    identifier='clip_text',\n",
    "    object=model,\n",
    "    preprocess=lambda x: clip.tokenize(x)[0],\n",
    "    encoder=t,\n",
    "    forward_method='encode_text',    \n",
    ")\n",
    "\n",
    "# Create a TorchModel for visual encoding\n",
    "visual_model = TorchModel(\n",
    "    identifier='clip_image',\n",
    "    object=model.visual,    \n",
    "    preprocess=preprocess,\n",
    "    encoder=t,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c5c236",
   "metadata": {},
   "source": [
    "## Create a Vector-Search Index\n",
    "\n",
    "Let's define a mult-modal search index on the basis of the models imported above.\n",
    "The `visual_model` is applied to the images, to make the `images` table searchable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8aef2c-484f-41a9-9956-d80b9c58eaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduperdb import VectorIndex, Listener\n",
    "\n",
    "db.add(\n",
    "    VectorIndex(\n",
    "        'my-index',\n",
    "        indexing_listener=Listener(\n",
    "            model=visual_model,\n",
    "            key='image',\n",
    "            select=images,\n",
    "        ),\n",
    "        compatible_listener=Listener(\n",
    "            model=text_model,\n",
    "            key='captions',\n",
    "            active=False,\n",
    "            select=None,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8b9e84",
   "metadata": {},
   "source": [
    "## Search Images Using Text\n",
    "\n",
    "Now we can demonstrate searching for images using text queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1731a574-921a-4cba-a65c-26bff9fb9c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduperdb import Document\n",
    "\n",
    "res = db.execute(\n",
    "    images\n",
    "        .like(Document({'captions': 'dog catches frisbee'}), vector_index='my-index', n=10)\n",
    "        .limit(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72522031-0af8-452a-bbd1-b27dede55154",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[3]['image'].x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
