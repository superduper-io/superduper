{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1c66caf",
   "metadata": {},
   "source": [
    "# Transfer learning using Sentence Transformers and Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc151f6",
   "metadata": {},
   "source": [
    "In this example, we'll be demonstrating how to simply implement transfer learning using SuperDuperDB.\n",
    "You'll find related examples on vector-search and simple training examples using scikit-learn in the \n",
    "the notebooks directory of the project. Transfer learning leverages similar components, and may be used synergistically with vector-search. Vectors are, after all, simultaneously featurizations of \n",
    "data and may be used in downstream learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73ff7b9",
   "metadata": {},
   "source": [
    "Let's first connect to MongoDB via SuperDuperDB, you read explanations of how to do this in \n",
    "the docs, and in the `notebooks/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44f8ef76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:faiss.loader:Loading faiss.\n",
      "INFO:faiss.loader:Successfully loaded faiss.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "from superduperdb import superduper\n",
    "from superduperdb.db.mongodb.query import Collection\n",
    "import pymongo\n",
    "\n",
    "db = superduper(\n",
    "    pymongo.MongoClient().documents\n",
    ")\n",
    "\n",
    "collection = Collection('transfer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fede97",
   "metadata": {},
   "source": [
    "We'll use textual data labelled with sentiment, to test the functionality. Transfer learning \n",
    "can be used on any data which can be processed with SuperDuperDB models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bb65106",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset imdb (/Users/dodo/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "674cda9ef7704fcdbac8221394b081f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:found 0 uris\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document({'_id': ObjectId('64c8f2fc13655f5a0a9cb0f9'), '_fold': 'train', 'text': \"A bunch of American students and their tutor decide to visit the ugliest part of Ireland in order to study ancient religious practices. Despite being repeatedly warned about the dangers of straying off the beaten path (by the local creepy Irish guy, natch), they do just that, and wind up with their insides on the outside courtesy of a family of inbred cannibals (the descendants of the infamous Sawney Bean clan, who according to the film's silly plot, upped sticks from Scotland and settled on the Emerald Isle).<br /><br />If you think that porn stars plus low budget horror automatically equals tons of nudity and terrible acting, then think again: Evil Breed is bristling with adult stars, but in fact, there's not nearly as much nudity as one might expect given the 'talent' involved, and the acting, although far from Oscar worthy, ain't all that bad (with the exception of Ginger Lynn Allen, who we know can do marvellous 'French', but whose Irish is lousy).<br /><br />Evil Breed opens in superb style with the brutal slaughter of a couple of amorous campers: after some brief under-canvas sex, the silicone enhanced hottie is dragged from the tent and torn in half; the guy has his arms and legs cut off and is roasted on a spit. It's a very gory start, and bodes well for the rest of the film.<br /><br />Unfortunately, after this promising beginning, things start to go seriously downhill: we are introduced to the main characters, an annoying bunch of twenty-somethings just begging to become cannibal chow, and are subjected to a fair amount of time wasting in the form of some terrible false scares, a lot of blarney about murderous druids from local Irish weirdo Gary (Simon Peacock), and worst of all, some sub-Scream, post-modernistic conversation about the conventions of horror films (how clever!).<br /><br />Then, just as it looks as though the film is never going to get any better, director Christian Viel decides to get serious: a guy gets a knife rammed through his head and there's a gratuitous sex-in-the-shower scene featuring lovely blonde Gillian Leigh (NOT a porn star, but I'm sure there's a career there waiting if she wants it). After that, things improve rapidly as the cannibals kick into top flesh-eating gear, and the film is transformed into a veritable bloodbath: Gary has a machete rammed up his ass (about time!), and is strangled with his intestines; Ginger Lynn kick-boxes a mutant; Jenna Jameson is torn open, eviscerated and has her silicone breast implant gnawed on by confused cannibal; a guy gets decapitated by cheese wire; and Taylor Hayes is seen bloody, bruised and naked with a dead foetus between her legs (apparantly, she's been captured and used as breeding stock).<br /><br />All of this is so outrageously gory that it makes sitting through the less interesting stuff worthwhile, and earns Evil Breed a final rating of 7/10.<br /><br />NB. A very troubled production and studio meddling resulted in Christian Viel eventually abandoning the project. Re-shoots were done and the gore was heavily trimmed for a US release. The good news is that although the film doesn't flow as well as it might have, and is cursed with a terrible ending, the UK DVD (the version I watched) seems to have been left relatively intact as far as the splatter is concerned (only 13s were cut from the film in total).\", 'label': 1})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "from datasets import load_dataset\n",
    "\n",
    "from superduperdb.container.document import Document as D\n",
    "\n",
    "data = load_dataset(\"imdb\")\n",
    "\n",
    "train_data = [\n",
    "    D({'_fold': 'train', **data['train'][int(i)]}) \n",
    "    for i in numpy.random.permutation(len(data['train']))\n",
    "][:10000]\n",
    "\n",
    "valid_data = [\n",
    "    D({'_fold': 'valid', **data['test'][int(i)]}) \n",
    "    for i in numpy.random.permutation(len(data['test']))\n",
    "][:1000]\n",
    "\n",
    "db.execute(collection.insert_many(train_data))\n",
    "\n",
    "r = db.execute(collection.find_one())\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a92214",
   "metadata": {},
   "source": [
    "Let's create a SuperDuperDB model based on a `sentence_transformers` model.\n",
    "You'll notice that we don't necessarily need a native SuperDuperDB integration to a model library \n",
    "in order to leverage its power with SuperDuperDB. For example, in this case, we just need \n",
    "to configure the `Model` wrapper to interoperate correctly with the `SentenceTransformer` class. After doing this, we can link the model to a collection, and daemonize the model using the `watch=True` keyword:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fef91c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /var/folders/y9/b74b9yj906s_wtj0rrh2lf7c0000gn/T/tmp9qh0wm8h\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /var/folders/y9/b74b9yj906s_wtj0rrh2lf7c0000gn/T/tmp9qh0wm8h/_remote_module_non_scriptable.py\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cpu\n",
      "WARNING:root:model/all-MiniLM-L6-v2/0 already exists - doing nothing\n",
      "WARNING:root:model/all-MiniLM-L6-v2/0 already exists - doing nothing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db6bf0c5de0740f09f084549cf298c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from superduperdb.container.model import Model\n",
    "import sentence_transformers\n",
    "\n",
    "from superduperdb.ext.numpy.array import array\n",
    "\n",
    "m = Model(\n",
    "    identifier='all-MiniLM-L6-v2',\n",
    "    object=sentence_transformers.SentenceTransformer('all-MiniLM-L6-v2'),\n",
    "    encoder=array('float32', shape=(384,)),\n",
    "    predict_method='encode',\n",
    "    batch_predict=True,\n",
    ")\n",
    "\n",
    "m.predict(\n",
    "    X='text',\n",
    "    db=db,\n",
    "    select=collection.find(),\n",
    "    watch=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fefc17",
   "metadata": {},
   "source": [
    "Now that we've created and added the model which computes features for the `\"text\"`, we can train a \n",
    "downstream model using Scikit-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c2faeeb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 58573.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]...................*.........*\n",
      "optimization finished, #iter = 28120\n",
      "obj = -7557.408109, rho = 0.295979\n",
      "nSV = 6656, nBSV = 0\n",
      "Total nSV = 6656\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=100, class_weight=&#x27;balanced&#x27;, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=100, class_weight=&#x27;balanced&#x27;, verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=100, class_weight='balanced', verbose=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model = superduper(\n",
    "    SVC(gamma='scale', class_weight='balanced', C=100, verbose=True),\n",
    "    postprocess=lambda x: int(x)\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X='text',\n",
    "    y='label',\n",
    "    db=db,\n",
    "    select=collection.find().featurize({'text': 'all-MiniLM-L6-v2'}),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e1f164",
   "metadata": {},
   "source": [
    "Now that the model has been trained, we can apply the model to the database, also daemonizing the model \n",
    "with `watch=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eee16436",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:model/svc/0 already exists - doing nothing\n",
      "WARNING:root:model/svc/0 already exists - doing nothing\n",
      "WARNING:root:model/svc/0 already exists - doing nothing\n"
     ]
    }
   ],
   "source": [
    "model.predict(\n",
    "    X='text',\n",
    "    db=db,\n",
    "    select=collection.find().featurize({'text': 'all-MiniLM-L6-v2'}),\n",
    "    watch=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b156c1",
   "metadata": {},
   "source": [
    "To verify that this process has worked, we can sample a few records, to inspect the sanity of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76958a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The writers probably had no experience in the army, and probably never glanced at a history book, bu\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "r = next(db.execute(collection.aggregate([{'$sample': {'size': 1}}])))\n",
    "print(r['text'][:100])\n",
    "print(r['_outputs']['text']['svc'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
