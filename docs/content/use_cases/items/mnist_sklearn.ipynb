{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15eb770d",
   "metadata": {},
   "source": [
    "# MNIST using scikit-learn and SuperDuperDB\n",
    "\n",
    "In a [previous example](mnist_torch.html) we discussed how to implement MNIST classification with CNNs in `torch`\n",
    "using SuperDuperDB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169c210e-eef8-4827-a47c-ce357a5898bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install superduperdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09ce280",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "import numpy as np\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9e4f2f",
   "metadata": {},
   "source": [
    "As before we'll import the python MongoDB client `pymongo`\n",
    "and \"wrap\" our database to convert it to a SuperDuper `Datalayer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4785878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Uncomment one of the following lines to use a bespoke MongoDB deployment\n",
    "# For testing the default connection is to mongomock\n",
    "\n",
    "mongodb_uri = os.getenv(\"MONGODB_URI\",\"mongomock://test\")\n",
    "# mongodb_uri = \"mongodb://localhost:27017\"\n",
    "# mongodb_uri = \"mongodb://superduper:superduper@mongodb:27017/documents\"\n",
    "# mongodb_uri = \"mongodb://<user>:<pass>@<mongo_cluster>/<database>\"\n",
    "# mongodb_uri = \"mongodb+srv://<username>:<password>@<atlas_cluster>/<database>\"\n",
    "\n",
    "# Super-Duper your Database!\n",
    "from superduperdb import superduper\n",
    "db = superduper(mongodb_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a55133",
   "metadata": {},
   "source": [
    "Similarly to last time, we can add data to SuperDuperDB in a way which very similar to using `pymongo`.\n",
    "This time, we'll add the data as `numpy.array` to SuperDuperDB, using the `Document-Encoder` formalism:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bf2a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduperdb.ext.numpy.array import array\n",
    "from superduperdb.container.document import Document as D\n",
    "from superduperdb.db.mongodb.query import Collection\n",
    "\n",
    "mnist = fetch_openml('mnist_784')\n",
    "ix = np.random.permutation(2000)\n",
    "X = np.array(mnist.data)[ix, :]\n",
    "y = np.array(mnist.target)[ix].astype(int)\n",
    "\n",
    "a = array('float64', shape=(784,))\n",
    "\n",
    "collection = Collection(name='mnist')\n",
    "\n",
    "data = [D({'img': a(X[i]), 'class': int(y[i])}) for i in range(len(X))]\n",
    "\n",
    "db.execute(\n",
    "    collection.insert_many(data, encoders=[a])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faf269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.execute(collection.find_one())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f20935",
   "metadata": {},
   "source": [
    "Models are built similarly to the `Datalayer`, by wrapping a standard Python-AI-ecosystem model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a7844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = superduper(\n",
    "    svm.SVC(gamma='scale', class_weight='balanced', C=100, verbose=True),\n",
    "    postprocess=lambda x: int(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8377634",
   "metadata": {},
   "source": [
    "Now let's fit the model. The optimization uses Scikit-Learn's inbuilt training procedures.\n",
    "Unlike in a standard `sklearn` use-case, we don't need to fetch the data client side. Instead, \n",
    "we simply name the fields in the MongoDB collection which we'd like to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa7785f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X='img', y='class', db=db, select=collection.find())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa908ad",
   "metadata": {},
   "source": [
    "Installed models and functionality can be viewed using `db.show`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f344fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.show('model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115cdf62",
   "metadata": {},
   "source": [
    "The model may be reloaded in another session from the database. \n",
    "As with `.fit`, the model may be applied to data in the database with `.predict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a21753",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = db.load('model', 'svc')\n",
    "m.predict(X='img', db=db, select=collection.find(), max_chunk_size=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcbc44f",
   "metadata": {},
   "source": [
    "We can verify that the predictions make sense by fetching a few random data-points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65da9f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = next(db.execute(collection.aggregate([{'$match': {'_fold': 'valid'}} ,{'$sample': {'size': 1}}])))\n",
    "print(r['class'])\n",
    "print(r['_outputs'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
