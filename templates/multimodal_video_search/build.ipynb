{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38c1a328-fd86-4c5f-bd54-b8664f433608",
   "metadata": {},
   "source": [
    "# Multimodal vector search - Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d985fe16-1b82-4f50-bfd6-380909e5ecc0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "APPLY = True\n",
    "TABLE_NAME = 'docs'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f8484d-2e35-472a-9b24-1a30ec1d144b",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "## Connect to superduper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb029a5e-fedf-4f07-8a31-d220cfbfbb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper import superduper\n",
    "\n",
    "db = superduper('mongomock://test_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032c2e7b-3f54-4263-b778-0fef60596efb",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "## Get useful sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b360009-0426-4ec4-993e-40908c23b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getter():\n",
    "    import os\n",
    "    import subprocess\n",
    "    subprocess.run(['rm', 'videos.zip'])\n",
    "    subprocess.run(['rm'\n",
    "                    , '-rf', 'videos'])\n",
    "    subprocess.run(['curl', '-O', 'https://superduperdb-public-demo.s3.amazonaws.com/videos.zip'])\n",
    "    subprocess.run(['unzip', 'videos.zip'])\n",
    "    subprocess.run(['rm', 'videos.zip'])\n",
    "    data = [{'x': f'videos/{x}'} for x in os.listdir('./videos')]\n",
    "    return data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6f7ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if APPLY:\n",
    "    data = getter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31257e4-06fa-4cc7-9626-bb4d03fdc029",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "## Create datatype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43284218",
   "metadata": {},
   "source": [
    "SuperduperDB supports automatic data conversion, so users donâ€™t need to worry about the compatibility of different data formats (`PIL.Image`, `numpy.array`, `pandas.DataFrame`, etc.) with the database.\n",
    "\n",
    "It also supports custom data conversion methods for transforming data, such as defining the following Datatype."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9026acd8",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "## Setup tables or collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2f0083",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper.components.table import Table\n",
    "from superduper import Schema\n",
    "\n",
    "table = Table(TABLE_NAME, fields={'x': 'file'})\n",
    "\n",
    "if APPLY:\n",
    "    db.apply(table, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e5422a-a05e-4692-8be0-23b2d8fd504d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if APPLY:\n",
    "    db[TABLE_NAME].insert(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fea927-ee4a-44cd-aaf2-634b574c316d",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "## Apply a chunker for search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d90bda-e8c4-494e-a38c-837fb63689ae",
   "metadata": {},
   "source": [
    ":::note\n",
    "Note that applying a chunker is ***not*** mandatory for search.\n",
    "If your data is already chunked (e.g. short text snippets or audio) or if you\n",
    "are searching through something like images, which can't be chunked, then this\n",
    "won't be necessary.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6746cb-0bce-4d64-9930-ea3598ac409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f093a6d0-9d2f-4ecf-b1bd-0027302c62de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "from superduper import ObjectModel\n",
    "from superduper.base.datatype import FileItem\n",
    "from superduper.misc.importing import isreallyinstance\n",
    "\n",
    "\n",
    "class Chunker:\n",
    "    def __hash__(self):\n",
    "        return 1234567890\n",
    "\n",
    "    def __call__(self, video_file):\n",
    "        # Set the sampling frequency for frames\n",
    "\n",
    "        if isreallyinstance(video_file, FileItem):\n",
    "            video_file = video_file.unpack()\n",
    "        sample_freq = 100\n",
    "        \n",
    "        # Open the video file using OpenCV\n",
    "        cap = cv2.VideoCapture(video_file)\n",
    "        \n",
    "        # Initialize variables\n",
    "        frame_count = 0\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        extracted_frames = []\n",
    "        progress = tqdm.tqdm()\n",
    "\n",
    "        # Iterate through video frames\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Get the current timestamp based on frame count and FPS\n",
    "            current_timestamp = frame_count // fps\n",
    "            \n",
    "            # Sample frames based on the specified frequency\n",
    "            if frame_count % sample_freq == 0:\n",
    "                extracted_frames.append({\n",
    "                    'image': Image.fromarray(frame[:,:,::-1]),  # Convert BGR to RGB\n",
    "                    'current_timestamp': current_timestamp,\n",
    "                })\n",
    "            frame_count += 1\n",
    "            progress.update(1)\n",
    "        \n",
    "        # Release resources \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        # Return the list of extracted frames\n",
    "        return extracted_frames\n",
    "\n",
    "\n",
    "chunker = ObjectModel(\n",
    "    'chunker', \n",
    "    object=Chunker(),\n",
    "    datatype='image=superduper_pillow.pil_image|current_timestamp=int',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33a16f9-3bac-45bb-80ac-3ccf265dce5f",
   "metadata": {},
   "source": [
    "Now we apply this chunker to the data by wrapping the chunker in `Listener`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d21872-d4dc-40dc-abab-fb07ba102ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper import Listener\n",
    "\n",
    "upstream_listener = Listener(\n",
    "    model=chunker,\n",
    "    select=db['docs'],\n",
    "    key='x',\n",
    "    identifier='chunker',\n",
    "    flatten=True,\n",
    "    upstream=[table],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0616e1e3-a5e0-4891-94b2-55ec0074cffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if APPLY:\n",
    "    db.apply(upstream_listener, force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907721f8-d5bf-4623-8871-3ab9a05001d7",
   "metadata": {},
   "source": [
    "# Build multimodal embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033e1eaf-2cdb-499a-ba83-cf080a1a6fda",
   "metadata": {},
   "source": [
    "We define the output data type of a model as a vector for vector transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143bf946-64b7-4452-8d20-44f2f9ae3fd6",
   "metadata": {},
   "source": [
    "Then define two models, one for text embedding and one for image embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b19850-d52c-46fb-a4b1-cb676bb9c6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multimodal_models import TextModel, ImageModel\n",
    "from superduper import Plugin\n",
    "\n",
    "plugin = Plugin(path='./multimodal_models.py')\n",
    "\n",
    "text_model = TextModel('text', datatype='vector[float:512]')\n",
    "image_model = ImageModel('image', datatype='vector[float:512]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0119da-9cfd-4a60-8847-c3bfdf37697f",
   "metadata": {},
   "source": [
    "Because we use multimodal models, we define different keys to specify which model to use for embedding calculations in the vector_index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b8b40d-3750-4d7b-aa60-62e07b734b04",
   "metadata": {},
   "source": [
    "## Create vector-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cede653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper import VectorIndex, Listener\n",
    "\n",
    "vector_index = VectorIndex(\n",
    "    'my-vector-index',\n",
    "    indexing_listener=Listener(\n",
    "        key=upstream_listener.outputs + '.image',\n",
    "        select=db[upstream_listener.outputs].select(),\n",
    "        model=image_model,\n",
    "        identifier=f'{text_model.identifier}-listener'\n",
    "    ),\n",
    "    compatible_listener=Listener(\n",
    "        key='text',\n",
    "        model=text_model,\n",
    "        select=None,\n",
    "        identifier='compatible-listener',\n",
    "    ),\n",
    "    upstream=[upstream_listener],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9a379f-254c-4305-a0bf-b786c49957c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if APPLY:\n",
    "    db.apply(vector_index, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff1904a-c71d-4294-b7d6-e89d9f0f12b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper import Application\n",
    "\n",
    "app = Application(\n",
    "    'video-search',\n",
    "    components=[\n",
    "        upstream_listener,\n",
    "        vector_index,\n",
    "    ],\n",
    "    upstream=[plugin]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cf1a8b-53ce-4201-9741-e2525f4da116",
   "metadata": {},
   "outputs": [],
   "source": [
    "if APPLY:\n",
    "    db.apply(app, force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a87f9d-581a-419a-81b8-a743250413e9",
   "metadata": {},
   "source": [
    "## Perform a vector search\n",
    "\n",
    "We can perform the vector searches using text description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce565823-4655-488c-8684-2240107fa30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper import Document\n",
    "query = \"Monkeys playing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fb039e-ab88-4773-94d3-9d1a3f942429",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper import Document\n",
    "query = \"Spaceship on the moon\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3ba07d-1124-4d94-a117-60d2e72581f7",
   "metadata": {},
   "source": [
    "Once we have this search target, we can execute a search as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6d9af9-a012-42bd-aad4-31b92d089caa",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2ecea5-3a58-457c-ac50-ddc742484f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if APPLY:\n",
    "    from IPython.display import display\n",
    "    select = db[upstream_listener.outputs].like({'text': query}, vector_index='my-vector-index', n=3).select()\n",
    "\n",
    "    for result in select.execute():\n",
    "        display(result[upstream_listener.outputs + '.image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4781429-a91d-45d8-bbb7-429b0ac82110",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper import Template, Table, Schema\n",
    "from superduper.components.dataset import RemoteData\n",
    "\n",
    "t = Template(\n",
    "    'multimodal_video_search', \n",
    "    template=app,\n",
    "    substitutions={'docs': 'table_name'},\n",
    "    default_tables=[\n",
    "        Table(\n",
    "            'sample_multimodal_video_search',\n",
    "            fields={'x': 'file'},\n",
    "            data=RemoteData(\n",
    "            'sample_videos',\n",
    "            getter=getter,\n",
    "        )\n",
    "    )],\n",
    "    types={\n",
    "        'table_name': {\n",
    "            'type': 'str',\n",
    "            'default': 'sample_multimodal_video_search',\n",
    "        }\n",
    "    },\n",
    "    db=db,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f15780-1ecc-475e-96c3-4875724deb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.export('.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
