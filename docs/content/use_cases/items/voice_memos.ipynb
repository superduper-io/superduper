{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae374483",
   "metadata": {},
   "source": [
    "# Cataloguing voice-memos for a self managed personal assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b99fc0",
   "metadata": {},
   "source": [
    "In this example we show-case SuperDuperDB's ability to combine models across data modalities, \n",
    "in this case audio and text, to devise highly sophisticated data based apps, with very little \n",
    "boilerplate code.\n",
    "\n",
    "The aim, is to:\n",
    "\n",
    "- Maintain a database of audio recordings\n",
    "- Index the content of these audio recordings\n",
    "- Search and interrogate the content of these audio recordings\n",
    "\n",
    "We accomplish this by:\n",
    "\n",
    "1. Use a `transformers` model by Facebook's AI team to transcribe the audio to text\n",
    "2. Use an OpenAI vectorization model to index the transcribed text\n",
    "3. Use OpenAI's ChatGPT model in combination with relevant recordings to interrogate the contents\n",
    "  of the audio database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce1a857",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install superduperdb\n",
    "!pip install torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb5a8f4",
   "metadata": {},
   "source": [
    "This functionality could be accomplised using any audio, in particular audio \n",
    "hosted on the web, or in an `s3` bucket. For instance, if you have a repository\n",
    "of audio of conference calls, or memos, this may be indexed in the same way.\n",
    "\n",
    "To make matters simpler, we use a dataset of audio recordings from the `datasets` library, to demonstrate the \n",
    "functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ab7114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "SAMPLING_RATE = 16000\n",
    "\n",
    "data = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689f71b5",
   "metadata": {},
   "source": [
    "As usual we wrap our MongoDB connector, to connect to the `Datalayer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e1d8149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "WARNING:root:mongomock://test\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from superduperdb import superduper\n",
    "\n",
    "# Uncomment one of the following lines to use a bespoke MongoDB deployment\n",
    "# For testing the default connection is to mongomock\n",
    "\n",
    "mongodb_uri = os.getenv(\"MONGODB_URI\",\"mongomock://test\")\n",
    "# mongodb_uri = \"mongodb://localhost:27017\"\n",
    "# mongodb_uri = \"mongodb://superduper:superduper@mongodb:27017/documents\"\n",
    "# mongodb_uri = \"mongodb://<user>:<pass>@<mongo_cluster>/<database>\"\n",
    "# mongodb_uri = \"mongodb+srv://<username>:<password>@<atlas_cluster>/<database>\"\n",
    "\n",
    "# Super-Duper your Database!\n",
    "from superduperdb import superduper\n",
    "db = superduper(mongodb_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262a8bee",
   "metadata": {},
   "source": [
    "Using an `Encoder`, we may add the audio data directly to a MongoDB collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b288d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduperdb.db.mongodb.query import Collection\n",
    "from superduperdb.ext.numpy.array import array\n",
    "from superduperdb.container.document import Document as D\n",
    "\n",
    "collection = Collection('voice-memos')\n",
    "enc = array('float64', shape=(None,))\n",
    "\n",
    "db.execute(collection.insert_many([\n",
    "    D({'audio': enc(r['audio']['array'])}) for r in data\n",
    "], encoders=(enc,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221f2b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.execute(collection.find_one()).unpack()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3ddd8f",
   "metadata": {},
   "source": [
    "Now that we've done that, let's apply a pretrained `transformers` model to this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222284f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n",
    "\n",
    "model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d5252c",
   "metadata": {},
   "source": [
    "We wrap this model using the SuperDuperDB wrapper for `transformers`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b2e38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduperdb.ext.transformers.model import Pipeline\n",
    "\n",
    "transcriber = Pipeline(\n",
    "    identifier='transcription',\n",
    "    object=model,\n",
    "    preprocess=processor,\n",
    "    preprocess_kwargs={'sampling_rate': SAMPLING_RATE, 'return_tensors': 'pt', 'padding': True},\n",
    "    postprocess=lambda x: processor.batch_decode(x, skip_special_tokens=True),\n",
    "    predict_method='generate',\n",
    "    preprocess_type='other',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a14b185",
   "metadata": {},
   "source": [
    "Let's verify this `Pipeline` works on a sample datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073ddff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "IPython.display.Audio(data[0]['audio']['array'], rate=SAMPLING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b453f913",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriber.predict(data[0]['audio']['array'], one=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41842efb",
   "metadata": {},
   "source": [
    "Now let's apply the `Pipeline` to all of the audio recordings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573dccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriber.predict(X='audio', db=db, select=collection.find(), max_chunk_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a02b93",
   "metadata": {},
   "source": [
    "We may now verify that all of the recordings have been transcribed in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a054796f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(db.execute(\n",
    "    Collection('voice-memos').find({}, {'_outputs.audio.transcription': 1})\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453b9220",
   "metadata": {},
   "source": [
    "As in previous examples, we can use OpenAI's text-embedding models to vectorize and search the \n",
    "textual transcriptions directly in MongoDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b27c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = '<YOUR-API-KEY>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aedc03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduperdb.container.vector_index import VectorIndex\n",
    "from superduperdb.container.listener import Listener\n",
    "from superduperdb.ext.openai.model import OpenAIEmbedding\n",
    "from superduperdb.db.mongodb.query import Collection\n",
    "\n",
    "db.add(\n",
    "    VectorIndex(\n",
    "        identifier='my-index',\n",
    "        indexing_listener=Listener(\n",
    "            model=OpenAIEmbedding(model='text-embedding-ada-002'),\n",
    "            key='_outputs.audio.transcription',\n",
    "            select=Collection(name='voice-memos').find(),\n",
    "        ),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f92b56f",
   "metadata": {},
   "source": [
    "Let's confirm this has worked, by searching for the \"royal cavern\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2e3e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(db.execute(\n",
    "    Collection('voice-memos').like(\n",
    "        {'_outputs.audio.transcription': 'royal cavern'},\n",
    "        n=2,\n",
    "        vector_index='my-index',\n",
    "    ).find({}, {'_outputs.audio.transcription': 1})\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72f505a",
   "metadata": {},
   "source": [
    "Now we can connect the previous steps with the `gpt-3.5.turbo`, which is a chat-completion \n",
    "model on OpenAI. The plan is to seed the completions with the most relevant audio recordings, \n",
    "as judged by their textual transcriptions. These transcriptions are retrieved using \n",
    "the previously configured `VectorIndex`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e206af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduperdb.ext.openai.model import OpenAIChatCompletion\n",
    "\n",
    "chat = OpenAIChatCompletion(\n",
    "    model='gpt-3.5-turbo',\n",
    "    prompt=(\n",
    "        'Use the following facts to answer this question\\n'\n",
    "        '{context}\\n\\n'\n",
    "        'Here\\'s the question:\\n'\n",
    "    ),\n",
    ")\n",
    "\n",
    "db.add(chat)\n",
    "\n",
    "print(db.show('model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb623c4",
   "metadata": {},
   "source": [
    "Let's test the full model! We can ask a question which asks about a specific fact \n",
    "mentioned somewhere in the audio recordings. The model will retrieve the most relevant\n",
    "recordings, and use these in formulating its answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757e9da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduperdb.container.document import Document\n",
    "\n",
    "q = 'Is anything really Greek?'\n",
    "\n",
    "print(db.predict(\n",
    "    model='gpt-3.5-turbo',\n",
    "    input=q,\n",
    "    context_select=Collection('voice-memos').like(\n",
    "        Document({'_outputs.audio.transcription': q}), vector_index='my-index'\n",
    "    ).find(),\n",
    "    context_key='_outputs.audio.transcription',\n",
    ")[0].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
