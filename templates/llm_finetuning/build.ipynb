{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d92f42b-7160-4965-a0ef-5e4ce46bd529",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "# Fine tune LLM on database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f8484d-2e35-472a-9b24-1a30ec1d144b",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "## Connect to superduper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d66021-ce62-4021-a2c5-158dee92b3bb",
   "metadata": {},
   "source": [
    ":::note\n",
    "Note that this is only relevant if you are running superduper in development mode.\n",
    "Otherwise refer to \"Configuring your production system\".\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb029a5e-fedf-4f07-8a31-d220cfbfbb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-Aug-30 21:53:28.08\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.misc.plugins\u001b[0m:\u001b[36m13  \u001b[0m | \u001b[1mLoading plugin: mongodb\u001b[0m\n",
      "\u001b[32m2024-Aug-30 21:53:28.21\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m103 \u001b[0m | \u001b[1mBuilding Data Layer\u001b[0m\n",
      "\u001b[32m2024-Aug-30 21:53:28.21\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.build\u001b[0m:\u001b[36m171 \u001b[0m | \u001b[1mConfiguration: \n",
      " +---------------+----------------------+\n",
      "| Configuration |        Value         |\n",
      "+---------------+----------------------+\n",
      "|  Data Backend | mongomock:///test_db |\n",
      "+---------------+----------------------+\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from superduper import superduper\n",
    "\n",
    "db = superduper('mongomock:///test_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032c2e7b-3f54-4263-b778-0fef60596efb",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "## Get LLM Finetuning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f5169e-ab2f-4eac-bd3f-30fd845f2a1b",
   "metadata": {},
   "source": [
    "The following are examples of training data in different formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b37c7dc-390a-428b-916a-09d191678cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "# <tab: Text>\n",
    "from datasets import load_dataset\n",
    "from superduper.base.document import Document\n",
    "dataset_name = \"timdettmers/openassistant-guanaco\"\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n",
    "\n",
    "train_documents = [\n",
    "    Document({**example, \"_fold\": \"train\"})\n",
    "    for example in train_dataset\n",
    "]\n",
    "eval_documents = [\n",
    "    Document({**example, \"_fold\": \"valid\"})\n",
    "    for example in eval_dataset\n",
    "]\n",
    "\n",
    "datas = train_documents + eval_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7902bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Prompt-Response>\n",
    "from datasets import load_dataset\n",
    "\n",
    "from superduper.base.document import Document\n",
    "dataset_name = \"mosaicml/instruct-v3\"\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n",
    "\n",
    "train_documents = [\n",
    "    Document({**example, \"_fold\": \"train\"})\n",
    "    for example in train_dataset\n",
    "]\n",
    "eval_documents = [\n",
    "    Document({**example, \"_fold\": \"valid\"})\n",
    "    for example in eval_dataset\n",
    "]\n",
    "\n",
    "datas = train_documents + eval_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c05195-3372-48c2-95c8-5ef51d65bcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Chat>\n",
    "from datasets import load_dataset\n",
    "from superduper.base.document import Document\n",
    "dataset_name = \"philschmid/dolly-15k-oai-style\"\n",
    "dataset = load_dataset(dataset_name)['train'].train_test_split(0.9)\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n",
    "\n",
    "train_documents = [\n",
    "    Document({**example, \"_fold\": \"train\"})\n",
    "    for example in train_dataset\n",
    "]\n",
    "eval_documents = [\n",
    "    Document({**example, \"_fold\": \"valid\"})\n",
    "    for example in eval_dataset\n",
    "]\n",
    "\n",
    "datas = train_documents + eval_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361a4705-e7d6-4244-9150-bfa8372f85ba",
   "metadata": {},
   "source": [
    "We can define different training parameters to handle this type of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c824212e-0c4f-4b93-b3fa-4d2a105fc655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Text>\n",
    "# Function for transformation after extracting data from the database\n",
    "transform = None\n",
    "key = ('text')\n",
    "training_kwargs=dict(dataset_text_field=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2d583a-a0f3-432d-b737-356ab3cd4378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Prompt-Response>\n",
    "# Function for transformation after extracting data from the database\n",
    "def transform(prompt, response):\n",
    "    return {'text': prompt + response + \"</s>\"}\n",
    "\n",
    "key = ('prompt', 'response')\n",
    "training_kwargs=dict(dataset_text_field=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225cdb09-d060-4d45-bcf3-cae92fb22ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Chat>\n",
    "# Function for transformation after extracting data from the database\n",
    "transform = None\n",
    "\n",
    "key = ('messages')\n",
    "training_kwargs=None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7a1ec0-bf28-4b59-8be1-e7bcfd4eeccc",
   "metadata": {},
   "source": [
    "Example input_text and output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eb8c36c-97f8-40f4-8b8d-736d55352138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: --------------\n",
      "### Human: Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.### Assistant: \n",
      "Response: --------------\n",
      "\"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions.\n",
      "\n",
      "Recent research has identified potential monopsonies in industries such as retail and fast food, where a few large companies control a significant portion of the market (Bivens & Mishel, 2013). In these industries, workers often face low wages, limited benefits, and reduced bargaining power, leading to a situation where they are dependent on the employer for their livelihood. This dependence can result in further suppression of wages and a decline in working conditions.\n",
      "\n",
      "Overall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue.\n",
      "\n",
      "References:\n",
      "Bivens, J., & Mishel, L. (2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.\n"
     ]
    }
   ],
   "source": [
    "# <tab: Text>\n",
    "data = datas[0]\n",
    "input_text, output_text = data[\"text\"].rsplit(\"### Assistant: \", maxsplit=1)\n",
    "input_text += \"### Assistant: \"\n",
    "output_text = output_text.rsplit(\"### Human:\")[0]\n",
    "print(\"Input: --------------\")\n",
    "print(input_text)\n",
    "print(\"Response: --------------\")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbef4a4-478d-43f1-8f40-b3b1e5923639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Prompt-Response>\n",
    "data = datas[0]\n",
    "input_text = data[\"prompt\"]\n",
    "output_text = data[\"response\"]\n",
    "print(\"Input: --------------\")\n",
    "print(input_text)\n",
    "print(\"Response: --------------\")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983e8612-9c58-4688-a3af-8c408f9b3063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Chat>\n",
    "data = datas[0]\n",
    "messages = data[\"messages\"]\n",
    "input_text = messages[:-1]\n",
    "output_text = messages[-1][\"content\"]\n",
    "print(\"Input: --------------\")\n",
    "print(input_text)\n",
    "print(\"Response: --------------\")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027bc0ba",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "## Insert simple data\n",
    "\n",
    "After turning on auto_schema, we can directly insert data, and superduper will automatically analyze the data type, and match the construction of the table and datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdbdf015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-Aug-30 21:53:39.13\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m363 \u001b[0m | \u001b[1mTable docs does not exist, auto creating...\u001b[0m\n",
      "\u001b[32m2024-Aug-30 21:53:39.13\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m369 \u001b[0m | \u001b[1mCreating table docs with schema {('_fold', 'str'), ('text', 'str')}\u001b[0m\n",
      "\u001b[32m2024-Aug-30 21:53:39.13\u001b[0m| \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.document\u001b[0m:\u001b[36m415 \u001b[0m | \u001b[33m\u001b[1mLeaf str already exists\u001b[0m\n",
      "\u001b[32m2024-Aug-30 21:53:40.86\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m344 \u001b[0m | \u001b[1mInserted 10364 documents into docs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from superduper import Document\n",
    "\n",
    "table_or_collection = db['docs']\n",
    "\n",
    "ids = db.execute(table_or_collection.insert([Document(data) for data in datas]))\n",
    "select = table_or_collection.select()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a46a283-4fef-40d8-8df4-7023479ec2dd",
   "metadata": {},
   "source": [
    "## Select a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2ef0e1e-22ea-4af3-b914-1a3eed23a754",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/opt-125m\"\n",
    "model_kwargs = dict()\n",
    "tokenizer_kwargs = dict()\n",
    "\n",
    "# or \n",
    "# model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# token = \"hf_xxxx\"\n",
    "# model_kwargs = dict(token=token)\n",
    "# tokenizer_kwargs = dict(token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf339d2-6eaa-4a90-8718-6d6e6120c400",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "## Build A Trainable LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6c2662-ce55-4767-8e3d-ef3901fd31ee",
   "metadata": {},
   "source": [
    "**Create an LLM Trainer for training**\n",
    "\n",
    "The parameters of this LLM Trainer are basically the same as `transformers.TrainingArguments`, but some additional parameters have been added for easier training setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5deed34b-189c-4662-8972-aed92718225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper_transformers import LLM, LLMTrainer\n",
    "\n",
    "trainer = LLMTrainer(\n",
    "    identifier=\"llm-finetune-trainer\",\n",
    "    output_dir=\"output/finetune\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    max_seq_length=512,\n",
    "    key=key,\n",
    "    select=select,\n",
    "    transform=transform,\n",
    "    training_kwargs=training_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089bc70f-00e0-4a13-a438-658146efd4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Lora>\n",
    "trainer.use_lora = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ab34bc-999c-4300-aa47-d40a78536d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: QLora>\n",
    "trainer.use_lora = True\n",
    "trainer.bits = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1392ffec-80aa-4be7-b40f-665c2803e980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Deepspeed>\n",
    "!pip install deepspeed\n",
    "deepspeed = {\n",
    "    \"train_batch_size\": \"auto\",\n",
    "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "    \"gradient_accumulation_steps\": \"auto\",\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 2,\n",
    "    },\n",
    "}\n",
    "trainer.use_lora = True\n",
    "trainer.bits = 4\n",
    "trainer.deepspeed = deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f414fc61-2466-4bf6-8ea2-460524806880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Multi-GPUS>\n",
    "trainer.use_lora = True\n",
    "trainer.bits = 4\n",
    "trainer.num_gpus = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a370f6c8-ce2f-443e-ae89-a4e95c4375a8",
   "metadata": {},
   "source": [
    "Create a trainable LLM model and add it to the database, then the training task will run automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "400c1ebb-345e-4030-9f9e-96099f53664c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-Aug-30 21:53:51.59\u001b[0m| \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.document\u001b[0m:\u001b[36m415 \u001b[0m | \u001b[33m\u001b[1mLeaf dill already exists\u001b[0m\n",
      "\u001b[32m2024-Aug-30 21:53:51.59\u001b[0m| \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.backends.local.artifacts\u001b[0m:\u001b[36m85  \u001b[0m | \u001b[33m\u001b[1mFile /tmp/test_db/4a8dc14137b3a79a81256a795b266fe82bda52d9 already exists\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['36869320-20a5-4cb3-ac4d-d40c146146ca'],\n",
       " LLM(trainer=LLMTrainer(identifier='llm-finetune-trainer', uuid='d6c6390a57a4446797bb5250549c7077', upstream=None, plugins=None, cache=False, key='text', select=docs.select(), transform=None, metric_values={}, signature='*args', data_prefetch=False, prefetch_size=1000, prefetch_factor=100, in_memory=True, compute_kwargs={}, output_dir='output/finetune', overwrite_output_dir=True, do_train=False, do_eval=False, do_predict=False, eval_strategy='no', prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=2, eval_accumulation_steps=None, eval_delay=0, torch_empty_cache_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3, max_steps=-1, lr_scheduler_type='linear', lr_scheduler_kwargs={}, warmup_ratio=0.0, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir=None, logging_strategy='steps', logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy='steps', save_steps=100, save_total_limit=3, save_safetensors=True, save_on_each_node=False, save_only_model=False, restore_callback_states_from_checkpoint=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=-1, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug='', dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, dataloader_prefetch_factor=None, past_index=-1, run_name=None, disable_tqdm=None, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp='', fsdp_min_num_params=0, fsdp_config=None, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=None, deepspeed=None, label_smoothing_factor=0.0, optim='adamw_torch', optim_args=None, adafactor=False, group_by_length=False, length_column_name='length', report_to=None, ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy='every_save', hub_token=None, hub_private_repo=False, hub_always_push=False, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, eval_do_concat_batches=True, fp16_backend='auto', evaluation_strategy='steps', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, dispatch_batches=None, split_batches=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, optim_target_modules=None, batch_eval_metrics=False, eval_on_start=False, eval_use_gather_object=False, use_lora=True, lora_r=8, lora_alpha=16, lora_dropout=0.05, lora_target_modules='all-linear', lora_bias='none', bits=None, max_seq_length=512, setup_chat_format=False, log_to_db=True, training_kwargs={'dataset_text_field': 'text'}, num_gpus=None, ray_configs=None), identifier='llm', uuid='6f31093e26b449cf990368893d5393c1', upstream=None, plugins=None, cache=False, signature='singleton', datatype=None, output_schema=None, flatten=False, model_update_kwargs={}, predict_kwargs={}, compute_kwargs={}, validation=None, metric_values={}, num_workers=0, prompt='{input}', prompt_func=None, max_batch_size=4, model_name_or_path='facebook/opt-125m', adapter_id=None, model_kwargs={}, tokenizer_kwargs={}, prompt_template='{input}'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = LLM(\n",
    "    identifier=\"llm\",\n",
    "    model_name_or_path=model_name,\n",
    "    trainer=trainer,\n",
    "    model_kwargs=model_kwargs,\n",
    "    tokenizer_kwargs=tokenizer_kwargs,\n",
    ")\n",
    "\n",
    "db.apply(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edd846d-aa81-456f-b2ea-fc2d230a41a2",
   "metadata": {},
   "source": [
    "## Load the trained model\n",
    "There are two methods to load a trained model:\n",
    "\n",
    "- **Load the model directly**: This will load the model with the best metrics (if the transformers' best model save strategy is set) or the last version of the model.\n",
    "- **Use a specified checkpoint**: This method downloads the specified checkpoint, then initializes the base model, and finally merges the checkpoint with the base model. This approach supports custom operations such as resetting flash_attentions, model quantization, etc., during initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db2e1a0d-c760-4a01-b4bf-c6e83296ca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Load Trained Model Directly>\n",
    "llm = db.load(\"model\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5fbf38-8f2b-4c3f-9ae8-c184001b4495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Use a specified checkpoint>\n",
    "from superduper_transformers import LLM\n",
    "\n",
    "experiment_id = db.show(\"checkpoint\")[-1]\n",
    "version = None # None means the last checkpoint\n",
    "checkpoint = db.load(\"checkpoint\", experiment_id, version=version)\n",
    "llm = LLM(\n",
    "    identifier=\"llm\",\n",
    "    model_name_or_path=model_name,\n",
    "    adapter_id=checkpoint,\n",
    "    model_kwargs=dict(load_in_4bit=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60b8933b-723d-481c-9d6e-dff14d256377",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dodo/.pyenv/versions/3.11.7/envs/superduper-3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'                                                                                                                                                                                                        '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict(input_text, max_new_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa77eb56-5aa4-4d5a-a285-6724c936bd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-Aug-30 21:54:29.36\u001b[0m| \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.document\u001b[0m:\u001b[36m415 \u001b[0m | \u001b[33m\u001b[1mLeaf dill already exists\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from superduper import Template\n",
    "\n",
    "t = Template('llm-finetune', template=llm, substitutions={'docs': 'collection', model_name: 'model_name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e65e57ff-6355-4685-93de-ff6f9ad0b7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.export('.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
