{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d92f42b-7160-4965-a0ef-5e4ce46bd529",
   "metadata": {},
   "source": [
    "# Fine tune LLM on database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f8484d-2e35-472a-9b24-1a30ec1d144b",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "## Connect to superduper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d66021-ce62-4021-a2c5-158dee92b3bb",
   "metadata": {},
   "source": [
    ":::note\n",
    "Note that this is only relevant if you are running superduper in development mode.\n",
    "Otherwise refer to \"Configuring your production system\".\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b39819c-2bc3-49a3-b285-d13b6ccc1242",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "APPLY = True\n",
    "TABLE_NAME = 'sample_llm_finetuning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb029a5e-fedf-4f07-8a31-d220cfbfbb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-Jan-13 13:15:39.10\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.misc.plugins\u001b[0m:\u001b[36m13  \u001b[0m | \u001b[1mLoading plugin: mongodb\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:39.19\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m64  \u001b[0m | \u001b[1mBuilding Data Layer\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:39.19\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m79  \u001b[0m | \u001b[1mData Layer built\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:39.19\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.backends.base.cluster\u001b[0m:\u001b[36m99  \u001b[0m | \u001b[1mCluster initialized in 0.00 seconds.\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:39.19\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.build\u001b[0m:\u001b[36m184 \u001b[0m | \u001b[1mConfiguration: \n",
      " +---------------+---------------------+\n",
      "| Configuration |        Value        |\n",
      "+---------------+---------------------+\n",
      "|  Data Backend | mongomock://test_db |\n",
      "+---------------+---------------------+\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from superduper import superduper\n",
    "\n",
    "db = superduper('mongomock://test_db', force_apply=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032c2e7b-3f54-4263-b778-0fef60596efb",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "## Get LLM Finetuning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f5169e-ab2f-4eac-bd3f-30fd845f2a1b",
   "metadata": {},
   "source": [
    "The following are examples of training data in different formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b37c7dc-390a-428b-916a-09d191678cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from superduper.base.document import Document\n",
    "\n",
    "def getter():\n",
    "\n",
    "    dataset_name = \"timdettmers/openassistant-guanaco\"\n",
    "    dataset = load_dataset(dataset_name)\n",
    "    \n",
    "    train_dataset = dataset[\"train\"]\n",
    "    eval_dataset = dataset[\"test\"]\n",
    "    \n",
    "    train_documents = [{**example, \"_fold\": \"train\"} for example in train_dataset][:10]\n",
    "    eval_documents = [{**example, \"_fold\": \"valid\"} for example in eval_dataset][:5]\n",
    "    \n",
    "    data = train_documents + eval_documents\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cef3a6a1-732c-4896-b00b-8624876533a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "if APPLY:\n",
    "    data = getter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361a4705-e7d6-4244-9150-bfa8372f85ba",
   "metadata": {},
   "source": [
    "We can define different training parameters to handle this type of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c824212e-0c4f-4b93-b3fa-4d2a105fc655",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = None\n",
    "key = ('text')\n",
    "training_kwargs=dict(dataset_text_field=\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7a1ec0-bf28-4b59-8be1-e7bcfd4eeccc",
   "metadata": {},
   "source": [
    "Example input_text and output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eb8c36c-97f8-40f4-8b8d-736d55352138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: --------------\n",
      "### Human: Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.### Assistant: \n",
      "Response: --------------\n",
      "\"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions.\n",
      "\n",
      "Recent research has identified potential monopsonies in industries such as retail and fast food, where a few large companies control a significant portion of the market (Bivens & Mishel, 2013). In these industries, workers often face low wages, limited benefits, and reduced bargaining power, leading to a situation where they are dependent on the employer for their livelihood. This dependence can result in further suppression of wages and a decline in working conditions.\n",
      "\n",
      "Overall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue.\n",
      "\n",
      "References:\n",
      "Bivens, J., & Mishel, L. (2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.\n"
     ]
    }
   ],
   "source": [
    "if APPLY:\n",
    "    d = data[0]\n",
    "    input_text, output_text = d[\"text\"].rsplit(\"### Assistant: \", maxsplit=1)\n",
    "    input_text += \"### Assistant: \"\n",
    "    output_text = output_text.rsplit(\"### Human:\")[0]\n",
    "    print(\"Input: --------------\")\n",
    "    print(input_text)\n",
    "    print(\"Response: --------------\")\n",
    "    print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027bc0ba",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "## Insert simple data\n",
    "\n",
    "After turning on auto_schema, we can directly insert data, and superduper will automatically analyze the data type, and match the construction of the table and datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdbdf015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-Jan-13 13:15:41.75\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m593 \u001b[0m | \u001b[1mComponent (table, sample_llm_finetuning) not found in cache, loading from db\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:41.75\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m599 \u001b[0m | \u001b[1mLoad (('table', 'sample_llm_finetuning')) from metadata...\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:41.75\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m331 \u001b[0m | \u001b[1mTable sample_llm_finetuning does not exist, auto creating...\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:42.38\u001b[0m| \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.misc.annotations\u001b[0m:\u001b[36m296 \u001b[0m | \u001b[33m\u001b[1m`superduper.ext.torch` is deprecated and will be removed in a future release. Please insteall `superduper_torch` and use `from superduper_torch import *` instead.\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:42.40\u001b[0m| \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.misc.annotations\u001b[0m:\u001b[36m296 \u001b[0m | \u001b[33m\u001b[1m`superduper.ext.pillow` is deprecated and will be removed in a future release. Please insteall `superduper_pillow` and use `from superduper_pillow import *` instead.\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:42.40\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m337 \u001b[0m | \u001b[1mCreating table sample_llm_finetuning with schema {('text', 'str'), ('_fold', 'str')}\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:42.40\u001b[0m| \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.document\u001b[0m:\u001b[36m564 \u001b[0m | \u001b[33m\u001b[1mLeaf str already exists\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:42.40\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m593 \u001b[0m | \u001b[1mComponent (schema, AUTO-_fold=<class 'str'>&text=<class 'str'>) not found in cache, loading from db\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:42.40\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m599 \u001b[0m | \u001b[1mLoad (('schema', \"AUTO-_fold=<class 'str'>&text=<class 'str'>\")) from metadata...\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:42.40\u001b[0m| \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.document\u001b[0m:\u001b[36m564 \u001b[0m | \u001b[33m\u001b[1mLeaf str already exists\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:42.40\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.apply\u001b[0m:\u001b[36m359 \u001b[0m | \u001b[1mFound new schema:AUTO-_fold=<class 'str'>&text=<class 'str'>:5972149b87f84fba\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:42.40\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m593 \u001b[0m | \u001b[1mComponent (table, sample_llm_finetuning) not found in cache, loading from db\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:42.40\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m599 \u001b[0m | \u001b[1mLoad (('table', 'sample_llm_finetuning')) from metadata...\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:42.40\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.apply\u001b[0m:\u001b[36m359 \u001b[0m | \u001b[1mFound new table:sample_llm_finetuning:3e5b7e20cc0d4de7\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:42.40\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.apply\u001b[0m:\u001b[36m79  \u001b[0m | \u001b[1mFound these changes and/ or additions that need to be made:\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:42.40\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.apply\u001b[0m:\u001b[36m81  \u001b[0m | \u001b[1m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:42.40\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.apply\u001b[0m:\u001b[36m82  \u001b[0m | \u001b[1mMETADATA EVENTS:\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:42.40\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.apply\u001b[0m:\u001b[36m83  \u001b[0m | \u001b[1m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:42.40\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.apply\u001b[0m:\u001b[36m90  \u001b[0m | \u001b[1m[0]: schema:AUTO-_fold=<class 'str'>&text=<class 'str'>:5972149b87f84fba: create ~ [1]\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:42.40\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.apply\u001b[0m:\u001b[36m94  \u001b[0m | \u001b[1m[1]: table:sample_llm_finetuning:3e5b7e20cc0d4de7: create\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:42.40\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.apply\u001b[0m:\u001b[36m96  \u001b[0m | \u001b[1m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:42.40\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.apply\u001b[0m:\u001b[36m97  \u001b[0m | \u001b[1mJOBS EVENTS:\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:42.40\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.apply\u001b[0m:\u001b[36m98  \u001b[0m | \u001b[1m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:42.40\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.apply\u001b[0m:\u001b[36m102 \u001b[0m | \u001b[1mNo job events...\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:42.40\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.apply\u001b[0m:\u001b[36m113 \u001b[0m | \u001b[1m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:42.40\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m562 \u001b[0m | \u001b[1mComponent 5972149b87f84fba not found in cache, loading from db with uuid\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:42.40\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m584 \u001b[0m | \u001b[1mAdding schema:AUTO-_fold=<class 'str'>&text=<class 'str'>:5972149b87f84fba to cache\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:42.40\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.components.component\u001b[0m:\u001b[36m620 \u001b[0m | \u001b[1mAdding schema: AUTO-_fold=<class 'str'>&text=<class 'str'> to cache\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:42.40\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m562 \u001b[0m | \u001b[1mComponent 3e5b7e20cc0d4de7 not found in cache, loading from db with uuid\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:42.40\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m584 \u001b[0m | \u001b[1mAdding table:sample_llm_finetuning:3e5b7e20cc0d4de7 to cache\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:42.40\u001b[0m| \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.backends.local.compute\u001b[0m:\u001b[36m52  \u001b[0m | \u001b[33m\u001b[1mCould not release futures for context 3e5b7e20cc0d4de7\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:42.40\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.backends.local.queue\u001b[0m:\u001b[36m120 \u001b[0m | \u001b[1mConsumed all events\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:42.41\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m310 \u001b[0m | \u001b[1mInserted 15 documents into sample_llm_finetuning\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "table_or_collection = db[TABLE_NAME]\n",
    "\n",
    "if APPLY:\n",
    "    table_or_collection.insert(data).execute()\n",
    "\n",
    "select = table_or_collection.select()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a46a283-4fef-40d8-8df4-7023479ec2dd",
   "metadata": {},
   "source": [
    "## Select a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2ef0e1e-22ea-4af3-b914-1a3eed23a754",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "model_kwargs = dict()\n",
    "tokenizer_kwargs = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf339d2-6eaa-4a90-8718-6d6e6120c400",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "## Build A Trainable LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6c2662-ce55-4767-8e3d-ef3901fd31ee",
   "metadata": {},
   "source": [
    "**Create an LLM Trainer for training**\n",
    "\n",
    "The parameters of this LLM Trainer are basically the same as `transformers.TrainingArguments`, but some additional parameters have been added for easier training setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5deed34b-189c-4662-8972-aed92718225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper_transformers import LLM, LLMTrainer\n",
    "\n",
    "trainer = LLMTrainer(\n",
    "    identifier=\"llm-finetune-trainer\",\n",
    "    output_dir=\"output/finetune\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,\n",
    "    save_total_limit=10,\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    key=key,\n",
    "    select=select,\n",
    "    transform=transform,\n",
    "    training_kwargs=training_kwargs,\n",
    "    use_lora=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a370f6c8-ce2f-443e-ae89-a4e95c4375a8",
   "metadata": {},
   "source": [
    "Create a trainable LLM model and add it to the database, then the training task will run automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "400c1ebb-345e-4030-9f9e-96099f53664c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(\n",
    "    identifier=\"llm\",\n",
    "    model_name_or_path=model_name,\n",
    "    trainer=trainer,\n",
    "    model_kwargs=model_kwargs,\n",
    "    tokenizer_kwargs=tokenizer_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cb0dd6a-2bc8-4288-9b99-49e1ab214965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-Jan-13 13:15:43.70\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m593 \u001b[0m | \u001b[1mComponent (trainer, llm-finetune-trainer) not found in cache, loading from db\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:43.70\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m599 \u001b[0m | \u001b[1mLoad (('trainer', 'llm-finetune-trainer')) from metadata...\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:43.70\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.apply\u001b[0m:\u001b[36m359 \u001b[0m | \u001b[1mFound new trainer:llm-finetune-trainer:43d4b6d421394594\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:43.70\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m593 \u001b[0m | \u001b[1mComponent (model, llm) not found in cache, loading from db\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:43.70\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m599 \u001b[0m | \u001b[1mLoad (('model', 'llm')) from metadata...\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:43.70\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.apply\u001b[0m:\u001b[36m359 \u001b[0m | \u001b[1mFound new model:llm:42d4b7ba8f204ee6\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:43.70\u001b[0m| \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.backends.local.artifacts\u001b[0m:\u001b[36m87  \u001b[0m | \u001b[33m\u001b[1mFile /tmp/test_db/4a8dc14137b3a79a81256a795b266fe82bda52d9 already exists\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:43.70\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.apply\u001b[0m:\u001b[36m79  \u001b[0m | \u001b[1mFound these changes and/ or additions that need to be made:\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:43.70\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.apply\u001b[0m:\u001b[36m81  \u001b[0m | \u001b[1m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:43.70\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.apply\u001b[0m:\u001b[36m82  \u001b[0m | \u001b[1mMETADATA EVENTS:\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:43.70\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.apply\u001b[0m:\u001b[36m83  \u001b[0m | \u001b[1m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:43.70\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.apply\u001b[0m:\u001b[36m90  \u001b[0m | \u001b[1m[0]: trainer:llm-finetune-trainer:43d4b6d421394594: create ~ [1]\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:43.70\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.apply\u001b[0m:\u001b[36m94  \u001b[0m | \u001b[1m[1]: model:llm:42d4b7ba8f204ee6: create\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:43.70\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.apply\u001b[0m:\u001b[36m96  \u001b[0m | \u001b[1m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:43.70\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.apply\u001b[0m:\u001b[36m97  \u001b[0m | \u001b[1mJOBS EVENTS:\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:43.70\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.apply\u001b[0m:\u001b[36m98  \u001b[0m | \u001b[1m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:43.70\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.apply\u001b[0m:\u001b[36m111 \u001b[0m | \u001b[1m[0]: model:llm:42d4b7ba8f204ee6.fit_in_db: fit_in_db\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:43.70\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.apply\u001b[0m:\u001b[36m106 \u001b[0m | \u001b[1m[1]: model:llm:42d4b7ba8f204ee6.set_status: set_status ~ [0]\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:43.70\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.apply\u001b[0m:\u001b[36m113 \u001b[0m | \u001b[1m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:43.70\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m562 \u001b[0m | \u001b[1mComponent 43d4b6d421394594 not found in cache, loading from db with uuid\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:43.72\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m584 \u001b[0m | \u001b[1mAdding trainer:llm-finetune-trainer:43d4b6d421394594 to cache\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:43.72\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.components.component\u001b[0m:\u001b[36m620 \u001b[0m | \u001b[1mAdding trainer: llm-finetune-trainer to cache\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:43.72\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m562 \u001b[0m | \u001b[1mComponent 42d4b7ba8f204ee6 not found in cache, loading from db with uuid\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:43.72\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m584 \u001b[0m | \u001b[1mAdding model:llm:42d4b7ba8f204ee6 to cache\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:43.72\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.components.component\u001b[0m:\u001b[36m620 \u001b[0m | \u001b[1mAdding model: llm to cache\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-Jan-13 13:15:45.01\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper_transformers.training\u001b[0m:\u001b[36m284 \u001b[0m | \u001b[1mStart training, experiment_id: f38a3a5dcec325a3f02d9b31e476e9fb1f77c6f5\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:45.02\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper_transformers.training\u001b[0m:\u001b[36m487 \u001b[0m | \u001b[1mStart training LLM model\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:45.02\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper_transformers.training\u001b[0m:\u001b[36m488 \u001b[0m | \u001b[1mtraining_args: LLMTrainer(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "bits=None,\n",
      "build_template=None,\n",
      "build_variables=None,\n",
      "cache=True,\n",
      "compute_kwargs={},\n",
      "data_prefetch=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=100,\n",
      "eval_strategy=steps,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=steps,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=2,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "identifier=llm-finetune-trainer,\n",
      "ignore_data_skip=False,\n",
      "in_memory=True,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "key=text,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "log_to_db=True,\n",
      "logging_dir=output/finetune/runs/Jan13_13-15-45_Duncans-MBP.fritz.box,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=1,\n",
      "logging_strategy=steps,\n",
      "lora_alpha=16,\n",
      "lora_bias=none,\n",
      "lora_dropout=0.05,\n",
      "lora_r=8,\n",
      "lora_target_modules=all-linear,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_seq_length=512,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "metric_values={},\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_gpus=None,\n",
      "num_train_epochs=2,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=output/finetune,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "plugins=None,\n",
      "prediction_loss_only=False,\n",
      "prefetch_factor=100,\n",
      "prefetch_size=1000,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_configs=None,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=output/finetune,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=100,\n",
      "save_strategy=steps,\n",
      "save_total_limit=10,\n",
      "seed=42,\n",
      "select={'identifier': 'sample-llm-finetuning-select', 'uuid': 'c5f265b81a1f4660', 'parts': [('select', (), {})], 'table': 'sample_llm_finetuning'},\n",
      "setup_chat_format=False,\n",
      "signature=*args,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "status=ready,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "training_kwargs={'dataset_text_field': 'text'},\n",
      "transform=None,\n",
      "upstream=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_lora=True,\n",
      "use_mps_device=False,\n",
      "uuid=43d4b6d421394594,\n",
      "validation=None,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:45.02\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper_transformers.training\u001b[0m:\u001b[36m509 \u001b[0m | \u001b[1mOverwriting model_kwargs for LLM training\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:45.02\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper_transformers.training\u001b[0m:\u001b[36m510 \u001b[0m | \u001b[1mquantization_config: None\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:45.02\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper_transformers.training\u001b[0m:\u001b[36m511 \u001b[0m | \u001b[1mdevice_map: mps\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:45.02\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper_transformers.training\u001b[0m:\u001b[36m515 \u001b[0m | \u001b[1mmodel_kwargs: {'pretrained_model_name_or_path': 'Qwen/Qwen2.5-0.5B', 'quantization_config': None, 'device_map': 'mps'}\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dodo/.pyenv/versions/3.10.13/envs/superduper-3.10/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-Jan-13 13:15:46.09\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper_transformers.training\u001b[0m:\u001b[36m519 \u001b[0m | \u001b[1mtokenizer_kwargs: %s {'pretrained_model_name_or_path': 'Qwen/Qwen2.5-0.5B'}\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:46.41\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper_transformers.training\u001b[0m:\u001b[36m534 \u001b[0m | \u001b[1mPreparing LoRA training\u001b[0m\n",
      "trainable params: 4,399,104 || all params: 498,431,872 || trainable%: 0.8826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dodo/.pyenv/versions/3.10.13/envs/superduper-3.10/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '0.13.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/Users/dodo/.pyenv/versions/3.10.13/envs/superduper-3.10/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/Users/dodo/.pyenv/versions/3.10.13/envs/superduper-3.10/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ba758c1e414aaeab366bfde64861c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c54582a15b4739911173c429dafc5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-Jan-13 13:15:46.68\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper_transformers.training\u001b[0m:\u001b[36m550 \u001b[0m | \u001b[1mAdd callback <superduper_transformers.training.LLMCallback object at 0x2aa392380>\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:08, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-Jan-13 13:15:57.29\u001b[0m| \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.backends.local.artifacts\u001b[0m:\u001b[36m87  \u001b[0m | \u001b[33m\u001b[1mFile /tmp/test_db/4a8dc14137b3a79a81256a795b266fe82bda52d9 already exists\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:57.31\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m562 \u001b[0m | \u001b[1mComponent 42d4b7ba8f204ee6 not found in cache, loading from db with uuid\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:57.31\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m562 \u001b[0m | \u001b[1mComponent 43d4b6d421394594 not found in cache, loading from db with uuid\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:57.33\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m584 \u001b[0m | \u001b[1mAdding trainer:llm-finetune-trainer:43d4b6d421394594 to cache\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:57.33\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m562 \u001b[0m | \u001b[1mComponent c7450d30113d4fd0 not found in cache, loading from db with uuid\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:57.33\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m584 \u001b[0m | \u001b[1mAdding checkpoint:f38a3a5dcec325a3f02d9b31e476e9fb1f77c6f5:c7450d30113d4fd0 to cache\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:57.33\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m584 \u001b[0m | \u001b[1mAdding model:llm:42d4b7ba8f204ee6 to cache\u001b[0m\n",
      "\u001b[32m2025-Jan-13 13:15:58.46\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper_transformers.model\u001b[0m:\u001b[36m347 \u001b[0m | \u001b[1mLoading adapter from output/finetune/checkpoint-10\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-Jan-13 13:15:59.15\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.backends.local.queue\u001b[0m:\u001b[36m120 \u001b[0m | \u001b[1mConsumed all events\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if APPLY:\n",
    "    db.apply(llm, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3440967-bd90-40d7-b78d-caab78f9472a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper import Template, Table, Schema, Application\n",
    "from superduper.components.dataset import RemoteData\n",
    "\n",
    "llm.trainer.use_lora = \"<var:use_lora>\"\n",
    "llm.trainer.num_train_epochs = \"<var:num_train_epochs>\"\n",
    "\n",
    "app = Application(identifier=\"llm\", components=[llm])\n",
    "\n",
    "t = Template(\n",
    "    'llm-finetune',\n",
    "    template=app,\n",
    "    substitutions={\n",
    "        TABLE_NAME: 'table_name',\n",
    "        model_name: 'model_name',\n",
    "    },\n",
    "    default_table=Table(\n",
    "        'sample_llm_finetuning',\n",
    "        schema=Schema(\n",
    "            'sample_llm_finetuning/schema',\n",
    "            fields={'x': 'str', 'y': 'int'},\n",
    "        ),\n",
    "        data=RemoteData(\n",
    "            'llm_finetuning',\n",
    "            getter=getter,\n",
    "        ),\n",
    "    ),\n",
    "    template_variables=['table_name', 'model_name', 'use_lora', 'num_train_epochs'],\n",
    "    types={\n",
    "        'collection': {\n",
    "            'type': 'str',\n",
    "            'default': 'dataset',\n",
    "        },\n",
    "        'model_name': {\n",
    "            'type': 'str',\n",
    "            'default': 'Qwen/Qwen2.5-0.5B',\n",
    "        },\n",
    "        'use_lora': {\n",
    "            'type': 'bool',\n",
    "            'default': True,\n",
    "        },\n",
    "        'num_train_epochs': {\n",
    "            'type': 'int',\n",
    "            'default': 3\n",
    "        },\n",
    "        'table_name': {\n",
    "            'type': 'str',\n",
    "            'default': 'sample_llm_finetuning',\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "t.export('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edd846d-aa81-456f-b2ea-fc2d230a41a2",
   "metadata": {},
   "source": [
    "## Load the trained model\n",
    "There are two methods to load a trained model:\n",
    "\n",
    "- **Load the model directly**: This will load the model with the best metrics (if the transformers' best model save strategy is set) or the last version of the model.\n",
    "- **Use a specified checkpoint**: This method downloads the specified checkpoint, then initializes the base model, and finally merges the checkpoint with the base model. This approach supports custom operations such as resetting flash_attentions, model quantization, etc., during initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2e1a0d-c760-4a01-b4bf-c6e83296ca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if APPLY:\n",
    "    llm = db.load(\"model\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b8933b-723d-481c-9d6e-dff14d256377",
   "metadata": {},
   "outputs": [],
   "source": [
    "if APPLY:\n",
    "    llm.predict(input_text, max_new_tokens=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
