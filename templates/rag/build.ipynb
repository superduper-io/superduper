{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38c1a328-fd86-4c5f-bd54-b8664f433608",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "# Retrieval augmented generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f8484d-2e35-472a-9b24-1a30ec1d144b",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "## Connect to superduper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d66021-ce62-4021-a2c5-158dee92b3bb",
   "metadata": {},
   "source": [
    ":::note\n",
    "Note that this is only relevant if you are running superduper in development mode.\n",
    "Otherwise refer to \"Configuring your production system\".\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb029a5e-fedf-4f07-8a31-d220cfbfbb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-Oct-08 14:04:03.04\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.misc.plugins\u001b[0m:\u001b[36m13  \u001b[0m | \u001b[1mLoading plugin: mongodb\u001b[0m\n",
      "\u001b[32m2024-Oct-08 14:04:03.07\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m103 \u001b[0m | \u001b[1mBuilding Data Layer\u001b[0m\n",
      "\u001b[32m2024-Oct-08 14:04:03.07\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.base.build\u001b[0m:\u001b[36m173 \u001b[0m | \u001b[1mConfiguration: \n",
      " +---------------+----------------------+\n",
      "| Configuration |        Value         |\n",
      "+---------------+----------------------+\n",
      "|  Data Backend | mongomock:///test_db |\n",
      "+---------------+----------------------+\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from superduper import superduper\n",
    "\n",
    "db = superduper('mongomock:///test_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7abae7f7-ad08-4dea-8198-626bb9e0f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_NAME = 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032c2e7b-3f54-4263-b778-0fef60596efb",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "## Get useful sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e7902bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Text>\n",
    "import json\n",
    "\n",
    "with open('data.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33486ec7-0316-4e0c-a409-c09ab4c16669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: PDF>\n",
    "!curl -O https://superduperdb-public-demo.s3.amazonaws.com/pdfs.zip && unzip -o pdfs.zip\n",
    "import os\n",
    "\n",
    "data = [f'pdfs/{x}' for x in os.listdir('./pdfs') if x.endswith('.pdf')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b745ed54-3818-4685-a3b5-6ab4e2afc44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = [{'x': d} for d in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ede8ae1",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "## Insert simple data\n",
    "\n",
    "After turning on auto_schema, we can directly insert data, and superduper will automatically analyze the data type, and match the construction of the table and datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5965fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-Oct-08 14:10:23.61\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m365 \u001b[0m | \u001b[1mTable data does not exist, auto creating...\u001b[0m\n",
      "\u001b[32m2024-Oct-08 14:10:23.61\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m371 \u001b[0m | \u001b[1mCreating table data with schema {('x', 'str'), ('_fold', 'str')}\u001b[0m\n",
      "\u001b[32m2024-Oct-08 14:10:23.61\u001b[0m| \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.base.document\u001b[0m:\u001b[36m452 \u001b[0m | \u001b[33m\u001b[1mLeaf str already exists\u001b[0m\n",
      "\u001b[32m2024-Oct-08 14:10:23.65\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m346 \u001b[0m | \u001b[1mInserted 209 documents into data\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from superduper import Document\n",
    "\n",
    "ids = db.execute(db[COLLECTION_NAME].insert([Document(data) for data in datas]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fea927-ee4a-44cd-aaf2-634b574c316d",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "## Apply a chunker for search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d90bda-e8c4-494e-a38c-837fb63689ae",
   "metadata": {},
   "source": [
    ":::note\n",
    "Note that applying a chunker is ***not*** mandatory for search.\n",
    "If your data is already chunked (e.g. short text snippets or audio) or if you\n",
    "are searching through something like images, which can't be chunked, then this\n",
    "won't be necessary.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d20eaa0-a416-4483-938e-23f79845739a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Text>\n",
    "from superduper import model\n",
    "\n",
    "CHUNK_SIZE = 200\n",
    "\n",
    "@model(flatten=True, model_update_kwargs={})\n",
    "def chunker(text):\n",
    "    text = text.split()\n",
    "    chunks = [' '.join(text[i:i + CHUNK_SIZE]) for i in range(0, len(text), CHUNK_SIZE)]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facd7dc0-fffa-40d8-af72-2b9e4852ad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: PDF>\n",
    "!pip install -q \"unstructured[pdf]\"\n",
    "from superduper import model\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "CHUNK_SIZE = 500\n",
    "\n",
    "@model(flatten=True)\n",
    "def chunker(pdf_file):\n",
    "    elements = partition_pdf(pdf_file)\n",
    "    text = '\\n'.join([e.text for e in elements])\n",
    "    chunks = [text[i:i + CHUNK_SIZE] for i in range(0, len(text), CHUNK_SIZE)]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33a16f9-3bac-45bb-80ac-3ccf265dce5f",
   "metadata": {},
   "source": [
    "Now we apply this chunker to the data by wrapping the chunker in `Listener`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93d21872-d4dc-40dc-abab-fb07ba102ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper import Listener\n",
    "\n",
    "upstream_listener = Listener(\n",
    "    model=chunker,\n",
    "    select=db[COLLECTION_NAME].select(),\n",
    "    key='x',\n",
    "    uuid=\"chunker\",\n",
    "    identifier='chunker',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5377c0-4c9b-4ba9-8f08-5e866b9220b5",
   "metadata": {},
   "source": [
    "## Select outputs of upstream listener"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809f5f62-95c3-483b-ae74-a5cdb5c1c83d",
   "metadata": {},
   "source": [
    ":::note\n",
    "This is useful if you have performed a first step, such as pre-computing \n",
    "features, or chunking your data. You can use this query to \n",
    "operate on those outputs.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a2cd87-723f-4cee-87c7-9b8181c9e54b",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "## Build text embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9b1f538-65ca-499e-b6d0-2dd733f81723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: OpenAI>\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = 'sk-<your-secret>'\n",
    "from superduper_openai import OpenAIEmbedding\n",
    "\n",
    "embedding_model = OpenAIEmbedding(identifier='text-embedding-ada-002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83facd8-8823-492f-a2c6-659f38d8e6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: JinaAI>\n",
    "import os\n",
    "from superduper_jina import JinaEmbedding\n",
    "\n",
    "os.environ[\"JINA_API_KEY\"] = \"jina_xxxx\"\n",
    " \n",
    "# define the model\n",
    "embedding_model = JinaEmbedding(identifier='jina-embeddings-v2-base-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4a9a60-41df-461d-b165-1d136ee25694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Sentence-Transformers>\n",
    "!pip install sentence-transformers\n",
    "from superduper import vector\n",
    "import sentence_transformers\n",
    "from superduper_sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(\n",
    "    identifier=\"embedding\",\n",
    "    object=sentence_transformers.SentenceTransformer(\"BAAI/bge-small-en\"),\n",
    "    datatype=vector(shape=(1024,)),\n",
    "    postprocess=lambda x: x.tolist(),\n",
    "    predict_kwargs={\"show_progress_bar\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31843db-8638-458a-a770-96a79041be88",
   "metadata": {},
   "source": [
    "## Create vector-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4663fa4b-c2ec-427d-bf8b-b8b109cc2ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper import VectorIndex, Listener\n",
    "\n",
    "vector_index_name = 'vector-index'\n",
    "\n",
    "vector_index = VectorIndex(\n",
    "    vector_index_name,\n",
    "    indexing_listener=Listener(\n",
    "        key=upstream_listener.outputs,\n",
    "        select=db[upstream_listener.outputs].select(),\n",
    "        model=embedding_model,\n",
    "        identifier='embedding-listener',\n",
    "        upstream=[upstream_listener],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "509c3505-54c5-4e68-84ec-3df8bea0fd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-Oct-08 14:15:11.75\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.components.listener\u001b[0m:\u001b[36m94  \u001b[0m | \u001b[1mRequesting listener setup on CDC service\u001b[0m\n",
      "\u001b[32m2024-Oct-08 14:15:11.75\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.components.listener\u001b[0m:\u001b[36m104 \u001b[0m | \u001b[1mSkipping listener setup on CDC service since no URI is set\u001b[0m\n",
      "\u001b[32m2024-Oct-08 14:15:11.75\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.jobs.queue\u001b[0m:\u001b[36m210 \u001b[0m | \u001b[1mRunning jobs for listener::chunker\u001b[0m\n",
      "\u001b[32m2024-Oct-08 14:15:11.75\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.backends.local.compute\u001b[0m:\u001b[36m67  \u001b[0m | \u001b[1mSubmitting job. function:<function method_job at 0x106c06e60>\u001b[0m\n",
      "\u001b[32m2024-Oct-08 14:15:11.76\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.components.model\u001b[0m:\u001b[36m732 \u001b[0m | \u001b[1mRequesting prediction in db - [chunker] with predict_id chunker\n",
      "\u001b[0m\n",
      "\u001b[32m2024-Oct-08 14:15:11.86\u001b[0m| \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.base.document\u001b[0m:\u001b[36m452 \u001b[0m | \u001b[33m\u001b[1mLeaf ID already exists\u001b[0m\n",
      "\u001b[32m2024-Oct-08 14:15:11.86\u001b[0m| \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.base.document\u001b[0m:\u001b[36m452 \u001b[0m | \u001b[33m\u001b[1mLeaf str already exists\u001b[0m\n",
      "\u001b[32m2024-Oct-08 14:15:11.87\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.components.model\u001b[0m:\u001b[36m862 \u001b[0m | \u001b[1mAdding 209 model outputs to `db`\u001b[0m\n",
      "\u001b[32m2024-Oct-08 14:15:11.93\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m346 \u001b[0m | \u001b[1mInserted 363 documents into _outputs__chunker\u001b[0m\n",
      "\u001b[32m2024-Oct-08 14:15:11.93\u001b[0m| \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.backends.local.compute\u001b[0m:\u001b[36m73  \u001b[0m | \u001b[32m\u001b[1mJob submitted on <superduper.backends.local.compute.LocalComputeBackend object at 0x16062be20>.  function:<function method_job at 0x106c06e60> future:c099d48f-8818-4626-a7cd-f10120e0fc23\u001b[0m\n",
      "\u001b[32m2024-Oct-08 14:15:11.94\u001b[0m| \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.misc.annotations\u001b[0m:\u001b[36m119 \u001b[0m | \u001b[33m\u001b[1madd is deprecated and will be removed in a future release.\u001b[0m\n",
      "\u001b[32m2024-Oct-08 14:15:11.94\u001b[0m| \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.base.document\u001b[0m:\u001b[36m452 \u001b[0m | \u001b[33m\u001b[1mLeaf ID already exists\u001b[0m\n",
      "\u001b[32m2024-Oct-08 14:15:11.94\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.components.listener\u001b[0m:\u001b[36m94  \u001b[0m | \u001b[1mRequesting listener setup on CDC service\u001b[0m\n",
      "\u001b[32m2024-Oct-08 14:15:11.94\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.components.listener\u001b[0m:\u001b[36m104 \u001b[0m | \u001b[1mSkipping listener setup on CDC service since no URI is set\u001b[0m\n",
      "\u001b[32m2024-Oct-08 14:15:11.94\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.jobs.queue\u001b[0m:\u001b[36m210 \u001b[0m | \u001b[1mRunning jobs for listener::embedding-listener\u001b[0m\n",
      "\u001b[32m2024-Oct-08 14:15:11.95\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.backends.local.compute\u001b[0m:\u001b[36m67  \u001b[0m | \u001b[1mSubmitting job. function:<function method_job at 0x106c06e60>\u001b[0m\n",
      "\u001b[32m2024-Oct-08 14:15:11.95\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.components.model\u001b[0m:\u001b[36m732 \u001b[0m | \u001b[1mRequesting prediction in db - [text-embedding-ada-002] with predict_id embedding-listener\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:06<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-Oct-08 14:15:18.55\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.components.model\u001b[0m:\u001b[36m862 \u001b[0m | \u001b[1mAdding 363 model outputs to `db`\u001b[0m\n",
      "\u001b[32m2024-Oct-08 14:15:21.01\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m346 \u001b[0m | \u001b[1mInserted 363 documents into _outputs__embedding-listener\u001b[0m\n",
      "\u001b[32m2024-Oct-08 14:15:21.02\u001b[0m| \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.backends.local.compute\u001b[0m:\u001b[36m73  \u001b[0m | \u001b[32m\u001b[1mJob submitted on <superduper.backends.local.compute.LocalComputeBackend object at 0x16062be20>.  function:<function method_job at 0x106c06e60> future:444109a7-933e-45db-8064-dd537272f0d7\u001b[0m\n",
      "\u001b[32m2024-Oct-08 14:15:21.05\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.jobs.queue\u001b[0m:\u001b[36m210 \u001b[0m | \u001b[1mRunning jobs for vector_index::vector-index\u001b[0m\n",
      "\u001b[32m2024-Oct-08 14:15:21.05\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.backends.local.compute\u001b[0m:\u001b[36m67  \u001b[0m | \u001b[1mSubmitting job. function:<function callable_job at 0x106c070a0>\u001b[0m\n",
      "\u001b[32m2024-Oct-08 14:15:21.06\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m153 \u001b[0m | \u001b[1mInitializing vector searcher with type in_memory\u001b[0m\n",
      "\u001b[32m2024-Oct-08 14:15:21.06\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m164 \u001b[0m | \u001b[1mUsing vector searcher: <class 'superduper.vector_search.in_memory.InMemoryVectorSearcher'>\u001b[0m\n",
      "\u001b[32m2024-Oct-08 14:15:21.06\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.components.vector_index\u001b[0m:\u001b[36m54  \u001b[0m | \u001b[1mLoading vectors of vector-index: 'vector-index'\u001b[0m\n",
      "\u001b[32m2024-Oct-08 14:15:21.06\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.components.vector_index\u001b[0m:\u001b[36m62  \u001b[0m | \u001b[1m_outputs__embedding-listener.select()\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading vectors into vector-table...: 363it [00:00, 953.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-Oct-08 14:15:21.44\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.components.vector_index\u001b[0m:\u001b[36m97  \u001b[0m | \u001b[1mLoaded 363 vectors into vector index succesfully\u001b[0m\n",
      "\u001b[32m2024-Oct-08 14:15:21.95\u001b[0m| \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.backends.local.compute\u001b[0m:\u001b[36m73  \u001b[0m | \u001b[32m\u001b[1mJob submitted on <superduper.backends.local.compute.LocalComputeBackend object at 0x16062be20>.  function:<function callable_job at 0x106c070a0> future:e971fd14-68d4-4b82-a34a-1b01c444166b\u001b[0m\n",
      "\u001b[32m2024-Oct-08 14:15:21.96\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m153 \u001b[0m | \u001b[1mInitializing vector searcher with type in_memory\u001b[0m\n",
      "\u001b[32m2024-Oct-08 14:15:21.96\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m164 \u001b[0m | \u001b[1mUsing vector searcher: <class 'superduper.vector_search.in_memory.InMemoryVectorSearcher'>\u001b[0m\n",
      "\u001b[32m2024-Oct-08 14:15:21.96\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.components.vector_index\u001b[0m:\u001b[36m54  \u001b[0m | \u001b[1mLoading vectors of vector-index: 'vector-index'\u001b[0m\n",
      "\u001b[32m2024-Oct-08 14:15:21.96\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.components.vector_index\u001b[0m:\u001b[36m62  \u001b[0m | \u001b[1m_outputs__embedding-listener.select()\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading vectors into vector-table...: 363it [00:00, 980.35it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-Oct-08 14:15:22.33\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.components.vector_index\u001b[0m:\u001b[36m97  \u001b[0m | \u001b[1mLoaded 363 vectors into vector index succesfully\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['dbe9ae8eb92a492882428ddd90890a88',\n",
       "  '851c4c2087ce475bb9c6ee1c27c999ba',\n",
       "  'ee8eb87cd3c34e0a84a3da6ea9dea620'],\n",
       " VectorIndex(identifier='vector-index', uuid='d9d4f47a1e454bc39619eed037aeaebd', upstream=None, plugins=None, cache=False, indexing_listener=Listener(identifier='embedding-listener', uuid='befa04bbe7ef48a79f02ccb54736cdea', upstream=[Listener(identifier='chunker', uuid='chunker', upstream=None, plugins=None, cache=False, key='x', model=ObjectModel(identifier='chunker', uuid='a9e49743199c4344a7a4300485dd8f88', upstream=None, plugins=None, cache=False, signature='*args,**kwargs', datatype=None, output_schema=None, flatten=True, model_update_kwargs={}, predict_kwargs={}, compute_kwargs={}, validation=None, metric_values={}, num_workers=0, serve=False, object=<function chunker at 0x1606a2440>), select=data.select(), predict_kwargs={}, predict_id='chunker')], plugins=None, cache=False, key='_outputs__chunker', model=OpenAIEmbedding(identifier='text-embedding-ada-002', uuid='10419e734c5b46aca9fc4f0da296563f', upstream=None, plugins=None, cache=False, signature='singleton', datatype=DataType(identifier='vector[1536]', uuid='8300cd853aee4052bf5529a0c2400379', upstream=None, plugins=None, cache=False, encoder=None, decoder=None, info=None, shape=(1536,), directory=None, encodable='native', bytes_encoding=<BytesEncoding.BYTES: 'Bytes'>, intermediate_type='bytes', media_type=None), output_schema=None, flatten=False, model_update_kwargs={}, predict_kwargs={}, compute_kwargs={}, validation=None, metric_values={}, num_workers=0, serve=False, model='text-embedding-ada-002', max_batch_size=8, openai_api_key=None, openai_api_base=None, client_kwargs={}, shape=(1536,), batch_size=100), select=_outputs__chunker.select(), predict_kwargs={}, predict_id='embedding-listener'), compatible_listener=None, measure=<VectorIndexMeasureType.cosine: 'cosine'>, metric_values={}))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.apply(vector_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91142c55-b256-4025-94c2-6c4d215c6975",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "## Create Vector Search Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d47799ab-b688-4eb8-82d4-6c0aa1204801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper.components.model import QueryModel\n",
    "\n",
    "item = {'_outputs__chunker': '<var:query>'}\n",
    "\n",
    "vector_search_model = QueryModel(\n",
    "    identifier=\"VectorSearch\",\n",
    "    select=db[upstream_listener.outputs].like(item, vector_index=vector_index_name, n=5).select(),\n",
    "    # The _source is the identifier of the upstream data, which can be used to locate the data from upstream sources using `_source`.\n",
    "    postprocess=lambda docs: [{\"text\": doc[vector_index.indexing_listener.outputs], \"_source\": doc[\"_source\"]} for doc in docs],\n",
    "    db=db\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e88b8905-739b-4c3e-9aba-c6671b0f1d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?-outputs-chunker-like-outputs-chunker-<var:query>-vector-index-vector-index-n-5-select\n",
      "-outputs-chunker-like-outputs-chunker-<var:query>-vector-index-vector-index-n-5-select\n",
      "\u001b[32m2024-Oct-08 14:24:20.60\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MacBook-Pro.local\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m930 \u001b[0m | \u001b[1m{}\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'_outputs__embedding-listener'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvector_search_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTell me about vector-search\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/superduper-io/superduper/superduper/components/component.py:625\u001b[0m, in \u001b[0;36mensure_initialized.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_initialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    624\u001b[0m     logging\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitialized  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 625\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/superduper-io/superduper/superduper/components/model.py:1361\u001b[0m, in \u001b[0;36mQueryModel.predict\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1359\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb\u001b[38;5;241m.\u001b[39mexecute(select)\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpostprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "Cell \u001b[0;32mIn[17], line 9\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(docs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msuperduper\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QueryModel\n\u001b[1;32m      3\u001b[0m item \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_outputs__chunker\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<var:query>\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m      5\u001b[0m vector_search_model \u001b[38;5;241m=\u001b[39m QueryModel(\n\u001b[1;32m      6\u001b[0m     identifier\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVectorSearch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     select\u001b[38;5;241m=\u001b[39mdb[upstream_listener\u001b[38;5;241m.\u001b[39moutputs]\u001b[38;5;241m.\u001b[39mlike(item, vector_index\u001b[38;5;241m=\u001b[39mvector_index_name, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39mselect(),\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# The _source is the identifier of the upstream data, which can be used to locate the data from upstream sources using `_source`.\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     postprocess\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m docs: [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: doc[vector_index\u001b[38;5;241m.\u001b[39mindexing_listener\u001b[38;5;241m.\u001b[39moutputs], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_source\u001b[39m\u001b[38;5;124m\"\u001b[39m: doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_source\u001b[39m\u001b[38;5;124m\"\u001b[39m]} \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs],\n\u001b[1;32m     10\u001b[0m     db\u001b[38;5;241m=\u001b[39mdb\n\u001b[1;32m     11\u001b[0m )\n",
      "Cell \u001b[0;32mIn[17], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msuperduper\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QueryModel\n\u001b[1;32m      3\u001b[0m item \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_outputs__chunker\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<var:query>\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m      5\u001b[0m vector_search_model \u001b[38;5;241m=\u001b[39m QueryModel(\n\u001b[1;32m      6\u001b[0m     identifier\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVectorSearch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     select\u001b[38;5;241m=\u001b[39mdb[upstream_listener\u001b[38;5;241m.\u001b[39moutputs]\u001b[38;5;241m.\u001b[39mlike(item, vector_index\u001b[38;5;241m=\u001b[39mvector_index_name, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39mselect(),\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# The _source is the identifier of the upstream data, which can be used to locate the data from upstream sources using `_source`.\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     postprocess\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m docs: [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mdoc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvector_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindexing_listener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m]\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_source\u001b[39m\u001b[38;5;124m\"\u001b[39m: doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_source\u001b[39m\u001b[38;5;124m\"\u001b[39m]} \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs],\n\u001b[1;32m     10\u001b[0m     db\u001b[38;5;241m=\u001b[39mdb\n\u001b[1;32m     11\u001b[0m )\n",
      "File \u001b[0;32m~/superduper-io/superduper/superduper/misc/special_dicts.py:301\u001b[0m, in \u001b[0;36mMongoStyleDict.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m key:\n\u001b[0;32m--> 301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: '_outputs__embedding-listener'"
     ]
    }
   ],
   "source": [
    "vector_search_model.predict('Tell me about vector-search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd5db043-a3e4-4dae-9a97-f80ef4b56e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/Users/dodo/superduper-io/superduper/superduper/misc/special_dicts.py\u001b[0m(301)\u001b[0;36m__getitem__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    299 \u001b[0;31m            \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    300 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0;34m'.'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 301 \u001b[0;31m            \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    302 \u001b[0;31m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    303 \u001b[0;31m            \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  !self\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document({'_outputs__chunker': '--- sidebar_label: Build simple select queries filename: build_simple_select_queries.md --- import Tabs from \\'@theme/Tabs\\'; import TabItem from \\'@theme/TabItem\\'; <!-- TABS --> # Build simple select queries <Tabs> <TabItem value=\"MongoDB\" label=\"MongoDB\" default> ```python select = db[\\'<table-name>\\'].select() ``` </TabItem> <TabItem value=\"SQL\" label=\"SQL\" default> ```python select = db[\\'<table-name>\\'].select() ``` </TabItem> </Tabs>', '_source': ObjectId('6705212f04115afc747a2a32'), '_fold': 'train', '_id': ObjectId('6705224f04115afc747a2b1a'), 'score': 0.7706174003563})\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  q\n"
     ]
    }
   ],
   "source": [
    "debug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1179a67b-4e40-496b-9851-98f32d42faa0",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "## Build LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98e5ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: OpenAI>\n",
    "from superduper_openai import OpenAIChatCompletion\n",
    "\n",
    "llm = OpenAIChatCompletion(identifier='llm', model='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf39c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Anthropic>\n",
    "from superduper_anthropic import AnthropicCompletions\n",
    "import os\n",
    "\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-xxx\"\n",
    "\n",
    "predict_kwargs = {\n",
    "    \"max_tokens\": 1024,\n",
    "    \"temperature\": 0.8,\n",
    "}\n",
    "\n",
    "llm = AnthropicCompletions(identifier='llm', model='claude-2.1', predict_kwargs=predict_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e48deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: vLLM>\n",
    "from superduper_vllm import VllmModel\n",
    "\n",
    "predict_kwargs = {\n",
    "    \"max_tokens\": 1024,\n",
    "    \"temperature\": 0.8,\n",
    "}\n",
    "\n",
    "\n",
    "llm = VllmModel(\n",
    "    identifier=\"llm\",\n",
    "    model_name=\"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\",\n",
    "    vllm_kwargs={\n",
    "        \"gpu_memory_utilization\": 0.7,\n",
    "        \"max_model_len\": 1024,\n",
    "        \"quantization\": \"awq\",\n",
    "    },\n",
    "    predict_kwargs=predict_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4ac344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Transformers>\n",
    "from superduper_transformers import LLM\n",
    "\n",
    "llm = LLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", load_in_8bit=True, device_map=\"cuda\", identifier=\"llm\", predict_kwargs=dict(max_new_tokens=128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdbfae2-af7d-4845-bce5-0cb230e3614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Llama.cpp>\n",
    "!huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.2-GGUF mistral-7b-instruct-v0.2.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\n",
    "\n",
    "from superduper_llama_cpp.model import LlamaCpp\n",
    "llm = LlamaCpp(identifier=\"llm\", model_name_or_path=\"mistral-7b-instruct-v0.2.Q4_K_M.gguf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ae6203-dcc4-493c-a8f8-f727f0f75778",
   "metadata": {},
   "source": [
    "## Answer question with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44baeb09-6f35-4cf2-b814-46283a59f7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper import model\n",
    "from superduper.components.graph import Graph, input_node\n",
    "\n",
    "prompt_template = (\n",
    "    \"Use the following context snippets, these snippets are not ordered!, Answer the question based on this context.\\n\"\n",
    "    \"{context}\\n\\n\"\n",
    "    \"Here's the question: {query}\"\n",
    ")\n",
    "\n",
    "@model\n",
    "def build_prompt(query, docs):\n",
    "    chunks = [doc[\"text\"] for doc in docs]\n",
    "    context = \"\\n\\n\".join(chunks)\n",
    "    prompt = prompt_template.format(context=context, query=query)\n",
    "    return prompt\n",
    "\n",
    "# We build a graph to handle the entire pipeline\n",
    "\n",
    "# create a input node, only have one input parameter `query`\n",
    "in_ = input_node('query')\n",
    "# pass the query to the vector search model\n",
    "vector_search_results = vector_search_model(query=in_)\n",
    "# pass the query and the search results to the prompt builder\n",
    "prompt = build_prompt(query=in_, docs=vector_search_results)\n",
    "# pass the prompt to the llm model\n",
    "answer = llm(prompt)\n",
    "# create a graph, and the graph output is the answer\n",
    "rag = answer.to_graph(\"rag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183bf5b6-4644-4e4c-b65b-e6bafdc6b49f",
   "metadata": {},
   "source": [
    "By applying the RAG model to the database, it will subsequently be accessible for use in other services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6787c78-4b14-4a72-818b-450408a74331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper import Application\n",
    "\n",
    "app = Application(\n",
    "    'rag-app',\n",
    "    components=[\n",
    "        upstream_listener,\n",
    "        vector_index,\n",
    "        vector_search_model,\n",
    "        rag,\n",
    "    ]\n",
    ")\n",
    "\n",
    "db.apply(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da0306b-0969-49ab-95c4-0eb93c39f515",
   "metadata": {},
   "source": [
    "You can now load the model elsewhere and make predictions using the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8dc26d-b91e-442d-9904-15cd4accc463",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = db.load(\"model\", 'rag')\n",
    "print(rag.predict(\"Tell me about superduper\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42119a4-6aef-46ec-a81d-cbe1167d8710",
   "metadata": {},
   "source": [
    "## Create template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e850c03-33c6-4c88-95d3-d14146a6a0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper import Template\n",
    "\n",
    "template = Template('rag-template', template=app, substitutions={'docs': 'collection'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7b5bd4-726a-4d50-acc6-46b701d4d879",
   "metadata": {},
   "outputs": [],
   "source": [
    "template.export('.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
