{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "238f94f6",
   "metadata": {},
   "source": [
    "# Text-2-Image document retrieval using pretrained CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70298808",
   "metadata": {},
   "source": [
    "In the first AI task which we implement for this collection of data, we'll be setting up a model to retrieve relevant images using provided text. We'll use the data from the `captions` field to retrieve the `img` field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3743c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../../')\n",
    "\n",
    "from superduperdb.client import the_client\n",
    "\n",
    "docs = the_client.coco.documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd2550b",
   "metadata": {},
   "source": [
    "Let's start adding a model to the collection.\n",
    "A nice open source model to test text-2-image retrieval is [CLIP](https://openai.com/blog/clip/) which understands images and texts and embeds these in a common vector space.\n",
    "\n",
    "Note that we are specifying the type of the model output, so that the collection knows how to store the results, as well as \"activating\" the model with `active=True`. That means, whenever we add data which fall under the `filter`, then these will get processed by the model, and the outputs will be added to the collection documents.\n",
    "\n",
    "The `key` argument specifies which part of the document the model should act. If `key=\"_base\"` then the model takes the whole document as input. Since we'll be encoding documents as images, then we'll chose `key=\"img\"`.\n",
    "\n",
    "We import the clip model from the `examples` directory on GitHub. However, for completeness, we quote the code here - it's a thin wrapper aroung the OpenAI model.\n",
    "\n",
    "In `examples.models`:\n",
    "\n",
    "```python\n",
    "from clip import load as load_clip, tokenize as clip_tokenize\n",
    "\n",
    "class CLIP(torch.nn.Module):\n",
    "    def __init__(self, name):\n",
    "        super().__init__()\n",
    "        self.model, self.image_preprocess = load_clip(name)\n",
    "\n",
    "    def preprocess(self, r):\n",
    "        if isinstance(r, str):\n",
    "            return clip_tokenize(r, truncate=True)[0, :]\n",
    "        elif isinstance(r, list) and isinstance(r[0], str):\n",
    "            return clip_tokenize(' '.join(r), truncate=True)[0, :]\n",
    "        return self.image_preprocess(r)\n",
    "\n",
    "    def forward(self, r):\n",
    "        if len(r.shape) == 2:\n",
    "            return self.model.encode_text(r)\n",
    "        return self.model.encode_image(r)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfee9613",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing chunk (1/17)\n",
      "finding documents under filter\n",
      "done.\n",
      "processing with clip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [07:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bulk writing...\n",
      "done.\n",
      "computing chunk (2/17)\n",
      "finding documents under filter\n",
      "done.\n",
      "processing with clip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [07:43<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bulk writing...\n",
      "done.\n",
      "computing chunk (3/17)\n",
      "finding documents under filter\n",
      "done.\n",
      "processing with clip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [07:39<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bulk writing...\n",
      "done.\n",
      "computing chunk (4/17)\n",
      "finding documents under filter\n",
      "done.\n",
      "processing with clip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|███▋                                                                                                                                                      | 12/500 [00:11<07:32,  1.08it/s]"
     ]
    }
   ],
   "source": [
    "from examples.models import CLIP\n",
    "\n",
    "docs.create_model(\n",
    "    name='clip',\n",
    "    object=CLIP('RN50'),\n",
    "    filter={},\n",
    "    type='float_tensor',\n",
    "    key='img',\n",
    "    verbose=True,\n",
    "    active=True,\n",
    "    loader_kwargs={'batch_size': 10},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861db6a0",
   "metadata": {},
   "source": [
    "We'll create a companion model which uses the same underlying object as the previous model. That's specified by adding the name instead of the object in the `object` argument. In this case the model is not `active`, since we'll only be using it for querying the collection. We don't need to specify a `type` since that was done in the last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b925d707",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.create_model(\n",
    "    name='clip_text',\n",
    "    object='clip',\n",
    "    key='captions',\n",
    "    active=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c908e762",
   "metadata": {},
   "source": [
    "We'll also create a measure which tests how similar to each other two outputs might be. Since CLIP was trained with cosine-similarity we'll use that here too.\n",
    "\n",
    "In `examples.measures`:\n",
    "\n",
    "```python\n",
    "\n",
    "def dot(x, y):\n",
    "    return x.matmul(y.T)\n",
    "\n",
    "\n",
    "def css(x, y):\n",
    "    x = x.div(x.norm(dim=1)[:, None])\n",
    "    y = y.div(y.norm(dim=1)[:, None])\n",
    "    return dot(x, y)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6cd6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.measures import css\n",
    "\n",
    "docs.create_measure('css', css)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda8af6c",
   "metadata": {},
   "source": [
    "In order to be able to measure performance on the validation set, we'll add a **metric**.\n",
    "\n",
    "In `examples.metrics`:\n",
    "\n",
    "```python\n",
    "class PatK:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        return y in x[:self.k]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66911bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.metrics import PatK\n",
    "\n",
    "docs.create_metric('p_at_10', PatK(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d34b2f",
   "metadata": {},
   "source": [
    "Now we're ready to go to add a **semantic index**. This is a tuple of models, one of which is activated in order to populate the collection with vectors. The idea is that any of the models in the **semantic index** can be used to query the collection using nearest neighbour lookup based on the **measure** chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd71ce6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from examples.models import CLIP\n",
    "\n",
    "docs.create_semantic_index(\n",
    "    'clip',\n",
    "    models=['clip', 'clip_text'],\n",
    "    measure='css',\n",
    "    metrics=['p_at_10'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0416b101",
   "metadata": {},
   "source": [
    "Now the semantic index has been created, we can search through the data using that index.\n",
    "\n",
    "We can see that we can get nice meaningful retrievals using the CLIP model from short descriptive pieces of text.\n",
    "This is very useful, since the model is now deployed to the database, listening for incoming queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a38189",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "docs.semantic_index = 'clip'\n",
    "r = docs.find_one()\n",
    "\n",
    "# example using item id directly\n",
    "for r in docs.find(like={'_id': r['_id']}, n=10):\n",
    "    display(r['img'])\n",
    "    \n",
    "# or a query which is interpreted by the CLIP model\n",
    "for r in docs.find(like={'captions': ['Dog catches a frisbee']}, n=10):\n",
    "    display(r['img'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76310241",
   "metadata": {},
   "source": [
    "Let's now evaluate the quality of this semantic index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29839514",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "docs.create_validation_set(\n",
    "    'text2image_retrieval', \n",
    "    filter={},\n",
    "    splitter=lambda x: ({'img': x['img']}, {'captions': [x['captions'][0]]}),\n",
    "    sample_size=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aff23c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.validate_semantic_index('clip', ['text2image_retrieval'], ['p_at_10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695fee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs['_semantic_indexes'].find_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407f2c87",
   "metadata": {},
   "source": [
    "In the next section of this example, let us train our own model from scratch. The model will be much simpler than the clip model, but will yield faster retrievals. It will be interesting to see how this compares to CLIP, and show-case SuperDuperDB as a framework for easily integrating and benchmarking AI models, in particular for retrieval."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
