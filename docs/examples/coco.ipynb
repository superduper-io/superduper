{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "213682c6",
   "metadata": {},
   "source": [
    "# Image retrieval, captioning and classification with CoCo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38bc025",
   "metadata": {},
   "source": [
    "This tutorial uses the [CoCo dataset \"Common objects in Context\"](https://cocodataset.org/#home) to show case some of the key-features of SuperDuperDB. In this example, you'll learn how to:\n",
    "\n",
    "- Prepare data in the best way for SuperDuperDB usage\n",
    "- Define data types\n",
    "- Upload and query data to and from the data base\n",
    "- Define multiple models on the database, including models with dependencies\n",
    "- Define a searchable semantic index based on existing models\n",
    "- Train a semantic index from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66577c0c",
   "metadata": {},
   "source": [
    "If you haven't downloaded the data already, execute the lines of bash below. We've tried to keep it clean,\n",
    "and for reasons of efficiency have resized the images using imagemagick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6232ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -o data/coco/\n",
    "!curl http://images.cocodataset.org/annotations/annotations_trainval2014.zip -o data/coco/raw.zip\n",
    "!unzip data/coco/raw.zip\n",
    "!mv data/coco/annotations/captions_train2014.json data/coco/\n",
    "!rm -rf data/coco/annotations\n",
    "!rm data/coco/raw.zip\n",
    "!curl http://images.cocodataset.org/zips/train2014.zip -o data/coco/images.zip\n",
    "!unzip data/coco/images.zip\n",
    "!rm data/coco/images.zip\n",
    "!sudo apt install imagemagick\n",
    "!mogrify -resize 224x data/coco/images/*.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b455a1c8",
   "metadata": {},
   "source": [
    "SuperDuperDB uses MongoDB for data storage. If you haven't done so already, install it using the following lines of bash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7ec394",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -qO - https://www.mongodb.org/static/pgp/server-6.0.asc | sudo apt-key add -\n",
    "!sudo apt-get install gnupg\n",
    "!wget -qO - https://www.mongodb.org/static/pgp/server-6.0.asc | sudo apt-key add -\n",
    "!echo \"deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu focal/mongodb-org/6.0 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-6.0.list\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install -y mongodb-org"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd6dc87",
   "metadata": {},
   "source": [
    "In case you haven't done so already, install the dependencies for this tutorial, including SuperDuperDB,\n",
    "which is a simple pip install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5382d874",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install pillow\n",
    "!pip install torch\n",
    "!pip install superduperdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1e6e1",
   "metadata": {},
   "source": [
    "SuperDuperDB can handle data in any format, including images. The documents in the database are MongoDB `bson` documents, which mix `json` with raw bytes and `ObjectId` objects. SuperDuperDB takes advantage of this by \n",
    "serializing more sophisticated objects to bytes, and reinstantiating the objects in memory, when data is queried.\n",
    "\n",
    "In order to tell SuperDuperDB what type an object has, one specifies this with a subdocument of the form:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"_content\": {\n",
    "        \"bytes\": ...,\n",
    "        \"type\": \"<my-type>\",\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "If however, the content is located on the web or the filesystem, one can specify the URLs directly:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"_content\": {\n",
    "        \"url\": \"<url-or-file>\",\n",
    "        \"type\": \"<my-type>\",\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Let's see this now in action. We reformat the CoCo data, so that each image is associated in one document with all of the captions which describe it, and add the location of the images using the `_content` formalism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a5dc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/coco/captions_train2014.json') as f:\n",
    "    raw = json.load(f)\n",
    "    \n",
    "raw['images'] = {x['id']: x for x in raw['images']}\n",
    "\n",
    "for im in raw['images']:\n",
    "    raw['images'][im]['captions'] = []\n",
    "    \n",
    "for a in raw['annotations']:\n",
    "    raw['images'][a['image_id']]['captions'].append(a['caption'])\n",
    "\n",
    "raw = list(raw['images'].values())\n",
    "\n",
    "for i, im in enumerate(raw):\n",
    "    # if image is already in memory, then add 'bytes': b'...' instead of 'url': '...'\n",
    "    # for content located on the web, use 'http://' or 'https://' instead of 'file://'\n",
    "    im['img'] = {\n",
    "        '_content': {'url': f'file://data/coco/images/{im[\"file_name\"]}', 'type': 'image'}\n",
    "    }\n",
    "    raw[i] = {'captions': im['captions'], 'img': im['img']}\n",
    "\n",
    "with open('data/coco/data.json', 'w') as f:\n",
    "    json.dump(raw, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a45f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "\n",
    "sys.path.append('../../')\n",
    "\n",
    "from superduperdb.client import the_client\n",
    "from IPython.display import display, clear_output\n",
    "import torch\n",
    "\n",
    "docs = the_client.coco_example.documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17b03ae",
   "metadata": {},
   "source": [
    "We'll load the data and add most of it to the database. We'll hold back some data so that we can see how to update \n",
    "the database later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bf022c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('data/coco/data.json') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "docs.insert_many(data[:-1000]), verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1433f7c0",
   "metadata": {},
   "source": [
    "We previously added the type `image` to the `_content` subrecords earlier.\n",
    "So that we can load the data using this type, we need to add this type to the database.\n",
    "You can see in `examples/types.py` how the class encodes and decodes data. Suffice to say at this point, \n",
    "that each type has an `encode` and `decode` method, which convert to and from `bytes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2728cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.types import FloatTensor, Image\n",
    "\n",
    "docs.create_type('float_tensor', FloatTensor())\n",
    "docs.create_type('image', Image())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70298808",
   "metadata": {},
   "source": [
    "In the first AI task which we implement for the `docs` collection, we'll be setting up a model to retrieve relevant images using provided text. For this data, that means the `captions` field being used to retrieve the `img` field. In order to be able to keep an objective record of performance, we can set up an immutable validation dataset from the collection. We use a **splitter** to define how we'd like to test retrieval. This splits the documents into query and retrieved document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8565389",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "docs.create_validation_set(\n",
    "    'text2image_retrieval', \n",
    "    filter={},\n",
    "    splitter=lambda x: ({'img': x['img']}, {'captions': [x['captions'][0]]}),\n",
    "    sample_size=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126c91f7",
   "metadata": {},
   "source": [
    "We can see what the data points in the validation set look like by querying:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c171e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs['_validation_sets'].find_one({'_validation_set': 'text2image_retrieval'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd2550b",
   "metadata": {},
   "source": [
    "You can see that the sample \"query\" is split into the `_other` field. This is important when evaluating semantic indexes.\n",
    "\n",
    "Now let's start adding a model to the collection.\n",
    "A nice open source model to test text-2-image retrieval is [CLIP](https://openai.com/blog/clip/) which understands images and texts and embeds these in a common vector space.\n",
    "\n",
    "Note that we are specifying the type of the model output, so that the collection knows how to store the results, as well as \"activating\" the model with `active=True`. That means, whenever we add data which fall under the `filter`, then these will get processed by the model, and the outputs will be added to the collection documents.\n",
    "\n",
    "The `key` argument specifies which part of the document the model should act. If `key=\"_base\"` then the model takes the whole document as input. Since we'll be encoding documents as images, then we'll chose `key=\"img`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfee9613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.models import CLIP\n",
    "\n",
    "docs.create_model(\n",
    "    name='clip',\n",
    "    object=CLIP('RN50'),\n",
    "    filter={},\n",
    "    type='float_tensor',\n",
    "    key='img',\n",
    "    verbose=True,\n",
    "    active=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861db6a0",
   "metadata": {},
   "source": [
    "We'll create a companion model which uses the same underlying object as the previous model. That's specified by adding the name instead of the object in the `object` argument. In this case the model is not `active`, since we'll only be using it for querying the collection. We don't need to specify a `type` since that was done in the last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b925d707",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.create_model(\n",
    "    name='clip_text',\n",
    "    object='clip',\n",
    "    key='captions',\n",
    "    active=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c908e762",
   "metadata": {},
   "source": [
    "We'll also create a measure which tests how similar to each other two outputs might be. Since CLIP was trained with cosine-similarity we'll use that here too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6cd6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.measures import css\n",
    "\n",
    "docs.create_measure('css', css)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda8af6c",
   "metadata": {},
   "source": [
    "In order to be able to measure performance on the validation set, we'll add a **metric**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66911bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.metrics import PatK\n",
    "\n",
    "docs.create_metric('p_at_10', PatK(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d34b2f",
   "metadata": {},
   "source": [
    "Now we're ready to go to add a **semantic index**. This is a tuple of models, one of which is activated in order to populate the collection with vectors. The idea is that any of the models in the **semantic index** can be used to query the collection using nearest neighbour lookup based on the **measure** chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd71ce6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from examples.models import CLIP\n",
    "\n",
    "docs.create_semantic_index(\n",
    "    'clip',\n",
    "    models=['clip', 'clip_text'],\n",
    "    measure='css',\n",
    "    metrics=['p_at_10'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a38189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bson import ObjectId\n",
    "from IPython.display import display\n",
    "\n",
    "docs.semantic_index = 'clip'\n",
    "for r in docs.find({'$like': {'document': {'_id': ObjectId('63d27372745cc274ef3518f2')}, 'n': 10}}):\n",
    "    display(r['img'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76310241",
   "metadata": {},
   "source": [
    "Let's now evaluate the quality of this semantic index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aff23c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.validate_semantic_index('clip', ['text2image_retrieval'], ['p_at_10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bdb5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs['_semantic_indexes'].find_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac54e49",
   "metadata": {},
   "source": [
    "We can see that we can get nice meaningful retrievals using the CLIP model from short descriptive pieces of text.\n",
    "This is very useful, since the model is now deployed to the database, listening for incoming queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193a26a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.list_semantic_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4e0e73",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "docs.semantic_index = 'clip'\n",
    "for r in docs.find({'$like': {'document': {'captions': ['Dog catches a frisbee']}, 'n': 5}}):\n",
    "    display(r['img'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407f2c87",
   "metadata": {},
   "source": [
    "In the next section of this example, let us train our own model from scratch. The model will be much simpler than the clip model, but will yield faster retrievals. It will be interesting to see how this compares to CLIP, and show-case SuperDuperDB as a framework for easily integrating and benchmarking AI models, in particular for retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d9293c",
   "metadata": {},
   "source": [
    "First we will implement a simpler sentence embedding, using a simple word-embedding approach based around Glove.\n",
    "Please look at the model in `examples.models.AverageOfGloves`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df44c5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://nlp.stanford.edu/data/glove.6B.zip -o data/glove.6B.zip\n",
    "!unzip data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbb979e",
   "metadata": {},
   "source": [
    "We may register this model to the collection in the same way we did for the textual part of CLIP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625813aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "from examples.models import AverageOfGloves\n",
    "\n",
    "with open('data/glove.6B/glove.6B.50d.txt') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "    \n",
    "lines = [x.split(' ') for x in lines[:-1]]\n",
    "index = [x[0] for x in lines]\n",
    "vectors = [[float(y) for y in x[1:]] for x in lines]\n",
    "vectors = numpy.array(vectors)\n",
    "\n",
    "glove = AverageOfGloves(torch.from_numpy(vectors).type(torch.float), index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527f2aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.create_model(\n",
    "    'average_glove',\n",
    "    object=glove,\n",
    "    key='captions',\n",
    "    active=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4d4c37",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "docs.create_model(\n",
    "    'clip_projection',\n",
    "    object=torch.nn.Linear(1024, 50),\n",
    "    active=True,\n",
    "    key='img',\n",
    "    type='float_tensor',\n",
    "    features={'img': 'clip'},\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ecddf4",
   "metadata": {},
   "source": [
    "Let's also create a loss function, in order to be able to perform the learning task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aebdd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.losses import ranking_loss\n",
    "\n",
    "docs.create_loss('ranking_loss', ranking_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a44467",
   "metadata": {},
   "source": [
    "A semantic index training requires:\n",
    "\n",
    "- 1 or more models\n",
    "- A measure function to measure similarity between model outputs\n",
    "- A loss function\n",
    "- One or more validation sets\n",
    "- One or more metrics to measure performance\n",
    "\n",
    "We now have all of these things ready and registered with the database, so we can start the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddf5276",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "docs.create_semantic_index(\n",
    "    'simple_image_search',\n",
    "    models=['clip_projection', 'average_glove'],\n",
    "    loss='ranking_loss',\n",
    "    filter={},\n",
    "    projection={'image': 0, '_like': 0},\n",
    "    metrics=['p_at_10'],\n",
    "    measure='css',\n",
    "    validation_sets=['text2image_retrieval'],\n",
    "    batch_size=250,\n",
    "    num_workers=0,\n",
    "    n_epochs=20,\n",
    "    lr=0.001,\n",
    "    log_weights=True,\n",
    "    download=True,\n",
    "    validation_interval=50,\n",
    "    no_improve_then_stop=5,\n",
    "    n_iterations=5000,\n",
    "    use_grads={'clip_projection': True, 'average_glove': False},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0041c4c",
   "metadata": {},
   "source": [
    "We now can see that we've set and trained our own semantic index. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12e0022",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.list_semantic_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6771a45",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "info = docs['_semantic_indexes'].find_one({'name': 'simple_image_search'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18871e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in info['metric_values']:\n",
    "    if k == 'loss':\n",
    "        print(info['metric_values'][k])\n",
    "        plt.figure()\n",
    "        plt.title('loss')\n",
    "        plt.plot(info['metric_values'][k])\n",
    "        continue\n",
    "    for result in info['metric_values'][k]:\n",
    "        plt.figure()\n",
    "        plt.title(f'{k}/{result}')\n",
    "        plt.plot(info['metric_values'][k][result])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374cf054",
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter in info['weights']:\n",
    "    plt.figure()\n",
    "    plt.title(parameter)\n",
    "    plt.plot(info['weights'][parameter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc334d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e43c4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.refresh_model('clip_projection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b1bf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "docs.semantic_index = 'simple_image_search'\n",
    "for r in docs.find({'$like': {'document': {'captions': ['Dog catches frisbee']}, 'n': 5}}):\n",
    "    display(r['img'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3ff23d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from examples.models import NounWords\n",
    "docs.create_model('noun_words', NounWords(), verbose=True, key='captions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aebe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.create_validation_set('attribute_prediction', sample_size=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289d515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import tqdm\n",
    "all_nouns = []\n",
    "for r in tqdm.tqdm(docs.find({'_fold': 'train'}, {'_outputs.captions.noun_words': 1}), total=docs.count_documents({})):\n",
    "    all_nouns.extend(r['_outputs']['captions']['noun_words'])\n",
    "    \n",
    "counts = dict(collections.Counter(all_nouns))\n",
    "all_nouns = [w for w in counts if counts[w] > 30]\n",
    "total = docs.count_documents({})\n",
    "pos_weights = [counts[w] / total for w in all_nouns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386dc8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.models import FewHot, TopK\n",
    "from examples.metrics import jacquard_index\n",
    "\n",
    "docs.create_model('nouns_to_few_hot', FewHot(all_nouns))\n",
    "docs.create_postprocessor('top_5', TopK(all_nouns, 5))\n",
    "docs.create_forward('attribute_predictor', torch.nn.Linear(1024, len(all_nouns)))\n",
    "docs.create_loss('nouns_loss', torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weights)))\n",
    "docs.create_metric('jacquard_index', jacquard_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a444d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.models import FewHot\n",
    "docs.create_model('nouns_to_few_hot', FewHot(post.tokens), active=False,\n",
    "                 key='_outputs.captions.noun_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60be2dbb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "docs.create_model('attribute_predictor', forward='attribute_predictor', postprocessor='top_5',\n",
    "                  key='img', features={'img': 'clip'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d80959",
   "metadata": {},
   "source": [
    "Let's test the model, using the `apply_model` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06695727",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.apply_model('attribute_predictor', docs.find_one())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e45e59",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "docs.create_imputation(\n",
    "    'noun_prediction',\n",
    "    model='attribute_predictor',\n",
    "    target='nouns_to_few_hot',\n",
    "    loss='nouns_loss',\n",
    "    metrics=['jacquard_index'],\n",
    "    validation_sets=['attribute_prediction'],\n",
    "    lr=0.001,\n",
    "    validation_interval=10,\n",
    "    n_iterations=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30dc496",
   "metadata": {},
   "source": [
    "We can view the results of learning (metrics, loss etc.) by looking in the `_imputations` subcollection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbfcd145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "\n",
    "sys.path.append('../../')\n",
    "\n",
    "from superduperdb.client import the_client\n",
    "from IPython.display import display, clear_output\n",
    "import torch\n",
    "\n",
    "docs = the_client.coco_example.documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4166411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02156996726989746,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 48,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 78560,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79e16047cf784c98b756ac3439e08e2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tqdm \n",
    "\n",
    "all_captions = []\n",
    "n = docs.count_documents({'_fold': 'train'})\n",
    "for r in tqdm.tqdm_notebook(docs.find({'_fold': 'train'}, {'captions': 1, '_id': 0}), total=n):\n",
    "    all_captions.extend(r['captions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6a4472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "all_captions = [re.sub('[^a-z ]', '', x.lower()).strip() for x in all_captions]\n",
    "words = ' '.join(all_captions).split(' ')\n",
    "counts = dict(collections.Counter(words))\n",
    "vocab = sorted([w for w in counts if counts[w] > 5 and w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d71d299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.models import ConditionalLM, SimpleTokenizer\n",
    "tokenizer = SimpleTokenizer(vocab)\n",
    "m = ConditionalLM(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4cd4be",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "docs.create_model('conditional_lm', object=m, active=False, features={'img': 'clip'}, key='img')\n",
    "docs.create_model('captioning_tokenizer', tokenizer, key='caption', active=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e501979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.losses import AutoRegressiveLoss\n",
    "loss = AutoRegressiveLoss(stop_token=tokenizer.lookup['</s>'])\n",
    "docs.create_loss('autoregressive_loss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c35157",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.list_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301837c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.list_splitters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2902496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.splitters import captioning_splitter\n",
    "#docs.create_splitter('captioning_splitter', captioning_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020d38e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "captioning_splitter(docs.find_one())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aa5079",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.create_imputation(\n",
    "    'image_captioner',\n",
    "    model='conditional_lm',\n",
    "    loss='autoregressive_loss',\n",
    "    target='captioning_target',\n",
    "    splitter='captioning_splitter',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8de1319",
   "metadata": {},
   "source": [
    "Now we have trained and evaluated several models of various types. This includes multiple interacting models with mutual dependencies. In the case of our own efficient semantic search, and also the attribute predictor, these models are downstream of the image clip model, in the sense that at inference time, clip must be present in order to be able to execute these models. In the case of attribute prediction, the training task was downstream from the \n",
    "spacy pipeline for part-of-speech tagging; these tags were used to produce targets for training. However at run-time, the spacy pipeline won't be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74500ec3",
   "metadata": {},
   "source": [
    "The models which we've added and trained are now ready to go, and when new data is added or updated to the collection, they will automatically process this data, and insert the model outputs into the collection documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa15fe60",
   "metadata": {},
   "source": [
    "Here is the complete set of models which exist in the collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6eedbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b1fe7c",
   "metadata": {},
   "source": [
    "Not all of these respond to incoming data, for that we need to specify the `active` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f54582",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.list_models(active=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3689545a",
   "metadata": {},
   "source": [
    "We can see that these models have processed all documents and their outputs saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59381c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.find_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3951ff",
   "metadata": {},
   "source": [
    "Now, let's test what happens when we add new data to the collection, by adding the remaining data points from the \n",
    "CoCo data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2653f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "update = [{**r, \"update\": True} for r in data[-1000:]]\n",
    "docs.insert_many(update, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
