{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcf339d2-6eaa-4a90-8718-6d6e6120c400",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "# Build A Trainable LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b1d747-edf1-4a72-90c3-40cbd2eba577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <testing: >\n",
    "# !pip install trl datasets transformers bitsandbytes peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350c8c06-d8c1-49d3-8b25-ed3a04784742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <testing: >\n",
    "import os\n",
    "os.environ[\"SUPERDUPERDB_DATA_BACKEND\"] = 'mongodb://localhost:27017/llm'\n",
    "os.environ[\"SUPERDUPERDB_ARTIFACT_STORE\"] = \"filesystem://./outoput/artifact_store\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784434a8-77fd-46b0-9e42-35a852ca486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <testing: >\n",
    "from superduperdb import superduper\n",
    "from superduperdb.backends.mongodb import Collection\n",
    "from superduperdb.base.document import Document\n",
    "\n",
    "db = superduper(os.environ.get(\"SUPERDUPERDB_DATA_BACKEND\", \"mongomock://test\"))\n",
    "db.drop(True)\n",
    "from datasets import load_dataset\n",
    "\n",
    "model_name = \"facebook/opt-350m\"\n",
    "dataset_name = \"timdettmers/openassistant-guanaco\"\n",
    "\n",
    "dataset = load_dataset(dataset_name)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n",
    "\n",
    "train_documents = [\n",
    "    Document({\"text\": example[\"text\"], \"_fold\": \"train\"})\n",
    "    for example in train_dataset\n",
    "]\n",
    "eval_documents = [\n",
    "    Document({\"text\": example[\"text\"], \"_fold\": \"valid\"})\n",
    "    for example in eval_dataset\n",
    "]\n",
    "\n",
    "db.execute(Collection(\"datas\").insert_many(train_documents[:100]))\n",
    "db.execute(Collection(\"datas\").insert_many(eval_documents[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6c2662-ce55-4767-8e3d-ef3901fd31ee",
   "metadata": {},
   "source": [
    "**Create an LLM Trainer for training**\n",
    "\n",
    "The parameters of this LLM Trainer are basically the same as `transformers.TrainingArguments`, but some additional parameters have been added for easier training setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deed34b-189c-4662-8972-aed92718225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduperdb.ext.transformers import LLM, LLMTrainer\n",
    "trainer = LLMTrainer(\n",
    "    identifier=\"llm-finetune-trainer\",\n",
    "    output_dir=\"output/finetune\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    max_seq_length=512,\n",
    "    key=key,\n",
    "    select=select,\n",
    "    transform=transform,\n",
    "    training_kwargs=training_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089bc70f-00e0-4a13-a438-658146efd4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Lora>\n",
    "trainer.use_lora = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ab34bc-999c-4300-aa47-d40a78536d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: QLora>\n",
    "trainer.use_lora = True\n",
    "trainer.bits = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1392ffec-80aa-4be7-b40f-665c2803e980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Deepspeed>\n",
    "!pip install deepspeed\n",
    "deepspeed = {\n",
    "    \"train_batch_size\": \"auto\",\n",
    "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "    \"gradient_accumulation_steps\": \"auto\",\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 2,\n",
    "    },\n",
    "}\n",
    "trainer.use_lora = True\n",
    "trainer.bits = 4\n",
    "trainer.deepspeed = deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f414fc61-2466-4bf6-8ea2-460524806880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Multi-GPUS>\n",
    "trainer.use_lora = True\n",
    "trainer.bits = 4\n",
    "trainer.num_gpus = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a370f6c8-ce2f-443e-ae89-a4e95c4375a8",
   "metadata": {},
   "source": [
    "Create a trainable LLM model and add it to the database, then the training task will run automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400c1ebb-345e-4030-9f9e-96099f53664c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(\n",
    "    identifier=\"llm\",\n",
    "    model_name_or_path=model_name,\n",
    "    trainer=trainer,\n",
    "    model_kwargs=model_kwargs,\n",
    "    tokenizer_kwargs=tokenizer_kwargs,\n",
    ")\n",
    "\n",
    "db.apply(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edd846d-aa81-456f-b2ea-fc2d230a41a2",
   "metadata": {},
   "source": [
    "# Load the trained model\n",
    "There are two methods to load a trained model:\n",
    "\n",
    "- **Load the model directly**: This will load the model with the best metrics (if the transformers' best model save strategy is set) or the last version of the model.\n",
    "- **Use a specified checkpoint**: This method downloads the specified checkpoint, then initializes the base model, and finally merges the checkpoint with the base model. This approach supports custom operations such as resetting flash_attentions, model quantization, etc., during initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2e1a0d-c760-4a01-b4bf-c6e83296ca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Load Trained Model Directly>\n",
    "llm = db.load(\"model\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5fbf38-8f2b-4c3f-9ae8-c184001b4495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Use a specified checkpoint>\n",
    "from superduperdb.ext.transformers import LLM, LLMTrainer\n",
    "experiment_id = db.show(\"checkpoint\")[-1]\n",
    "version = None # None means the last checkpoint\n",
    "checkpoint = db.load(\"checkpoint\", experiment_id, version=version)\n",
    "llm = LLM(\n",
    "    identifier=\"llm\",\n",
    "    model_name_or_path=model_name,\n",
    "    adapter_id=checkpoint,\n",
    "    model_kwargs=dict(load_in_4bit=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113dafd6-cb86-4892-bbe0-088d3da5e6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <testing: >\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is the capital of Germany? Explain why thats the case and if it was different in the past?\",\n",
    "    }\n",
    "]\n",
    "print(llm.predict(messages, max_new_tokens=200, do_sample=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
