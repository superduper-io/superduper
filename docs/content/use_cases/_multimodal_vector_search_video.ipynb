{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38c1a328-fd86-4c5f-bd54-b8664f433608",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "# Multimodal vector search - Video"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ceaceab6-02eb-45ac-bff1-721d7332d41c",
   "metadata": {},
   "source": [
    "<snippet: configure_your_production_system: *>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "123b8f3c-5025-4930-9c90-0549740a051a",
   "metadata": {},
   "source": [
    "<snippet: start_your_cluster: *>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "db0ce690-8ff7-4988-a377-6b0d8a1fa9f8",
   "metadata": {},
   "source": [
    "<snippet: connect_to_superduperdb: *>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8a84db8b-e9ec-4356-b045-b718de8ccb6c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<snippet: get_useful_sample_data: Video>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a702b1-faf9-4edb-8a55-efc4add84a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = [{'x': d} for d in data[:3]]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e4c320e-6487-4b78-9b1c-453c4922e23f",
   "metadata": {},
   "source": [
    "<snippet: create_datatype: Video>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1bfdcc76-811b-4673-bcf6-d8448532203d",
   "metadata": {},
   "source": [
    "<snippet: setup_tables_or_collections: *>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a947c52e-919e-4440-b1d6-914e690314d4",
   "metadata": {},
   "source": [
    "Inserting data, all fields will be matched with the schema for data conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afead32f-fc4c-4b11-9d31-d38bf061c232",
   "metadata": {},
   "outputs": [],
   "source": [
    "db['documents'].insert(datas).execute()\n",
    "select = db['documents'].select()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "81114386-6ebf-453d-b93d-cae5c2ecec00",
   "metadata": {},
   "source": [
    "<snippet: apply_a_chunker_for_search: Video>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907721f8-d5bf-4623-8871-3ab9a05001d7",
   "metadata": {},
   "source": [
    "## Build multimodal embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033e1eaf-2cdb-499a-ba83-cf080a1a6fda",
   "metadata": {},
   "source": [
    "We define the output data type of a model as a vector for vector transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28848ff1-45ab-4926-8676-777edf237347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: MongoDB>\n",
    "from superduperdb.components.vector_index import vector\n",
    "output_datatpye = vector(shape=(1024,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acf66c5-7369-4aa8-a8a0-5842bd17b469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: SQL>\n",
    "from superduperdb.components.vector_index import sqlvector\n",
    "output_datatpye = sqlvector(shape=(1024,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143bf946-64b7-4452-8d20-44f2f9ae3fd6",
   "metadata": {},
   "source": [
    "Then define two models, one for text embedding and one for image embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33513d3-9f86-4108-8f8b-4a6251bdd9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Text-Image>\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "import clip\n",
    "from superduperdb import vector\n",
    "from superduperdb.ext.torch import TorchModel\n",
    "\n",
    "# Load the CLIP model and obtain the preprocessing function\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device='cpu')\n",
    "\n",
    "# Create a TorchModel for text encoding\n",
    "compatible_model = TorchModel(\n",
    "    identifier='clip_text', # Unique identifier for the model\n",
    "    object=model, # CLIP model\n",
    "    preprocess=lambda x: clip.tokenize(x)[0],  # Model input preprocessing using CLIP \n",
    "    postprocess=lambda x: x.tolist(), # Convert the model output to a list\n",
    "    datatype=output_datatpye,  # Vector encoder with shape (1024,)\n",
    "    forward_method='encode_text', # Use the 'encode_text' method for forward pass \n",
    ")\n",
    "\n",
    "# Create a TorchModel for visual encoding\n",
    "model = TorchModel(\n",
    "    identifier='clip_image',  # Unique identifier for the model\n",
    "    object=model.visual,  # Visual part of the CLIP model    \n",
    "    preprocess=preprocess, # Visual preprocessing using CLIP\n",
    "    postprocess=lambda x: x.tolist(), # Convert the output to a list \n",
    "    datatype=output_datatpye, # Vector encoder with shape (1024,)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0119da-9cfd-4a60-8847-c3bfdf37697f",
   "metadata": {},
   "source": [
    "Because we use multimodal models, we define different keys to specify which model to use for embedding calculations in the vector_index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e75fab-8504-4d17-a7d9-f98667a5d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "compatible_key = 'text' # we use text key for text embedding\n",
    "indexing_key = upstream_listener.outputs_key + '.image' # we use indexing_key for image embedding, use the image field of the result\n",
    "select = upstream_listener.outputs_select"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7eaca47f-32ab-4776-abc0-f6fa2eeb9043",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<snippet: create_vector_index: 2-Modalities>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a87f9d-581a-419a-81b8-a743250413e9",
   "metadata": {},
   "source": [
    "## Perform a vector search\n",
    "\n",
    "We can perform the vector searches using text description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce565823-4655-488c-8684-2240107fa30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Text>\n",
    "from superduperdb import Document\n",
    "item = Document({compatible_key: \"The moment of a soccer shot\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3ba07d-1124-4d94-a117-60d2e72581f7",
   "metadata": {},
   "source": [
    "Once we have this search target, we can execute a search as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a061de0b-2694-4b36-844c-7753a465360f",
   "metadata": {},
   "outputs": [],
   "source": [
    "select = query_table_or_collection.like(item, vector_index=vector_index_name, n=5).select()\n",
    "results = list(db.execute(select))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6d9af9-a012-42bd-aad4-31b92d089caa",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2ecea5-3a58-457c-ac50-ddc742484f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "for result in results:\n",
    "    display(Document(result.unpack())[indexing_key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693b4878-39a2-444d-8e17-72a00e6c246d",
   "metadata": {},
   "source": [
    "## Check the system stays updated\n",
    "\n",
    "You can add new data; once the data is added, all related models will perform calculations according to the underlying constructed model and listener, simultaneously updating the vector index to ensure that each query uses the latest data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef97f5a-bb41-46ca-a85e-489824741216",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_datas = [{'x': data[-1]}]\n",
    "ids = db['documents'].insert(new_datas).execute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
