{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ca02914-dac0-42ac-ac90-d1ebe87e6774",
   "metadata": {},
   "source": [
    "# Basic RAG tutorial with templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ace1e3-1b4f-4c73-9a95-ae6116373a57",
   "metadata": {},
   "source": [
    ":::info\n",
    "In this tutorial we show you how to do retrieval augmented generation (RAG) with `superduperdb`.\n",
    "Note that this is just an example of the flexibility and power which `superduperdb` gives \n",
    "to developers. `superduperdb` is about much more than RAG and LLMs. \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed5486d-133d-468f-befa-014f81ffbc94",
   "metadata": {},
   "source": [
    "As in the vector-search tutorial we'll use `superduperdb` documentation for the tutorial.\n",
    "We'll add this to a testing database by downloading the data snapshot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296ae5f2-32a9-4f80-aeb7-44e82baf749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://superduperdb-public-demo.s3.amazonaws.com/text.json\n",
    "import json\n",
    "\n",
    "from superduperdb import superduper, Document\n",
    "\n",
    "db = superduper('mongomock://test')\n",
    "\n",
    "with open('text.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "_ = db['docu'].insert_many([{'txt': r} for r in data]).execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe6ccbe-e12d-4d89-8559-6875d047b593",
   "metadata": {},
   "source": [
    "Let's verify the data in the `db` by querying one datapoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c97c8eb-5ffe-4fe6-87fd-26c9853b3a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "db['docu'].find_one().execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad947dd-e848-42f3-93b3-7697a8816ed2",
   "metadata": {},
   "source": [
    "The first step in a RAG application is to create a `VectorIndex`. The results of searching \n",
    "with this index will be used as input to the LLM for answering questions.\n",
    "\n",
    "Read about `VectorIndex` [here](../apply_api/vector_index.md) and follow along the tutorial on \n",
    "vector-search [here](./vector_search.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f443ee6-3ff4-4b24-9767-4d295ea1bc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "from superduperdb import Stack, Document, VectorIndex, Listener, vector\n",
    "from superduperdb.ext.sentence_transformers.model import SentenceTransformer\n",
    "from superduperdb.base.code import Code\n",
    "\n",
    "def postprocess(x):\n",
    "    return x.tolist()\n",
    "\n",
    "datatype = vector(shape=384, identifier=\"my-vec\")\n",
    "    \n",
    "model = SentenceTransformer(\n",
    "    identifier=\"my-embedding\",\n",
    "    datatype=datatype,\n",
    "    predict_kwargs={\"show_progress_bar\": True},\n",
    "    signature=\"*args,**kwargs\",\n",
    "    model=\"all-MiniLM-L6-v2\",      \n",
    "    device=\"cpu\",\n",
    "    postprocess=Code.from_object(postprocess),\n",
    ")\n",
    "\n",
    "listener = Listener(\n",
    "    identifier=\"my-listener\",\n",
    "    model=model,\n",
    "    key='txt',\n",
    "    select=db['docu'].find(),\n",
    "    predict_kwargs={'max_chunk_size': 50},\n",
    ")\n",
    "\n",
    "vector_index = VectorIndex(\n",
    "    identifier=\"my-index\",\n",
    "    indexing_listener=listener,\n",
    "    measure=\"cosine\"\n",
    ")\n",
    "\n",
    "db.apply(vector_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82d6525-7216-416f-88a5-8e48beb39045",
   "metadata": {},
   "source": [
    "Now that we've set up a `VectorIndex`, we can connect this index with an LLM in a number of ways.\n",
    "A simple way to do that is with the `SequentialModel`. The first part of the `SequentialModel`\n",
    "executes a query and provides the results to the LLM in the second part. \n",
    "\n",
    "The `RetrievalPrompt` component takes a query with a \"free\" `Variable` as input. \n",
    "This gives users great flexibility with regard to how they fetch the context\n",
    "for their downstream models.\n",
    "\n",
    "We're using OpenAI, but you can use any type of LLm with `superduperdb`. We have several \n",
    "native integrations (see [here](../ai_integraitons/)) but you can also [bring your own model](../models/bring_your_own_model.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b5b77b-cdb3-4c76-a5c4-bbc57519badb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from superduperdb.ext.llm.prompter import *\n",
    "from superduperdb.base.variables import Variable\n",
    "from superduperdb import Document\n",
    "from superduperdb.components.model import SequentialModel\n",
    "from superduperdb.ext.openai import OpenAIChatCompletion\n",
    "\n",
    "q = db['docu'].like(Document({'txt': Variable('prompt')}), vector_index='my-index', n=5).find().limit(10)\n",
    "\n",
    "def get_output(c):\n",
    "    return [r['txt'] for r in c]\n",
    "\n",
    "prompt_template = RetrievalPrompt('my-prompt', select=q, postprocess=Code.from_object(get_output))\n",
    "\n",
    "llm = OpenAIChatCompletion(\n",
    "    'gpt-3.5-turbo',\n",
    "    client_kwargs={'api_key': '<OPENAI_API_KEY>'},\n",
    ")\n",
    "seq = SequentialModel('rag', models=[prompt_template, llm])\n",
    "\n",
    "db.apply(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75784c5-bfbc-4892-8788-e0f9e576c072",
   "metadata": {},
   "source": [
    "Now we can test the `SequentialModel` with a sample question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428caeee-5e82-4268-9bd5-1499fc21667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq.predict_one('Tell be about vector-indexes?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda45127-5cfc-42e9-90cb-3ea987e9281f",
   "metadata": {},
   "source": [
    ":::tip\n",
    "Did you know you can use any tools from the Python ecosystem with `superduperdb`.\n",
    "That includes `langchain` and `llamaindex` which can be very useful for RAG applications.\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
