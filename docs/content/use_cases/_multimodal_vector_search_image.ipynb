{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38c1a328-fd86-4c5f-bd54-b8664f433608",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "# Multimodal vector search - Image"
   ]
  },
  {
   "cell_type": "raw",
   "id": "34fc0ff3-6134-46ef-bbab-dbe4deb1893a",
   "metadata": {},
   "source": [
    "<snippet: configure_your_production_system: *>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5de54e82-6103-411d-a9b0-225510ad6251",
   "metadata": {},
   "source": [
    "<snippet: start_your_cluster: *>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "db0ce690-8ff7-4988-a377-6b0d8a1fa9f8",
   "metadata": {},
   "source": [
    "<snippet: connect_to_superduperdb: *>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8a84db8b-e9ec-4356-b045-b718de8ccb6c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<snippet: get_useful_sample_data: Image>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a702b1-faf9-4edb-8a55-efc4add84a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = [{'img': d} for d in data[:100]]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa0c03b6-891a-4f54-9954-7a2282150b17",
   "metadata": {},
   "source": [
    "<snippet: insert_simple_data: *>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296b245a-6254-4219-a8e9-4a6360cff5c3",
   "metadata": {},
   "source": [
    "## Build multimodal embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7b2646-8693-41dc-98e9-3c15b78ee68e",
   "metadata": {},
   "source": [
    "We define the output data type of a model as a vector for vector transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5375ff04-8b6d-40b0-a8b2-6aa124636871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: MongoDB>\n",
    "from superduperdb.components.vector_index import vector\n",
    "output_datatpye = vector(shape=(1024,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489a19c2-c673-4bf9-9bea-f4bb279fc462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: SQL>\n",
    "from superduperdb.components.vector_index import sqlvector\n",
    "output_datatpye = sqlvector(shape=(1024,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40772155-8a01-4577-a9c9-7928fcd012f6",
   "metadata": {},
   "source": [
    "Then define two models, one for text embedding and one for image embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8751ede0-4b9f-4f92-b4ec-f6b0e0740c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Text-Image>\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "import clip\n",
    "from superduperdb import vector\n",
    "from superduperdb.ext.torch import TorchModel\n",
    "\n",
    "# Load the CLIP model and obtain the preprocessing function\n",
    "model, preprocess = clip.load(\"RN50\", device='cpu')\n",
    "\n",
    "# Create a TorchModel for text encoding\n",
    "compatible_model = TorchModel(\n",
    "    identifier='clip_text', # Unique identifier for the model\n",
    "    object=model, # CLIP model\n",
    "    preprocess=lambda x: clip.tokenize(x)[0],  # Model input preprocessing using CLIP \n",
    "    postprocess=lambda x: x.tolist(), # Convert the model output to a list\n",
    "    datatype=output_datatpye,  # Vector encoder with shape (1024,)\n",
    "    forward_method='encode_text', # Use the 'encode_text' method for forward pass \n",
    ")\n",
    "\n",
    "# Create a TorchModel for visual encoding\n",
    "model = TorchModel(\n",
    "    identifier='clip_image',  # Unique identifier for the model\n",
    "    object=model.visual,  # Visual part of the CLIP model    \n",
    "    preprocess=preprocess, # Visual preprocessing using CLIP\n",
    "    postprocess=lambda x: x.tolist(), # Convert the output to a list \n",
    "    datatype=output_datatpye, # Vector encoder with shape (1024,)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0119da-9cfd-4a60-8847-c3bfdf37697f",
   "metadata": {},
   "source": [
    "Because we use multimodal models, we define different keys to specify which model to use for embedding calculations in the vector_index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12e75fab-8504-4d17-a7d9-f98667a5d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexing_key = 'img' # we use img key for img embedding\n",
    "compatible_key = 'text' # we use text key for text embedding"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7eaca47f-32ab-4776-abc0-f6fa2eeb9043",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<snippet: create_vector_index: 2-Modalities>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a87f9d-581a-419a-81b8-a743250413e9",
   "metadata": {},
   "source": [
    "## Perform a vector search\n",
    "\n",
    "We can perform the vector searches using two types of data:\n",
    "\n",
    "- Text: By text description, we can find images similar to the text description.\n",
    "- Image: By using an image, we can find images similar to the provided image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce565823-4655-488c-8684-2240107fa30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Text>\n",
    "item = Document({compatible_key: \"Find a black dog\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8059626-dff8-4fe0-b872-97b8eb8b1b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Image>\n",
    "from IPython.display import display\n",
    "search_image = data[0]\n",
    "display(search_image)\n",
    "item = Document({indexing_key: search_image})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3ba07d-1124-4d94-a117-60d2e72581f7",
   "metadata": {},
   "source": [
    "Once we have this search target, we can execute a search as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a061de0b-2694-4b36-844c-7753a465360f",
   "metadata": {},
   "outputs": [],
   "source": [
    "select = query_table_or_collection.like(item, vector_index=vector_index_name, n=5).select()\n",
    "results = list(db.execute(select))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6d9af9-a012-42bd-aad4-31b92d089caa",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2ecea5-3a58-457c-ac50-ddc742484f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "for result in results:\n",
    "    display(result[indexing_key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693b4878-39a2-444d-8e17-72a00e6c246d",
   "metadata": {},
   "source": [
    "## Check the system stays updated\n",
    "\n",
    "You can add new data; once the data is added, all related models will perform calculations according to the underlying constructed model and listener, simultaneously updating the vector index to ensure that each query uses the latest data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef97f5a-bb41-46ca-a85e-489824741216",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_datas = [{'img': d} for d in data[100:200]]\n",
    "ids = db.execute(table_or_collection.insert(new_datas))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
