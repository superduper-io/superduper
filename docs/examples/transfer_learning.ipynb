{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1c66caf",
   "metadata": {},
   "source": [
    "# Transfer learning using Sentence Transformers and Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc151f6",
   "metadata": {},
   "source": [
    "In this example, we'll be demonstrating how to simply implement transfer learning using SuperDuperDB.\n",
    "You'll find related examples on vector-search and simple training examples using scikit-learn in the \n",
    "the notebooks directory of the project. Transfer learning leverages similar components, and may be used synergistically with vector-search. Vectors are, after all, simultaneously featurizations of \n",
    "data and may be used in downstream learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73ff7b9",
   "metadata": {},
   "source": [
    "Let's first connect to MongoDB via SuperDuperDB, you read explanations of how to do this in \n",
    "the docs, and in the `notebooks/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44f8ef76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:faiss.loader:Loading faiss.\n",
      "INFO:faiss.loader:Successfully loaded faiss.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "from superduperdb import superduper\n",
    "from superduperdb.db.mongodb.query import Collection\n",
    "import pymongo\n",
    "\n",
    "db = superduper(\n",
    "    pymongo.MongoClient().documents\n",
    ")\n",
    "\n",
    "collection = Collection('transfer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fede97",
   "metadata": {},
   "source": [
    "We'll use textual data labelled with sentiment, to test the functionality. Transfer learning \n",
    "can be used on any data which can be processed with SuperDuperDB models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bb65106",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset imdb (/Users/dodo/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1758ba08dbf34f809c3f1ce3599eaafc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:found 0 uris\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document({'_id': ObjectId('64c96de74107fec93297b820'), '_fold': 'train', 'text': 'Most folks might say that if one were to spend a Saturday night watching a movie,you must be really bored. Actually,I had just gotten back home from being out and turned on the TV and there it was,\"Paulie\". <br /><br />I had missed the opening credits,so I didn\\'t know the name of it but I saw that it had Cheech Marin in it,so I naturally thought I had tuned into \"Born In East L.A.\" When I saw him talking to a talking parrot,I was ready to dismiss this as the kind of flop movie they show late in the night.<br /><br />Happy to say,it was better than that. As you know,if you don\\'t already Paulie is lost and trying to get back to his original owner. Seems it\\'s taken years to find her. What should be Paulie\\'s advantage is actually a dis-advantage in ways. People come across a literate parrot and all they see is a way to make money or benefit themselves. <br /><br />While Cheech Marin\\'s character \"is\" making money from him,he\\'s not mean to him. The dance sequences with the parrots are something kids will find cute,I found them amusing.<br /><br />Paulie,who\\'s naive\\',learns quickly that not all humans are nice people. Especially the owner of an animal research lab. The man lies to him saying he\\'ll help him find his owner as long as he helps in his animal communications research. Paulie is now stuck but decides to make a fool out of the man at a demonstration to others of Paulie\\'s vocal ability.<br /><br />Throughout the film Paulie\\'s telling his story to the facilities janitor who ends up freeing him,several other animals and finding the location of Paulie\\'s owner. It\\'s a touching reunion. <br /><br />8 out of 10,the wing clipping scene should have been assumed and not shown. That part might bother younger children. Otherwise,it\\'s a great movie for older kids and adults who are a kid at heart. (END)', 'label': 1})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "from datasets import load_dataset\n",
    "\n",
    "from superduperdb.container.document import Document as D\n",
    "\n",
    "data = load_dataset(\"imdb\")\n",
    "\n",
    "train_data = [\n",
    "    D({'_fold': 'train', **data['train'][int(i)]}) \n",
    "    for i in numpy.random.permutation(len(data['train']))\n",
    "][:5000]\n",
    "\n",
    "valid_data = [\n",
    "    D({'_fold': 'valid', **data['test'][int(i)]}) \n",
    "    for i in numpy.random.permutation(len(data['test']))\n",
    "][:500]\n",
    "\n",
    "db.execute(collection.insert_many(train_data))\n",
    "\n",
    "r = db.execute(collection.find_one())\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a92214",
   "metadata": {},
   "source": [
    "Let's create a SuperDuperDB model based on a `sentence_transformers` model.\n",
    "You'll notice that we don't necessarily need a native SuperDuperDB integration to a model library \n",
    "in order to leverage its power with SuperDuperDB. For example, in this case, we just need \n",
    "to configure the `Model` wrapper to interoperate correctly with the `SentenceTransformer` class. After doing this, we can link the model to a collection, and daemonize the model using the `listen=True` keyword:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef91c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /var/folders/y9/b74b9yj906s_wtj0rrh2lf7c0000gn/T/tmpafi9hkj6\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /var/folders/y9/b74b9yj906s_wtj0rrh2lf7c0000gn/T/tmpafi9hkj6/_remote_module_non_scriptable.py\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cpu\n",
      "WARNING:root:model/all-MiniLM-L6-v2/0 already exists - doing nothing\n",
      "WARNING:root:model/all-MiniLM-L6-v2/0 already exists - doing nothing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a531fc7e639546cd8e5749034190f20c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from superduperdb.container.model import Model\n",
    "import sentence_transformers\n",
    "\n",
    "from superduperdb.ext.numpy.array import array\n",
    "\n",
    "m = Model(\n",
    "    identifier='all-MiniLM-L6-v2',\n",
    "    object=sentence_transformers.SentenceTransformer('all-MiniLM-L6-v2'),\n",
    "    encoder=array('float32', shape=(384,)),\n",
    "    predict_method='encode',\n",
    "    batch_predict=True,\n",
    ")\n",
    "\n",
    "m.predict(\n",
    "    X='text',\n",
    "    db=db,\n",
    "    select=collection.find(),\n",
    "    listen=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fefc17",
   "metadata": {},
   "source": [
    "Now that we've created and added the model which computes features for the `\"text\"`, we can train a \n",
    "downstream model using Scikit-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2faeeb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model = superduper(\n",
    "    SVC(gamma='scale', class_weight='balanced', C=100, verbose=True),\n",
    "    postprocess=lambda x: int(x)\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X='text',\n",
    "    y='label',\n",
    "    db=db,\n",
    "    select=collection.find().featurize({'text': 'all-MiniLM-L6-v2'}),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e1f164",
   "metadata": {},
   "source": [
    "Now that the model has been trained, we can apply the model to the database, also daemonizing the model \n",
    "with `listen=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee16436",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(\n",
    "    X='text',\n",
    "    db=db,\n",
    "    select=collection.find().featurize({'text': 'all-MiniLM-L6-v2'}),\n",
    "    listen=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b156c1",
   "metadata": {},
   "source": [
    "To verify that this process has worked, we can sample a few records, to inspect the sanity of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76958a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = next(db.execute(collection.aggregate([{'$sample': {'size': 1}}])))\n",
    "print(r['text'][:100])\n",
    "print(r['_outputs']['text']['svc'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
