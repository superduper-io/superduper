{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38c1a328-fd86-4c5f-bd54-b8664f433608",
   "metadata": {},
   "source": [
    "# Multimodal vector search - images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f8484d-2e35-472a-9b24-1a30ec1d144b",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "## Connect to superduper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d66021-ce62-4021-a2c5-158dee92b3bb",
   "metadata": {},
   "source": [
    ":::note\n",
    "Note that this is only relevant if you are running superduper in development mode.\n",
    "Otherwise refer to \"Configuring your production system\".\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6614595a-49e4-41ae-abe7-97e6251c79bf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "APPLY = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb029a5e-fedf-4f07-8a31-d220cfbfbb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper import superduper\n",
    "\n",
    "db = superduper('mongomock:///test_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032c2e7b-3f54-4263-b778-0fef60596efb",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "## Get useful sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c2a973-56bc-4a43-b375-6055bba40f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getter():\n",
    "    import subprocess\n",
    "    subprocess.run([\n",
    "        'curl', '-O', 'https://superduperdb-public-demo.s3.amazonaws.com/images_classification.zip',\n",
    "    ])\n",
    "    subprocess.run(['rm', '-rf', 'images'])\n",
    "    subprocess.run(['rm', '-rf', '__MACOSX'])\n",
    "    subprocess.run(['unzip', 'images_classification.zip'])\n",
    "    subprocess.run(['rm', 'images_classification.zip'])\n",
    "    import json\n",
    "    from PIL import Image\n",
    "    with open('images/images.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "    data = data[:100]\n",
    "    data = [{'img': Image.open(r['image_path'])} for r in data]\n",
    "    subprocess.run(['rm', '-rf', '__MACOSX'])\n",
    "    subprocess.run(['rm', '-rf', 'images'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1d2a2e-dea4-4908-a479-0be6bc10f0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if APPLY:\n",
    "    data = getter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296b245a-6254-4219-a8e9-4a6360cff5c3",
   "metadata": {},
   "source": [
    "## Build multimodal embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7b2646-8693-41dc-98e9-3c15b78ee68e",
   "metadata": {},
   "source": [
    "We define the output data type of a model as a vector for vector transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489a19c2-c673-4bf9-9bea-f4bb279fc462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper.components.vector_index import sqlvector\n",
    "\n",
    "output_datatype = sqlvector(shape=(1024,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40772155-8a01-4577-a9c9-7928fcd012f6",
   "metadata": {},
   "source": [
    "Then define two models, one for text embedding and one for image embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8751ede0-4b9f-4f92-b4ec-f6b0e0740c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "from superduper import vector, imported\n",
    "from superduper_torch import TorchModel\n",
    "\n",
    "rn50 = imported(clip.load)('RN50', device='cpu')\n",
    "\n",
    "compatible_model = TorchModel(\n",
    "    identifier='clip_text',\n",
    "    object=rn50[0],\n",
    "    preprocess=lambda x: clip.tokenize(x)[0],\n",
    "    postprocess=lambda x: x.tolist(),\n",
    "    datatype=output_datatype,\n",
    "    forward_method='encode_text',\n",
    ")\n",
    "\n",
    "embedding_model = TorchModel(\n",
    "    identifier='clip_image',\n",
    "    object=rn50[0].visual,\n",
    "    preprocess=rn50[1],\n",
    "    postprocess=lambda x: x.tolist(),\n",
    "    datatype=output_datatype,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0119da-9cfd-4a60-8847-c3bfdf37697f",
   "metadata": {},
   "source": [
    "Because we use multimodal models, we define different keys to specify which model to use for embedding calculations in the vector_index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e75fab-8504-4d17-a7d9-f98667a5d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexing_key = 'img'\n",
    "compatible_key = 'text'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b8b40d-3750-4d7b-aa60-62e07b734b04",
   "metadata": {},
   "source": [
    "## Create vector-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ee3ff4-880e-477b-bbdf-5b8d89c56de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index_name = 'my-vector-index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cede653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper import VectorIndex, Listener\n",
    "\n",
    "vector_index = VectorIndex(\n",
    "    vector_index_name,\n",
    "    indexing_listener=Listener(\n",
    "        key=indexing_key,\n",
    "        select=db['docs'].select(),\n",
    "        model=embedding_model,\n",
    "        identifier='indexing-listener',\n",
    "    ),\n",
    "    compatible_listener=Listener(\n",
    "        key=compatible_key,\n",
    "        model=compatible_model,\n",
    "        select=None,\n",
    "        identifier='compatible-listener',\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2508c32b-1db2-4b45-a1d3-8b0e352d59a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper import Application\n",
    "\n",
    "application = Application(\n",
    "    'image-vector-search',\n",
    "    components=[vector_index],\n",
    ")\n",
    "\n",
    "if APPLY:\n",
    "    db.apply(application, force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339180f6-9855-4c5d-ae70-78040d72d170",
   "metadata": {},
   "source": [
    "## Add the data\n",
    "\n",
    "The order in which data is added is not important. *However* if your data requires a custom `Schema` in order to work, it's easier to add the `Application` first, and the data later. The advantage of this flexibility, is that once the `Application` is installed, it's waiting for incoming data, so that the `Application` is always up-to-date. This comes in particular handy with AI scenarios which need to respond to changing news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b5bc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "if APPLY:\n",
    "    from superduper import Document\n",
    "    \n",
    "    table_or_collection = db['docs']\n",
    "    \n",
    "    ids = db.execute(table_or_collection.insert([Document(r) for r in data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a87f9d-581a-419a-81b8-a743250413e9",
   "metadata": {},
   "source": [
    "## Perform a vector search\n",
    "\n",
    "We can perform the vector searches using two types of data:\n",
    "\n",
    "- Text: By text description, we can find images similar to the text description.\n",
    "- Image: By using an image, we can find images similar to the provided image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce565823-4655-488c-8684-2240107fa30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if APPLY:\n",
    "    item = Document({compatible_key: \"Find a black dog.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8059626-dff8-4fe0-b872-97b8eb8b1b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "if APPLY:\n",
    "    from IPython.display import display\n",
    "    search_image = data[0]\n",
    "    display(search_image)\n",
    "    item = Document(search_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3ba07d-1124-4d94-a117-60d2e72581f7",
   "metadata": {},
   "source": [
    "Once we have this search target, we can execute a search as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a061de0b-2694-4b36-844c-7753a465360f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if APPLY:\n",
    "    select = db['docs'].like(item, vector_index=vector_index_name, n=5).select()\n",
    "\n",
    "    results = list(db.execute(select))\n",
    "\n",
    "    from IPython.display import display\n",
    "    for result in results:\n",
    "        display(result[indexing_key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1390e3b-d942-4442-9071-ef8045d96847",
   "metadata": {},
   "source": [
    "## Create a `Template`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118bc2d4-5313-4584-8607-1eacfff09660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper import Template, Table, Schema\n",
    "from superduper.components.dataset import RemoteData\n",
    "from superduper_pillow import pil_image\n",
    "\n",
    "\n",
    "template = Template(\n",
    "    'multimodal_image_search',\n",
    "    template=application,\n",
    "    default_table=Table(\n",
    "        'sample_multimodal_image_search', \n",
    "        schema=Schema(\n",
    "            'sample_multimodal_image_search/schema',\n",
    "            fields={'img': pil_image},\n",
    "        ),\n",
    "        data=RemoteData('sample_images', getter=getter),\n",
    "    ),\n",
    "    substitutions={'docs': 'table_name', 'cpu': 'device'},\n",
    "    types={\n",
    "        'device': {\n",
    "            'type': 'str',\n",
    "            'default': 'cpu',\n",
    "        },\n",
    "        'table_name': {\n",
    "            'type': 'str',\n",
    "            'default': 'sample_multimodal_image_search',\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "template.export('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9576b349-6297-4b9b-9a1f-f86f9e5a2a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "template.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51323cca-9e69-4452-9b54-2a307eb5b5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index.indexing_listener.select"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
