{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ca02914-dac0-42ac-ac90-d1ebe87e6774",
   "metadata": {},
   "source": [
    "# Basic RAG tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ace1e3-1b4f-4c73-9a95-ae6116373a57",
   "metadata": {},
   "source": [
    ":::info\n",
    "In this tutorial we show you how to do retrieval augmented generation (RAG) with `Superduper`.\n",
    "Note that this is just an example of the flexibility and power which `Superduper` gives \n",
    "to developers. `Superduper` is about much more than RAG and LLMs. \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed5486d-133d-468f-befa-014f81ffbc94",
   "metadata": {},
   "source": [
    "As in the vector-search tutorial we'll use `Superduper` documentation for the tutorial.\n",
    "We'll add this to a testing database by downloading the data snapshot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "693f2a31-e443-499a-88a5-55f4d26de446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  720k  100  720k    0     0   679k      0  0:00:01  0:00:01 --:--:--  681k\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://superduper-public-demo.s3.amazonaws.com/text.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296ae5f2-32a9-4f80-aeb7-44e82baf749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from superduper import superduper, Document\n",
    "\n",
    "db = superduper('mongomock://test')\n",
    "\n",
    "with open('text.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "_ = db['docu'].insert_many([{'txt': r} for r in data]).execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe6ccbe-e12d-4d89-8559-6875d047b593",
   "metadata": {},
   "source": [
    "Let's verify the data in the `db` by querying one datapoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c97c8eb-5ffe-4fe6-87fd-26c9853b3a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "db['docu'].find_one().execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad947dd-e848-42f3-93b3-7697a8816ed2",
   "metadata": {},
   "source": [
    "The first step in a RAG application is to create a `VectorIndex`. The results of searching \n",
    "with this index will be used as input to the LLM for answering questions.\n",
    "\n",
    "Read about `VectorIndex` [here](../apply_api/vector_index.md) and follow along the tutorial on \n",
    "vector-search [here](./vector_search.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f443ee6-3ff4-4b24-9767-4d295ea1bc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "from superduper import Application, Document, VectorIndex, Listener, vector\n",
    "from superduper.ext.sentence_transformers.model import SentenceTransformer\n",
    "from superduper.base.code import Code\n",
    "\n",
    "def postprocess(x):\n",
    "    return x.tolist()\n",
    "\n",
    "datatype = vector(shape=384, identifier=\"my-vec\")\n",
    "    \n",
    "model = SentenceTransformer(\n",
    "    identifier=\"my-embedding\",\n",
    "    datatype=datatype,\n",
    "    predict_kwargs={\"show_progress_bar\": True},\n",
    "    signature=\"*args,**kwargs\",\n",
    "    model=\"all-MiniLM-L6-v2\",      \n",
    "    device=\"cpu\",\n",
    "    postprocess=Code.from_object(postprocess),\n",
    ")\n",
    "\n",
    "listener = Listener(\n",
    "    identifier=\"my-listener\",\n",
    "    model=model,\n",
    "    key='txt',\n",
    "    select=db['docu'].find(),\n",
    "    predict_kwargs={'max_chunk_size': 50},\n",
    ")\n",
    "\n",
    "vector_index = VectorIndex(\n",
    "    identifier=\"my-index\",\n",
    "    indexing_listener=listener,\n",
    "    measure=\"cosine\"\n",
    ")\n",
    "\n",
    "db.apply(vector_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82d6525-7216-416f-88a5-8e48beb39045",
   "metadata": {},
   "source": [
    "Now that we've set up a `VectorIndex`, we can connect this index with an LLM in a number of ways.\n",
    "A simple way to do that is with the `SequentialModel`. The first part of the `SequentialModel`\n",
    "executes a query and provides the results to the LLM in the second part. \n",
    "\n",
    "The `RetrievalPrompt` component takes a query with a \"free\" variable as input, signified with `<var:???>`. \n",
    "This gives users great flexibility with regard to how they fetch the context\n",
    "for their downstream models.\n",
    "\n",
    "We're using OpenAI, but you can use any type of LLM with `Superduper`. We have several \n",
    "native integrations (see [here](../ai_integraitons/)) but you can also [bring your own model](../models/bring_your_own_model.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b5b77b-cdb3-4c76-a5c4-bbc57519badb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from superduper.ext.llm.prompter import *\n",
    "from superduper import Document\n",
    "from superduper.components.model import SequentialModel\n",
    "from superduper.ext.openai import OpenAIChatCompletion\n",
    "\n",
    "q = db['docu'].like(Document({'txt': '<var:prompt>'}), vector_index='my-index', n=5).find().limit(10)\n",
    "\n",
    "def get_output(c):\n",
    "    return [r['txt'] for r in c]\n",
    "\n",
    "prompt_template = RetrievalPrompt('my-prompt', select=q, postprocess=Code.from_object(get_output))\n",
    "\n",
    "llm = OpenAIChatCompletion('gpt-3.5-turbo')\n",
    "seq = SequentialModel('rag', models=[prompt_template, llm])\n",
    "\n",
    "db.apply(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75784c5-bfbc-4892-8788-e0f9e576c072",
   "metadata": {},
   "source": [
    "Now we can test the `SequentialModel` with a sample question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428caeee-5e82-4268-9bd5-1499fc21667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq.predict('Tell be about vector-indexes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda45127-5cfc-42e9-90cb-3ea987e9281f",
   "metadata": {},
   "source": [
    ":::tip\n",
    "Did you know you can use any tools from the Python ecosystem with `Superduper`.\n",
    "That includes `langchain` and `llamaindex` which can be very useful for RAG applications.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314bbc1a-a189-402d-be37-9f340c25734d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper import Application\n",
    "\n",
    "app = Application('rag-app', components=[vector_index, seq, plugin_1, plugin_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86198631-1aef-4d78-8b11-0d99edea1baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "app.encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069ec70c-93f2-41aa-b0f8-4d2a154d15f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "app.export('rag-app')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb33df7-a908-42bb-a825-a00b2a66d1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat rag-app/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "913a104d-64c8-45f8-9de3-265ca4845c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dodo/.pyenv/versions/3.11.7/envs/superduper-3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from superduper import *\n",
    "\n",
    "app = Component.read('rag-app')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "409f8c71-41c2-46f8-a588-225c36860bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-Jun-17 09:42:33.43| INFO     | Duncans-MBP.fritz.box| superduper.base.document:362  | Building leaf <class 'superduper.components.vector_index.VectorIndex'> with identifier: my-index\n",
      "2024-Jun-17 09:42:33.43| INFO     | Duncans-MBP.fritz.box| superduper.base.document:362  | Building leaf <class 'superduper.components.listener.Listener'> with identifier: my-listener\n",
      "2024-Jun-17 09:42:33.43| INFO     | Duncans-MBP.fritz.box| superduper.base.document:362  | Building leaf <class 'superduper.ext.sentence_transformers.model.SentenceTransformer'> with identifier: my-embedding\n",
      "2024-Jun-17 09:42:33.44| INFO     | Duncans-MBP.fritz.box| superduper.base.document:362  | Building leaf <class 'superduper.components.datatype.DataType'> with identifier: my-vec\n",
      "2024-Jun-17 09:42:33.44| INFO     | Duncans-MBP.fritz.box| superduper.base.document:362  | Building leaf <class 'superduper.base.code.Code'> with identifier: postprocess\n",
      "2024-Jun-17 09:42:33.44| INFO     | Duncans-MBP.fritz.box| superduper.base.document:362  | Building leaf <class 'superduper.backends.mongodb.query.MongoQuery'> with identifier: docu-find\n",
      "2024-Jun-17 09:42:33.44| INFO     | Duncans-MBP.fritz.box| superduper.base.document:362  | Building leaf <class 'superduper.components.model.SequentialModel'> with identifier: rag\n",
      "2024-Jun-17 09:42:33.44| INFO     | Duncans-MBP.fritz.box| superduper.base.document:362  | Building leaf <class 'superduper.ext.llm.prompter.RetrievalPrompt'> with identifier: my-prompt\n",
      "2024-Jun-17 09:42:33.44| INFO     | Duncans-MBP.fritz.box| superduper.base.document:362  | Building leaf <class 'superduper.base.code.Code'> with identifier: get_output\n",
      "2024-Jun-17 09:42:33.44| INFO     | Duncans-MBP.fritz.box| superduper.base.document:362  | Building leaf <class 'superduper.backends.mongodb.query.MongoQuery'> with identifier: docu-like-txt-var-prompt-vector-index-my-index-n-5-find-limit-10\n",
      "2024-Jun-17 09:42:33.44| INFO     | Duncans-MBP.fritz.box| superduper.base.document:362  | Building leaf <class 'superduper.ext.openai.model.OpenAIChatCompletion'> with identifier: gpt-3.5-turbo\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">╭──────────────────────────────────────────────────── rag-app ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> <span style=\"color: #800080; text-decoration-color: #800080\">identifier</span>: <span style=\"color: #000080; text-decoration-color: #000080\">rag-app</span>                                                                                             <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> <span style=\"color: #800080; text-decoration-color: #800080\">uuid</span>: <span style=\"color: #000080; text-decoration-color: #000080\">9115f5ec-5575-4a11-8678-664f3904bab7</span>                                                                      <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> <span style=\"color: #800080; text-decoration-color: #800080\">components</span>: <span style=\"color: #000080; text-decoration-color: #000080\">[VectorIndex(identifier='my-index', uuid='650db68c-8786-4204-bc2d-6cc4f1d2511c', </span>                   <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> <span style=\"color: #000080; text-decoration-color: #000080\">indexing_listener=Listener(identifier='my-listener', uuid='02f5b3d4-7a0a-48d8-990c-bdae29424038', key='txt', </span>   <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> <span style=\"color: #000080; text-decoration-color: #000080\">model=SentenceTransformer(preferred_devices=('cuda', 'mps', 'cpu'), device='cpu', identifier='my-embedding', </span>   <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> <span style=\"color: #000080; text-decoration-color: #000080\">uuid='b1351454-3714-4c57-bacf-2f2a667d5fdc', signature='*args,**kwargs', datatype=DataType(identifier='my-vec',</span> <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> <span style=\"color: #000080; text-decoration-color: #000080\">uuid='ecfbe6d5-5c1f-4b80-b224-aaf0a1f3ee1d', encoder=None, decoder=None, info=None, shape=(384,), </span>              <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> <span style=\"color: #000080; text-decoration-color: #000080\">directory=None, encodable='native', bytes_encoding=&lt;BytesEncoding.BYTES: 'Bytes'&gt;, intermediate_type='bytes', </span>  <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> <span style=\"color: #000080; text-decoration-color: #000080\">media_type=None), output_schema=None, flatten=False, model_update_kwargs={}, </span>                                   <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> <span style=\"color: #000080; text-decoration-color: #000080\">predict_kwargs={'show_progress_bar': True}, compute_kwargs={}, validation=None, metric_values={}, </span>              <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> <span style=\"color: #000080; text-decoration-color: #000080\">num_workers=0, object=SentenceTransformer(</span>                                                                      <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> <span style=\"color: #000080; text-decoration-color: #000080\">  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel </span>          <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> <span style=\"color: #000080; text-decoration-color: #000080\">  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': </span>  <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> <span style=\"color: #000080; text-decoration-color: #000080\">True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, </span>                            <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> <span style=\"color: #000080; text-decoration-color: #000080\">'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})</span>            <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> <span style=\"color: #000080; text-decoration-color: #000080\">  (2): Normalize()</span>                                                                                              <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> <span style=\"color: #000080; text-decoration-color: #000080\">), model='all-MiniLM-L6-v2', preprocess=None, postprocess=Code(identifier='postprocess', </span>                       <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> <span style=\"color: #000080; text-decoration-color: #000080\">uuid='fadfa78c-4c6b-4914-885a-e1372da93078', code='from superduper import code\\n\\n@code\\ndef </span>                 <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> <span style=\"color: #000080; text-decoration-color: #000080\">postprocess(x):\\n    return x.tolist()\\n')), select=docu.find(), active=True, predict_kwargs={'max_chunk_size':</span> <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> <span style=\"color: #000080; text-decoration-color: #000080\">50}), compatible_listener=None, measure=&lt;VectorIndexMeasureType.cosine: 'cosine'&gt;, metric_values={}), </span>          <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> <span style=\"color: #000080; text-decoration-color: #000080\">SequentialModel(identifier='rag', uuid='fa46eb15-112c-496f-965f-c935494825c5', signature='**kwargs', </span>           <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> <span style=\"color: #000080; text-decoration-color: #000080\">datatype=None, output_schema=None, flatten=False, model_update_kwargs={}, predict_kwargs={}, compute_kwargs={},</span> <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> <span style=\"color: #000080; text-decoration-color: #000080\">validation=None, metric_values={}, num_workers=0, models=[RetrievalPrompt(identifier='my-prompt', </span>              <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> <span style=\"color: #000080; text-decoration-color: #000080\">uuid='ded3b9b8-828d-41a4-bc37-02217fe0bc08', signature='**kwargs', datatype=None, output_schema=None, </span>          <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> <span style=\"color: #000080; text-decoration-color: #000080\">flatten=False, model_update_kwargs={}, predict_kwargs={}, compute_kwargs={}, validation=None, metric_values={},</span> <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> <span style=\"color: #000080; text-decoration-color: #000080\">num_workers=0, preprocess=None, postprocess=Code(identifier='get_output', </span>                                      <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> <span style=\"color: #000080; text-decoration-color: #000080\">uuid='c1d6fb70-b6c7-42b4-8872-8bfd243ddf07', code=\"from superduper import code\\n\\n@code\\ndef get_output(c):\\n</span> <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> <span style=\"color: #000080; text-decoration-color: #000080\">return [r['txt'] for r in c]\\n\"), select=docu.like({'txt': '&lt;var:prompt&gt;'}, vector_index=\"my-index\", </span>           <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> <span style=\"color: #000080; text-decoration-color: #000080\">n=5).find().limit(10), prompt_explanation=\"HERE ARE SOME FACTS SEPARATED BY '---' IN OUR DATA REPOSITORY WHICH </span> <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> <span style=\"color: #000080; text-decoration-color: #000080\">WILL HELP YOU ANSWER THE QUESTION.\", prompt_introduction='HERE IS THE QUESTION WHICH YOU SHOULD ANSWER BASED </span>   <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> <span style=\"color: #000080; text-decoration-color: #000080\">ONLY ON THE PREVIOUS FACTS:', join='\\n---\\n'), OpenAIChatCompletion(identifier='gpt-3.5-turbo', </span>                <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> <span style=\"color: #000080; text-decoration-color: #000080\">uuid='bc04fcdf-3217-4cb7-9517-38fc632fc8f7', signature='singleton', datatype=None, output_schema=None, </span>         <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> <span style=\"color: #000080; text-decoration-color: #000080\">flatten=False, model_update_kwargs={}, predict_kwargs={}, compute_kwargs={}, validation=None, metric_values={},</span> <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> <span style=\"color: #000080; text-decoration-color: #000080\">num_workers=0, model='gpt-3.5-turbo', max_batch_size=8, openai_api_key=None, openai_api_base=None, </span>             <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> <span style=\"color: #000080; text-decoration-color: #000080\">client_kwargs={}, batch_size=1, prompt='')])]</span>                                                                   <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">╭────────────────────────────────────────────── Component Metadata ───────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">Variables</span>                                                                                                       <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> <span style=\"color: #800080; text-decoration-color: #800080\">prompt</span>                                                                                                          <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">Leaves</span>                                                                                                          <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m╭─\u001b[0m\u001b[1;32m───────────────────────────────────────────────────\u001b[0m\u001b[1;32m rag-app \u001b[0m\u001b[1;32m───────────────────────────────────────────────────\u001b[0m\u001b[1;32m─╮\u001b[0m\n",
       "\u001b[1;32m│\u001b[0m \u001b[35midentifier\u001b[0m: \u001b[34mrag-app\u001b[0m                                                                                             \u001b[1;32m│\u001b[0m\n",
       "\u001b[1;32m│\u001b[0m \u001b[35muuid\u001b[0m: \u001b[34m9115f5ec-5575-4a11-8678-664f3904bab7\u001b[0m                                                                      \u001b[1;32m│\u001b[0m\n",
       "\u001b[1;32m│\u001b[0m \u001b[35mcomponents\u001b[0m: \u001b[34m[VectorIndex(identifier='my-index', uuid='650db68c-8786-4204-bc2d-6cc4f1d2511c', \u001b[0m                   \u001b[1;32m│\u001b[0m\n",
       "\u001b[1;32m│\u001b[0m \u001b[34mindexing_listener=Listener(identifier='my-listener', uuid='02f5b3d4-7a0a-48d8-990c-bdae29424038', key='txt', \u001b[0m   \u001b[1;32m│\u001b[0m\n",
       "\u001b[1;32m│\u001b[0m \u001b[34mmodel=SentenceTransformer(preferred_devices=('cuda', 'mps', 'cpu'), device='cpu', identifier='my-embedding', \u001b[0m   \u001b[1;32m│\u001b[0m\n",
       "\u001b[1;32m│\u001b[0m \u001b[34muuid='b1351454-3714-4c57-bacf-2f2a667d5fdc', signature='*args,**kwargs', datatype=DataType(identifier='my-vec',\u001b[0m \u001b[1;32m│\u001b[0m\n",
       "\u001b[1;32m│\u001b[0m \u001b[34muuid='ecfbe6d5-5c1f-4b80-b224-aaf0a1f3ee1d', encoder=None, decoder=None, info=None, shape=(384,), \u001b[0m              \u001b[1;32m│\u001b[0m\n",
       "\u001b[1;32m│\u001b[0m \u001b[34mdirectory=None, encodable='native', bytes_encoding=<BytesEncoding.BYTES: 'Bytes'>, intermediate_type='bytes', \u001b[0m  \u001b[1;32m│\u001b[0m\n",
       "\u001b[1;32m│\u001b[0m \u001b[34mmedia_type=None), output_schema=None, flatten=False, model_update_kwargs={}, \u001b[0m                                   \u001b[1;32m│\u001b[0m\n",
       "\u001b[1;32m│\u001b[0m \u001b[34mpredict_kwargs={'show_progress_bar': True}, compute_kwargs={}, validation=None, metric_values={}, \u001b[0m              \u001b[1;32m│\u001b[0m\n",
       "\u001b[1;32m│\u001b[0m \u001b[34mnum_workers=0, object=SentenceTransformer(\u001b[0m                                                                      \u001b[1;32m│\u001b[0m\n",
       "\u001b[1;32m│\u001b[0m \u001b[34m  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \u001b[0m          \u001b[1;32m│\u001b[0m\n",
       "\u001b[1;32m│\u001b[0m \u001b[34m  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': \u001b[0m  \u001b[1;32m│\u001b[0m\n",
       "\u001b[1;32m│\u001b[0m \u001b[34mTrue, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, \u001b[0m                            \u001b[1;32m│\u001b[0m\n",
       "\u001b[1;32m│\u001b[0m \u001b[34m'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\u001b[0m            \u001b[1;32m│\u001b[0m\n",
       "\u001b[1;32m│\u001b[0m \u001b[34m  (2): Normalize()\u001b[0m                                                                                              \u001b[1;32m│\u001b[0m\n",
       "\u001b[1;32m│\u001b[0m \u001b[34m), model='all-MiniLM-L6-v2', preprocess=None, postprocess=Code(identifier='postprocess', \u001b[0m                       \u001b[1;32m│\u001b[0m\n",
       "\u001b[1;32m│\u001b[0m \u001b[34muuid='fadfa78c-4c6b-4914-885a-e1372da93078', code='from superduper import code\\n\\n@code\\ndef \u001b[0m                 \u001b[1;32m│\u001b[0m\n",
       "\u001b[1;32m│\u001b[0m \u001b[34mpostprocess(x):\\n    return x.tolist()\\n')), select=docu.find(), active=True, predict_kwargs={'max_chunk_size':\u001b[0m \u001b[1;32m│\u001b[0m\n",
       "\u001b[1;32m│\u001b[0m \u001b[34m50}), compatible_listener=None, measure=<VectorIndexMeasureType.cosine: 'cosine'>, metric_values={}), \u001b[0m          \u001b[1;32m│\u001b[0m\n",
       "\u001b[1;32m│\u001b[0m \u001b[34mSequentialModel(identifier='rag', uuid='fa46eb15-112c-496f-965f-c935494825c5', signature='**kwargs', \u001b[0m           \u001b[1;32m│\u001b[0m\n",
       "\u001b[1;32m│\u001b[0m \u001b[34mdatatype=None, output_schema=None, flatten=False, model_update_kwargs={}, predict_kwargs={}, compute_kwargs={},\u001b[0m \u001b[1;32m│\u001b[0m\n",
       "\u001b[1;32m│\u001b[0m \u001b[34mvalidation=None, metric_values={}, num_workers=0, models=[RetrievalPrompt(identifier='my-prompt', \u001b[0m              \u001b[1;32m│\u001b[0m\n",
       "\u001b[1;32m│\u001b[0m \u001b[34muuid='ded3b9b8-828d-41a4-bc37-02217fe0bc08', signature='**kwargs', datatype=None, output_schema=None, \u001b[0m          \u001b[1;32m│\u001b[0m\n",
       "\u001b[1;32m│\u001b[0m \u001b[34mflatten=False, model_update_kwargs={}, predict_kwargs={}, compute_kwargs={}, validation=None, metric_values={},\u001b[0m \u001b[1;32m│\u001b[0m\n",
       "\u001b[1;32m│\u001b[0m \u001b[34mnum_workers=0, preprocess=None, postprocess=Code(identifier='get_output', \u001b[0m                                      \u001b[1;32m│\u001b[0m\n",
       "\u001b[1;32m│\u001b[0m \u001b[34muuid='c1d6fb70-b6c7-42b4-8872-8bfd243ddf07', code=\"from superduper import code\\n\\n@code\\ndef get_output(c):\\n\u001b[0m \u001b[1;32m│\u001b[0m\n",
       "\u001b[1;32m│\u001b[0m \u001b[34mreturn [r['txt'] for r in c]\\n\"), select=docu.like({'txt': '<var:prompt>'}, vector_index=\"my-index\", \u001b[0m           \u001b[1;32m│\u001b[0m\n",
       "\u001b[1;32m│\u001b[0m \u001b[34mn=5).find().limit(10), prompt_explanation=\"HERE ARE SOME FACTS SEPARATED BY '---' IN OUR DATA REPOSITORY WHICH \u001b[0m \u001b[1;32m│\u001b[0m\n",
       "\u001b[1;32m│\u001b[0m \u001b[34mWILL HELP YOU ANSWER THE QUESTION.\", prompt_introduction='HERE IS THE QUESTION WHICH YOU SHOULD ANSWER BASED \u001b[0m   \u001b[1;32m│\u001b[0m\n",
       "\u001b[1;32m│\u001b[0m \u001b[34mONLY ON THE PREVIOUS FACTS:', join='\\n---\\n'), OpenAIChatCompletion(identifier='gpt-3.5-turbo', \u001b[0m                \u001b[1;32m│\u001b[0m\n",
       "\u001b[1;32m│\u001b[0m \u001b[34muuid='bc04fcdf-3217-4cb7-9517-38fc632fc8f7', signature='singleton', datatype=None, output_schema=None, \u001b[0m         \u001b[1;32m│\u001b[0m\n",
       "\u001b[1;32m│\u001b[0m \u001b[34mflatten=False, model_update_kwargs={}, predict_kwargs={}, compute_kwargs={}, validation=None, metric_values={},\u001b[0m \u001b[1;32m│\u001b[0m\n",
       "\u001b[1;32m│\u001b[0m \u001b[34mnum_workers=0, model='gpt-3.5-turbo', max_batch_size=8, openai_api_key=None, openai_api_base=None, \u001b[0m             \u001b[1;32m│\u001b[0m\n",
       "\u001b[1;32m│\u001b[0m \u001b[34mclient_kwargs={}, batch_size=1, prompt='')])]\u001b[0m                                                                   \u001b[1;32m│\u001b[0m\n",
       "\u001b[1;32m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[34m╭─\u001b[0m\u001b[34m─────────────────────────────────────────────\u001b[0m\u001b[34m Component Metadata \u001b[0m\u001b[34m──────────────────────────────────────────────\u001b[0m\u001b[34m─╮\u001b[0m\n",
       "\u001b[34m│\u001b[0m \u001b[33mVariables\u001b[0m                                                                                                       \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m \u001b[35mprompt\u001b[0m                                                                                                          \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m \u001b[33mLeaves\u001b[0m                                                                                                          \u001b[34m│\u001b[0m\n",
       "\u001b[34m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "app.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
