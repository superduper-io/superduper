{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38c1a328-fd86-4c5f-bd54-b8664f433608",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "# Retrieval augmented generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f8484d-2e35-472a-9b24-1a30ec1d144b",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "## Connect to superduper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d66021-ce62-4021-a2c5-158dee92b3bb",
   "metadata": {},
   "source": [
    ":::note\n",
    "Note that this is only relevant if you are running superduper in development mode.\n",
    "Otherwise refer to \"Configuring your production system\".\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb029a5e-fedf-4f07-8a31-d220cfbfbb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-Aug-25 20:58:32.27\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.misc.plugins\u001b[0m:\u001b[36m13  \u001b[0m | \u001b[1mLoading plugin: mongodb\u001b[0m\n",
      "\u001b[32m2024-Aug-25 20:58:32.37\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m103 \u001b[0m | \u001b[1mBuilding Data Layer\u001b[0m\n",
      "\u001b[32m2024-Aug-25 20:58:32.37\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.build\u001b[0m:\u001b[36m171 \u001b[0m | \u001b[1mConfiguration: \n",
      " +---------------+----------------------+\n",
      "| Configuration |        Value         |\n",
      "+---------------+----------------------+\n",
      "|  Data Backend | mongomock:///test_db |\n",
      "+---------------+----------------------+\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from superduper import superduper\n",
    "\n",
    "db = superduper('mongomock:///test_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032c2e7b-3f54-4263-b778-0fef60596efb",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "## Get useful sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e7902bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Text>\n",
    "# !curl -O https://superduperdb-public-demo.s3.amazonaws.com/text.json\n",
    "import json\n",
    "\n",
    "with open('text.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33486ec7-0316-4e0c-a409-c09ab4c16669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: PDF>\n",
    "!curl -O https://superduperdb-public-demo.s3.amazonaws.com/pdfs.zip && unzip -o pdfs.zip\n",
    "import os\n",
    "\n",
    "data = [f'pdfs/{x}' for x in os.listdir('./pdfs') if x.endswith('.pdf')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b745ed54-3818-4685-a3b5-6ab4e2afc44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = [{'x': d} for d in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ede8ae1",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "## Insert simple data\n",
    "\n",
    "After turning on auto_schema, we can directly insert data, and superduper will automatically analyze the data type, and match the construction of the table and datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5965fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-Aug-25 20:58:38.99\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m363 \u001b[0m | \u001b[1mTable docs does not exist, auto creating...\u001b[0m\n",
      "\u001b[32m2024-Aug-25 20:58:38.99\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m369 \u001b[0m | \u001b[1mCreating table docs with schema {('x', 'str'), ('_fold', 'str')}\u001b[0m\n",
      "\u001b[32m2024-Aug-25 20:58:38.99\u001b[0m| \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.document\u001b[0m:\u001b[36m415 \u001b[0m | \u001b[33m\u001b[1mLeaf str already exists\u001b[0m\n",
      "\u001b[32m2024-Aug-25 20:58:39.04\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m344 \u001b[0m | \u001b[1mInserted 210 documents into docs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from superduper import Document\n",
    "\n",
    "ids = db.execute(db['docs'].insert([Document(data) for data in datas]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fea927-ee4a-44cd-aaf2-634b574c316d",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "## Apply a chunker for search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d90bda-e8c4-494e-a38c-837fb63689ae",
   "metadata": {},
   "source": [
    ":::note\n",
    "Note that applying a chunker is ***not*** mandatory for search.\n",
    "If your data is already chunked (e.g. short text snippets or audio) or if you\n",
    "are searching through something like images, which can't be chunked, then this\n",
    "won't be necessary.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d20eaa0-a416-4483-938e-23f79845739a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Text>\n",
    "from superduper import model\n",
    "\n",
    "CHUNK_SIZE = 200\n",
    "\n",
    "@model(flatten=True, model_update_kwargs={})\n",
    "def chunker(text):\n",
    "    text = text.split()\n",
    "    chunks = [' '.join(text[i:i + CHUNK_SIZE]) for i in range(0, len(text), CHUNK_SIZE)]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facd7dc0-fffa-40d8-af72-2b9e4852ad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: PDF>\n",
    "!pip install -q \"unstructured[pdf]\"\n",
    "from superduper import model\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "CHUNK_SIZE = 500\n",
    "\n",
    "@model(flatten=True)\n",
    "def chunker(pdf_file):\n",
    "    elements = partition_pdf(pdf_file)\n",
    "    text = '\\n'.join([e.text for e in elements])\n",
    "    chunks = [text[i:i + CHUNK_SIZE] for i in range(0, len(text), CHUNK_SIZE)]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33a16f9-3bac-45bb-80ac-3ccf265dce5f",
   "metadata": {},
   "source": [
    "Now we apply this chunker to the data by wrapping the chunker in `Listener`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93d21872-d4dc-40dc-abab-fb07ba102ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper import Listener\n",
    "\n",
    "upstream_listener = Listener(\n",
    "    model=chunker,\n",
    "    select=db['docs'].select(),\n",
    "    key='x',\n",
    "    uuid=\"chunker\",\n",
    "    identifier='chunker',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5377c0-4c9b-4ba9-8f08-5e866b9220b5",
   "metadata": {},
   "source": [
    "## Select outputs of upstream listener"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809f5f62-95c3-483b-ae74-a5cdb5c1c83d",
   "metadata": {},
   "source": [
    ":::note\n",
    "This is useful if you have performed a first step, such as pre-computing \n",
    "features, or chunking your data. You can use this query to \n",
    "operate on those outputs.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a2cd87-723f-4cee-87c7-9b8181c9e54b",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "## Build text embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9b1f538-65ca-499e-b6d0-2dd733f81723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: OpenAI>\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = 'sk-<secret>'\n",
    "from superduper_openai import OpenAIEmbedding\n",
    "\n",
    "embedding_model = OpenAIEmbedding(identifier='text-embedding-ada-002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83facd8-8823-492f-a2c6-659f38d8e6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: JinaAI>\n",
    "import os\n",
    "from superduper_jina import JinaEmbedding\n",
    "\n",
    "os.environ[\"JINA_API_KEY\"] = \"jina_xxxx\"\n",
    " \n",
    "# define the model\n",
    "embedding_model = JinaEmbedding(identifier='jina-embeddings-v2-base-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4a9a60-41df-461d-b165-1d136ee25694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Sentence-Transformers>\n",
    "!pip install sentence-transformers\n",
    "from superduper import vector\n",
    "import sentence_transformers\n",
    "from superduper_sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(\n",
    "    identifier=\"embedding\",\n",
    "    object=sentence_transformers.SentenceTransformer(\"BAAI/bge-small-en\"),\n",
    "    datatype=vector(shape=(1024,)),\n",
    "    postprocess=lambda x: x.tolist(),\n",
    "    predict_kwargs={\"show_progress_bar\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31843db-8638-458a-a770-96a79041be88",
   "metadata": {},
   "source": [
    "## Create vector-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4663fa4b-c2ec-427d-bf8b-b8b109cc2ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper import VectorIndex, Listener\n",
    "\n",
    "vector_index_name = 'vector-index'\n",
    "\n",
    "vector_index = \\\n",
    "    VectorIndex(\n",
    "        vector_index_name,\n",
    "        indexing_listener=Listener(\n",
    "            key=upstream_listener.outputs,      # the `Document` key `model` should ingest to create embedding\n",
    "            select=db[upstream_listener.outputs].select(),       # a `Select` query telling which data to search over\n",
    "            model=embedding_model,         # a `_Predictor` how to convert data to embeddings\n",
    "            uuid=\"embedding-listener\",\n",
    "            identifier='embedding-listener',\n",
    "            upstream=[upstream_listener],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91142c55-b256-4025-94c2-6c4d215c6975",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "## Create Vector Search Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5b99541-fd10-41c1-b6a7-1da6c1d4dbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = {'_outputs__chunker': '<var:query>'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d47799ab-b688-4eb8-82d4-6c0aa1204801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper.components.model import QueryModel\n",
    "\n",
    "vector_search_model = QueryModel(\n",
    "    identifier=\"VectorSearch\",\n",
    "    select=db[upstream_listener.outputs].like(item, vector_index=vector_index_name, n=5).select(),\n",
    "    # The _source is the identifier of the upstream data, which can be used to locate the data from upstream sources using `_source`.\n",
    "    postprocess=lambda docs: [{\"text\": doc['_outputs__chunker'], \"_source\": doc[\"_source\"]} for doc in docs],\n",
    "    db=db\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1179a67b-4e40-496b-9851-98f32d42faa0",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "## Build LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f98e5ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: OpenAI>\n",
    "from superduper_openai import OpenAIChatCompletion\n",
    "\n",
    "llm = OpenAIChatCompletion(identifier='llm', model='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf39c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Anthropic>\n",
    "from superduper_anthropic import AnthropicCompletions\n",
    "import os\n",
    "\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-xxx\"\n",
    "\n",
    "predict_kwargs = {\n",
    "    \"max_tokens\": 1024,\n",
    "    \"temperature\": 0.8,\n",
    "}\n",
    "\n",
    "llm = AnthropicCompletions(identifier='llm', model='claude-2.1', predict_kwargs=predict_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e48deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: vLLM>\n",
    "from superduper_vllm import VllmModel\n",
    "\n",
    "predict_kwargs = {\n",
    "    \"max_tokens\": 1024,\n",
    "    \"temperature\": 0.8,\n",
    "}\n",
    "\n",
    "\n",
    "llm = VllmModel(\n",
    "    identifier=\"llm\",\n",
    "    model_name=\"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\",\n",
    "    vllm_kwargs={\n",
    "        \"gpu_memory_utilization\": 0.7,\n",
    "        \"max_model_len\": 1024,\n",
    "        \"quantization\": \"awq\",\n",
    "    },\n",
    "    predict_kwargs=predict_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4ac344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Transformers>\n",
    "from superduper_transformers import LLM\n",
    "\n",
    "llm = LLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", load_in_8bit=True, device_map=\"cuda\", identifier=\"llm\", predict_kwargs=dict(max_new_tokens=128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdbfae2-af7d-4845-bce5-0cb230e3614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Llama.cpp>\n",
    "!huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.2-GGUF mistral-7b-instruct-v0.2.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\n",
    "\n",
    "from superduper_llama_cpp.model import LlamaCpp\n",
    "llm = LlamaCpp(identifier=\"llm\", model_name_or_path=\"mistral-7b-instruct-v0.2.Q4_K_M.gguf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ae6203-dcc4-493c-a8f8-f727f0f75778",
   "metadata": {},
   "source": [
    "## Answer question with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44baeb09-6f35-4cf2-b814-46283a59f7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduper import model\n",
    "from superduper.components.graph import Graph, input_node\n",
    "\n",
    "prompt_template = (\n",
    "    \"Use the following context snippets, these snippets are not ordered!, Answer the question based on this context.\\n\"\n",
    "    \"{context}\\n\\n\"\n",
    "    \"Here's the question: {query}\"\n",
    ")\n",
    "\n",
    "@model\n",
    "def build_prompt(query, docs):\n",
    "    chunks = [doc[\"text\"] for doc in docs]\n",
    "    context = \"\\n\\n\".join(chunks)\n",
    "    prompt = prompt_template.format(context=context, query=query)\n",
    "    return prompt\n",
    "\n",
    "# We build a graph to handle the entire pipeline\n",
    "\n",
    "# create a input node, only have one input parameter `query`\n",
    "in_ = input_node('query')\n",
    "# pass the query to the vector search model\n",
    "vector_search_results = vector_search_model(query=in_)\n",
    "# pass the query and the search results to the prompt builder\n",
    "prompt = build_prompt(query=in_, docs=vector_search_results)\n",
    "# pass the prompt to the llm model\n",
    "answer = llm(prompt)\n",
    "# create a graph, and the graph output is the answer\n",
    "rag = answer.to_graph(\"rag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183bf5b6-4644-4e4c-b65b-e6bafdc6b49f",
   "metadata": {},
   "source": [
    "By applying the RAG model to the database, it will subsequently be accessible for use in other services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6787c78-4b14-4a72-818b-450408a74331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-Aug-25 21:00:42.45\u001b[0m| \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.document\u001b[0m:\u001b[36m415 \u001b[0m | \u001b[33m\u001b[1mLeaf _input already exists\u001b[0m\n",
      "\u001b[32m2024-Aug-25 21:00:42.45\u001b[0m| \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.document\u001b[0m:\u001b[36m415 \u001b[0m | \u001b[33m\u001b[1mLeaf llm already exists\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([],\n",
       " Application(identifier='rag-app', uuid='e1e98a1c9ef244d1b3386c78bc74d391', upstream=None, plugins=None, cache=False, components=[Listener(identifier='chunker', uuid='chunker', upstream=None, plugins=None, cache=False, key='x', model=ObjectModel(identifier='chunker', uuid='3bc1b7c65f8f493d97344256718517ae', upstream=None, plugins=None, cache=False, signature='*args,**kwargs', datatype=None, output_schema=None, flatten=True, model_update_kwargs={}, predict_kwargs={}, compute_kwargs={}, validation=None, metric_values={}, num_workers=0, object=<function chunker at 0x150180040>), select=docs.select(), predict_kwargs={}, predict_id='chunker'), VectorIndex(identifier='vector-index', uuid='6e34deef08d54c66acf35b58f8a5fa0c', upstream=None, plugins=None, cache=False, indexing_listener=Listener(identifier='embedding-listener', uuid='embedding-listener', upstream=[Listener(identifier='chunker', uuid='chunker', upstream=None, plugins=None, cache=False, key='x', model=ObjectModel(identifier='chunker', uuid='3bc1b7c65f8f493d97344256718517ae', upstream=None, plugins=None, cache=False, signature='*args,**kwargs', datatype=None, output_schema=None, flatten=True, model_update_kwargs={}, predict_kwargs={}, compute_kwargs={}, validation=None, metric_values={}, num_workers=0, object=<function chunker at 0x150180040>), select=docs.select(), predict_kwargs={}, predict_id='chunker')], plugins=None, cache=False, key='_outputs__chunker', model=OpenAIEmbedding(identifier='text-embedding-ada-002', uuid='a974ce236d824ed983e3f87c1bedac89', upstream=None, plugins=None, cache=False, signature='singleton', datatype=DataType(identifier='vector[1536]', uuid='42a283a0c2534e7095d3d6eff57f537b', upstream=None, plugins=None, cache=False, encoder=None, decoder=None, info=None, shape=(1536,), directory=None, encodable='native', bytes_encoding=<BytesEncoding.BYTES: 'Bytes'>, intermediate_type='bytes', media_type=None), output_schema=None, flatten=False, model_update_kwargs={}, predict_kwargs={}, compute_kwargs={}, validation=None, metric_values={}, num_workers=0, model='text-embedding-ada-002', max_batch_size=8, openai_api_key=None, openai_api_base=None, client_kwargs={}, shape=(1536,), batch_size=100), select=_outputs__chunker.select(), predict_kwargs={}, predict_id='embedding-listener'), compatible_listener=None, measure=<VectorIndexMeasureType.cosine: 'cosine'>, metric_values={}), QueryModel(identifier='VectorSearch', uuid='c9ddf0f6d2cd4cb09fa0eef300da2e94', upstream=None, plugins=None, cache=False, signature='**kwargs', datatype=None, output_schema=None, flatten=False, model_update_kwargs={}, predict_kwargs={}, compute_kwargs={}, validation=None, metric_values={}, num_workers=0, preprocess=None, postprocess=<function <lambda> at 0x1518fa5c0>, select=_outputs__chunker.like({'_outputs__chunker': '<var:query>'}, vector_index=\"vector-index\", n=5).select()), Graph(identifier='rag', uuid='55c1fa8c04b044c59953bc38ef19a93b', upstream=None, plugins=None, cache=False, signature='singleton', datatype=None, output_schema=None, flatten=False, model_update_kwargs={}, predict_kwargs={}, compute_kwargs={}, validation=None, metric_values={}, num_workers=0, models=[Input(identifier='_input', uuid='cf0c7412b535483ea6d95ca00fa640c6', upstream=None, plugins=None, cache=False, signature='singleton', datatype=None, output_schema=None, flatten=False, model_update_kwargs={}, predict_kwargs={}, compute_kwargs={}, validation=None, metric_values={}, num_workers=0, spec='query'), QueryModel(identifier='VectorSearch', uuid='c9ddf0f6d2cd4cb09fa0eef300da2e94', upstream=None, plugins=None, cache=False, signature='**kwargs', datatype=None, output_schema=None, flatten=False, model_update_kwargs={}, predict_kwargs={}, compute_kwargs={}, validation=None, metric_values={}, num_workers=0, preprocess=None, postprocess=<function <lambda> at 0x1532faa20>, select=_outputs__chunker.like({'_outputs__chunker': '<var:query>'}, vector_index=\"vector-index\", n=5).select()), ObjectModel(identifier='build_prompt', uuid='fe86fc07909e4274a63e2e2ac9e51d78', upstream=None, plugins=None, cache=False, signature='*args,**kwargs', datatype=None, output_schema=None, flatten=False, model_update_kwargs={}, predict_kwargs={}, compute_kwargs={}, validation=None, metric_values={}, num_workers=0, object=<function build_prompt at 0x16f35ac00>), OpenAIChatCompletion(identifier='llm', uuid='ae8205d70f0241ea8dd6cc7747ab026a', upstream=None, plugins=None, cache=False, signature='singleton', datatype='str', output_schema=None, flatten=False, model_update_kwargs={}, predict_kwargs={}, compute_kwargs={}, validation=None, metric_values={}, num_workers=0, model='gpt-3.5-turbo', max_batch_size=8, openai_api_key=None, openai_api_base=None, client_kwargs={}, batch_size=1, prompt='')], edges=[['_input', 'VectorSearch', [None, 'query']], ['_input', 'build_prompt', [None, 'query']], ['VectorSearch', 'build_prompt', [None, 'docs']], ['build_prompt', 'llm', [None, 'X']]], input=Input(identifier='_input', uuid='cf0c7412b535483ea6d95ca00fa640c6', upstream=None, plugins=None, cache=False, signature='singleton', datatype=None, output_schema=None, flatten=False, model_update_kwargs={}, predict_kwargs={}, compute_kwargs={}, validation=None, metric_values={}, num_workers=0, spec='query'), outputs=[OpenAIChatCompletion(identifier='llm', uuid='ae8205d70f0241ea8dd6cc7747ab026a', upstream=None, plugins=None, cache=False, signature='singleton', datatype='str', output_schema=None, flatten=False, model_update_kwargs={}, predict_kwargs={}, compute_kwargs={}, validation=None, metric_values={}, num_workers=0, model='gpt-3.5-turbo', max_batch_size=8, openai_api_key=None, openai_api_base=None, client_kwargs={}, batch_size=1, prompt='')])], namespace=[{'type_id': 'listener', 'identifier': 'chunker'}, {'type_id': 'vector_index', 'identifier': 'vector-index'}, {'type_id': 'model', 'identifier': 'VectorSearch'}, {'type_id': 'model', 'identifier': 'rag'}, {'type_id': 'model', 'identifier': 'chunker'}, {'type_id': 'listener', 'identifier': 'embedding-listener'}, {'type_id': 'model', 'identifier': 'text-embedding-ada-002'}, {'type_id': 'datatype', 'identifier': 'vector[1536]'}, {'type_id': 'model', 'identifier': '_input'}, {'type_id': 'model', 'identifier': 'VectorSearch'}, {'type_id': 'model', 'identifier': 'build_prompt'}, {'type_id': 'model', 'identifier': 'llm'}]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from superduper import Application\n",
    "\n",
    "app = Application(\n",
    "    'rag-app',\n",
    "    components=[\n",
    "        upstream_listener,\n",
    "        vector_index,\n",
    "        vector_search_model,\n",
    "        rag,\n",
    "    ]\n",
    ")\n",
    "\n",
    "db.apply(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da0306b-0969-49ab-95c4-0eb93c39f515",
   "metadata": {},
   "source": [
    "You can now load the model elsewhere and make predictions using the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a8dc26d-b91e-442d-9904-15cd4accc463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-Aug-25 20:59:50.29\u001b[0m| \u001b[1mINFO    \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.datalayer\u001b[0m:\u001b[36m889 \u001b[0m | \u001b[1m{}\u001b[0m\n",
      "SuperDuperDB is a virtual AI-datalayer open-sourced in Python under the Apache 2.0 license. It allows developers to connect an AI development environment directly to data, connect an AI production environment directly to data, and create a flexible platform connecting AI and data for collaboration. SuperDuperDB can handle classical AI/machine learning paradigms like classification, regression, forecasting, clustering, and more. Users can choose how to deploy SuperDuperDB, use their own models or integrate AI APIs and frameworks, work with various data types, version and track functionality, and control data exposure to API services. Key features include AI integration with existing data infrastructure, streaming inference, scalable model training, model chaining, and a simple but extendable interface. Users can get started with SuperDuperDB by installing and configuring it, trying code snippets and use cases, reading about usage patterns and key functionality, exploring production features, building understanding through fundamentals and API references, creating custom components, and engaging with the community for support and feedback.\n"
     ]
    }
   ],
   "source": [
    "rag = db.load(\"model\", 'rag')\n",
    "print(rag.predict(\"Tell me about superduper\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42119a4-6aef-46ec-a81d-cbe1167d8710",
   "metadata": {},
   "source": [
    "## Create template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e850c03-33c6-4c88-95d3-d14146a6a0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-Aug-25 21:00:59.65\u001b[0m| \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.document\u001b[0m:\u001b[36m415 \u001b[0m | \u001b[33m\u001b[1mLeaf chunker already exists\u001b[0m\n",
      "\u001b[32m2024-Aug-25 21:00:59.65\u001b[0m| \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.document\u001b[0m:\u001b[36m415 \u001b[0m | \u001b[33m\u001b[1mLeaf chunker already exists\u001b[0m\n",
      "\u001b[32m2024-Aug-25 21:00:59.66\u001b[0m| \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.document\u001b[0m:\u001b[36m415 \u001b[0m | \u001b[33m\u001b[1mLeaf dill_lazy already exists\u001b[0m\n",
      "\u001b[32m2024-Aug-25 21:00:59.66\u001b[0m| \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.document\u001b[0m:\u001b[36m415 \u001b[0m | \u001b[33m\u001b[1mLeaf docs-select already exists\u001b[0m\n",
      "\u001b[32m2024-Aug-25 21:00:59.66\u001b[0m| \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.document\u001b[0m:\u001b[36m415 \u001b[0m | \u001b[33m\u001b[1mLeaf VectorSearch already exists\u001b[0m\n",
      "\u001b[32m2024-Aug-25 21:00:59.66\u001b[0m| \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.document\u001b[0m:\u001b[36m415 \u001b[0m | \u001b[33m\u001b[1mLeaf dill already exists\u001b[0m\n",
      "\u001b[32m2024-Aug-25 21:00:59.66\u001b[0m| \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.document\u001b[0m:\u001b[36m415 \u001b[0m | \u001b[33m\u001b[1mLeaf -outputs-chunker-like-outputs-chunker-<var:query>-vector-index-vector-index-n-5-select already exists\u001b[0m\n",
      "\u001b[32m2024-Aug-25 21:00:59.66\u001b[0m| \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.document\u001b[0m:\u001b[36m415 \u001b[0m | \u001b[33m\u001b[1mLeaf dill_lazy already exists\u001b[0m\n",
      "\u001b[32m2024-Aug-25 21:00:59.66\u001b[0m| \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.document\u001b[0m:\u001b[36m415 \u001b[0m | \u001b[33m\u001b[1mLeaf _input already exists\u001b[0m\n",
      "\u001b[32m2024-Aug-25 21:00:59.66\u001b[0m| \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mDuncans-MBP.fritz.box\u001b[0m| \u001b[36msuperduper.base.document\u001b[0m:\u001b[36m415 \u001b[0m | \u001b[33m\u001b[1mLeaf llm already exists\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from superduper import Template\n",
    "\n",
    "template = Template('rag-template', template=app, substitutions={'docs': 'collection'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad7b5bd4-726a-4d50-acc6-46b701d4d879",
   "metadata": {},
   "outputs": [],
   "source": [
    "template.export('.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
