[
  "---\nsidebar_position: 5\n---\n\n# Encoding data\n\nIn AI, typical types of data are:\n\n- **Numbers** (integers, floats, etc.)\n- **Text**\n- **Images**\n- **Audio**\n- **Videos**\n- **...bespoke in house data**\n\nMost databases don't support any data other than numbers and text.\nSuperduper enables the use of these more interesting data-types using the `Document` wrapper.\n\n### `Document`\n\nThe `Document` wrapper, wraps dictionaries, and is the container which is used whenever \ndata is exchanged with your database. That means inputs, and queries, wrap dictionaries \nused with `Document` and also results are returned wrapped with `Document`.\n\nWhenever the `Document` contains data which is in need of specialized serialization,\nthen the `Document` instance contains calls to `DataType` instances.\n\n### `DataType`\n\nThe [`DataType` class](../apply_api/datatype), allows users to create and encoder custom datatypes, by providing \ntheir own encoder/decoder pairs.\n\nHere is an example of applying an `DataType` to add an image to a `Document`:\n\n```python\nimport pickle\nimport PIL.Image\nfrom superduper import DataType, Document\n\nimage = PIL.Image.open('my_image.jpg')\n\nmy_image_encoder = DataType(\n    identifier='my-pil',\n    encoder=lambda x, info: pickle.dumps(x),\n    decoder=lambda x, info: pickle.loads(x),\n)\n```\n\nWhen all data is inserted into the database, each piece of data is encoded using the corresponding datatype. \n```\n>> encoded_data = my_image_encoder.encode_data(image)\n>> encoded_data\nb'\\x80\\x04\\x95[\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x12PIL.PngImagePlugin\\x94\\x8c\\x0cPngImageFile\\x94\\x93\\x94)\\x81\\x94]\\x94(}\\x94\\x8c\\x0ctransparency\\x94K\\x00s\\x8c\\x01P\\x94K\\x01K\\x01\\x86\\x94]\\x94(K\\x00K\\x00K\\x00eC\\x01\\x00\\x94eb.'\n```\n\nWhen the data is retrieved from the database, it is decoded accordingly.\n```python\n>>> my_image_encoder.decode_data(encoded_data)\n<PIL.PngImagePlugin.PngImageFile image mode=P size=1x1>\n```\n\nBy default, data encoded with `DataType` is saved in the database, but developers \nmay alternatively save data in the `db.artifact_store` instead. \n\nThis may be achiever by specifying the `encodable=...` parameter:\n\n```python\nmy_image_encoder = DataType(\n    identifier='my-pil',\n    encoder=lambda x, info: pickle.dumps(x),\n    decoder=lambda x, info: pickle.loads(x),\n    encodable='artifact',    # saves to disk/ db.artifact_store\n    # encodable='lazy_artifact', # Just in time loading\n)\n```\n\n### `Schema`\n\nA `Schema` allows developers to connect named fields of dictionaries \nor columns of `pandas.DataFrame` objects with `DataType` instances.\n\nA `Schema` is used, in particular, for SQL databases/ tables, and for \nmodels that return multiple outputs.\n\nHere is an example `Schema`, which is used together with text and image \nfields:\n\n```python\nschema = Schema('my-schema', fields={'my-text': 'str', 'my-img': my_image_encoder})\n```\n\nAll data is encoded using the schema when saved, and decoded using the schema when queried.\n\n```python\n>>> saved_data = Document({'my-img': image}).encode(schema)\n>>> saved_data\n{'my-img': b'\\x80\\x04\\x95[\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x12PIL.PngImagePlugin\\x94\\x8c\\x0cPngImageFile\\x94\\x93\\x94)\\x81\\x94]\\x94(}\\x94\\x8c\\x0ctransparency\\x94K\\x00s\\x8c\\x01P\\x94K\\x01K\\x01\\x86\\x94]\\x94(K\\x00K\\x00K\\x00eC\\x01\\x00\\x94eb.',\n '_schema': 'my-schema',\n '_builds': {},\n '_files': {},\n '_blobs': {}}\n```\n\n```python\n>>> Document.decode(saved_data, schema=schema).unpack()\n{'my-img': <PIL.PngImagePlugin.PngImageFile image mode=P size=1x1>}\n```\n\n\n",
  "# `Document` wrapper\n\n...",
  "# Fundamentals\n\nIn this section, we try and guide developers through the principles and algorithms underlying Superduper.\n\nFor a more \"how-to\" approach refer first to the [tutorials](../tutorials/intro.md), [use-cases](../../use_cases), [reusable snippets](../reusable_snippets/), and to [core api](../core_api/) usage.",
  "---\nsidebar_position: 3\n---\n\n# Datalayer\n\nThe `Datalayer` is the principle point of entry in Superduper for:\n\n- Communicating with the database\n- Instructing models and other components to work together with the database\n- Accessing and storing meta-data about your Superduper models and data\n\nTechnically, the `Datalayer` \"wires together\" several important backends involved in the AI workflow:\n\n- Querying the database via the **databackend**\n- Storing and retrieving serialized model-weights and other artifacts from the **artifact store**\n- Storing and retrieval important meta-data, from the **meta-data store** and information about models and other components which are to be installed with Superduper\n- Performing computations over the data in the **databackend** using the models saved in the **artifact store**\n\n```python\nfrom superduper import superduper\n\ndb = superduper()\n\ndb.databackend\n# <superduper.backends.mongodb.data_backend.MongoDataBackend at 0x1562815d0>\n\ndb.artifact_store\n# <superduper.backends.mongodb.artifacts.MongoArtifactStore at 0x156869f50>\n\ndb.metadata\n# <superduper.backends.mongodb.metadata.MongoMetaDataStore at 0x156866a10>\n\ndb.compute\n# <superduper.backends.local.LocalComputeBackend 0x152866a10>\n```\n\nOur aim is to make it easy to set-up each aspect of the `Datalayer` with your preferred\nconnections/ engines.\n\n### Data-backend\n\nThe databackend typically connects to your database (although `superduper` also supports other databackends such as a directory of `pandas` dataframes), \nand dispatches queries written in an query API which is compatible with that databackend, but which also includes additional aspects\nspecific to `superduper`.\n\nRead more [here](../data_integrations/supported_query_APIs.md).\n\nThe databackend is configured by setting the URI `CFG.databackend` in the [configuration system](../get_started/configuration.md).\n\nWe support the same databackends as supported by the [`ibis` project](https://ibis-project.org/):\n\n- [**MongoDB**](https://www.mongodb.com/)\n- [**PostgreSQL**](https://www.postgresql.org/)\n- [**SQLite**](https://www.sqlite.org/index.html)\n- [**DuckDB**](https://duckdb.org/)\n- [**BigQuery**](https://cloud.google.com/bigquery)\n- [**ClickHouse**](https://clickhouse.com/)\n- [**DataFusion**](https://arrow.apache.org/datafusion/)\n- [**Druid**](https://druid.apache.org/)\n- [**Impala**](https://impala.apache.org/)\n- [**MSSQL**](https://www.microsoft.com/en-us/sql-server/)\n- [**MySQL**](https://www.mysql.com/)\n- [**Oracle**](https://www.oracle.com/database/)\n- [**pandas**](https://pandas.pydata.org/)\n- [**Polars**](https://www.pola.rs/)\n- [**PySpark**](https://spark.apache.org/docs/3.3.1/api/python/index.html)\n- [**Snowflake**](https://www.snowflake.com/en/)\n- [**Trino**](https://trino.io/)\n\n### Artifact Store\n\nThe artifact-store is the place where large pieces of data associated with your AI models are saved.\nUsers have the possibility to configure either a local filesystem, or an artifact store on MongoDB `gridfs`:\n\nFor example:\n\n```python\nCFG.artifact_store = 'mongodb://localhost:27017/documents'\n```\n\nOr:\n\n```python\nCFG.artifact_store = 'filesystem://./data'\n```\n\n### Metadata Store\n\nThe meta-data store is the place where important information associated with models and \nrelated components are kept:\n\n- Where are the data artifacts saved for a component?\n- Important parameters necessary for using a component\n- Important parameters which were used to create a component (e.g. in training or otherwise)\n\nSimilarly to the databackend and artifact store, the metadata store is configurable:\n\n```python\nCFG.metadata = 'mongodb://localhost:27017/documents'\n```\n\nWe support metadata store via:\n\n1. [MongoDB](https://www.mongodb.com/)\n1. All databases supported by [SQLAlchemy](https://www.sqlalchemy.org/).\n   For example, these databases supported by the databackend are also supported by the metadata store.\n   - [PostgreSQL](https://www.postgresql.org/)\n   - [MySQL](https://www.mysql.com/)\n   - [SQLite](https://www.sqlite.org/)\n   - [MSSQL](https://www.microsoft.com/en-us/sql-server/sql-server-downloads)\n\n\n### Compute backend\n\nThe compute-backend is designed to be a configurable engine for performing computations with models.\nWe support 2 backends:\n\n- Local (default: run compute in process on the local machine)\n- `dask` (run compute on a configured `dask` cluster)\n\n## Default settings\n\nIn such cases, the default configuration is to use the same configuration as used in the \ndatabackend.\n\nI.e., for MongoDB the following are equivalent:\n\n```python\ndb = superduper('mongodb://localhost:27018/documents')\n```\n\n...and\n\n```python\ndb = superduper(\n    'mongodb://localhost:27018/documents',\n    metadata_store='mongodb://localhost:27018/documents',\n    artifact_store='mongodb://localhost:27018/documents',\n)\n```\n\nWhenever a database is supported by the artifact store and metadata store, \nthe same behaviour holds. However, since there is no general pattern\nfor storing large files in SQL databases, the fallback artifact store\nis on the local filesystem. So the following are equivalent:\n\n```python\ndb = superduper('sqlite://<my-database>.db')\n```\n\n...and\n\n```python\nfrom superduper.backends.local.compute import LocalComputeBackend\n\ndb = superduper(\n    'sqlite://<my-database>.db',\n    metadata_store='sqlite://<my-database>.db',\n    artifact_store='filesystem://.superduper/artifacts/',\n    compute=LocalComputeBackend(),\n)\n```\n\n## Key methods\n\nHere are the key methods which you'll use again and again:\n\n### `db.execute`\n\nThis method executes a query. For an overview of how this works see [here](../data_integrations/supported_query_APIs.md).\n\n### `db.add`\n\nThis method adds `Component` instances to the `db.artifact_store` connection, and registers meta-data\nabout those instances in the `db.metadata_store`.\n\nIn addition, each sub-class of `Component` has certain \"set-up\" tasks, such as inference, additional configurations, \nor training, and these are scheduled by `db.add`.\n\n<!-- See [here]() for more information about the `Component` class and it's descendants. -->\n\n### `db.show`\n\nThis methods displays which `Component` instances are registered with the system.\n\n### `db.remove`\n\nThis method removes a `Component` instance from the system.\n\n## Additional methods\n\n### `db.validate`\n\nValidate your components (mostly models)\n\n### `db.predict`\n\nInfer predictions from models hosted by Superduper. Read more about this and about models [here](../apply_api/model.md).\n",
  "---\nsidebar_position: 5\n---\n\n# Predictors and Models\n\n## Predictors\n\nThe base class which enables predictions in `superduper` is the `Predictor` mixin class.\n\nA `Predictor` is a class which implements the `.predict` method; this mimics `.predict` from \n[Scikit-Learn](https://scikit-learn.org/stable/) and related frameworks, but has support\nfor prediction directly via the `Datalayer`.\n\nA typical call to `.predict` looks like this:\n\n```python\npredictor.predict(\n    X='<key-column>'     # key of documents or column of table to take as input\n    db=db                # `Datalayer` instance, built via `db = superduper()`\n    select=my_select     # database query over which to compute outputs\n    **predict_kwargs     # additional parameters for `.predict`\n)\n```\n\nExamples of `Predictor` classes are the AI-API classes in\n\n- `superduper.ext.openai.OpenAI*`\n- `superduper.ext.anthropic.Anthropic*`\n- `superduper.ext.cohere.Cohere*`\n\n## Models\n\nA model is a particular type of `Predictor` which carries large chunks of data around\nin order to implement predictions. These blobs can be, for example, the weights \nof a deep learning architecture or similar important data.\n\nExamples of `Model` are:\n\n- `superduper.ext.torch.TorchModel`\n- `superduper.ext.sklearn.Estimator`\n- `superdueprdb.ext.transformers.Pipeline`\n\nEach of these inheriting classes also implements the `.fit` method, which re-parametrizes the class in question, \ntypicall via a machine learning task and objective function.\n\nA typical call to `.fit` looks like this:\n\n```python\nmodel.fit(\n    X='<input-key-column>',    # key of documents or column of table to take as input\n    y='<target-key>',          # key of documents or column of table to take as target of fitting\n    db=db,                     # `Datalayer` instance, built via `db = superduper()`\n    select=my_select,          # database query for training and validation data\n    **fit_kwargs,              # additional parameters for .fit\n)\n```\n",
  "---\nsidebar_position: 7\n---\n\n# Vector-search\n\nSuperduper allows users to implement vector-search in their database by either \nusing in-database functionality, or via a sidecar implementation with `lance` and `FastAPI`.\n\n## Philosophy\n\nIn Superduper, from a user point-of-view vector-search isn't a completely different beast than other ways of \nusing the system:\n\n- The vector-preparation is exactly the same as preparing outputs with any model, \n  with the special difference that the outputs are vectors, arrays or tensors.\n- Vector-searches are just another type of database query which happen to use \n  the stored vectors.\n\n## Algorithm\n\nHere is a schematic of how vector-search works:\n\n![](/img/vector-search.png)\n\n## Explanation\n\nA vector-search query has the schematic form:\n\n```python\ntable_or_collection\n    .like(Document(<dict-to-search-with>))      # the operand is vectorized using registered models\n    .filter_results(*args, **kwargs)            # the results of vector-search are filtered\n```\n\n```python\ntable_or_collection\n    .filter_results(*args, **kwargs)            # the results of vector-search are filtered\n    .like(Document(<dict-to-search-with>))      # the operand is vectorized using registered models\n```\n\n...or\n\nThe type of such a query is a `CompoundSelect`. It's 2 parts are the vector-search part (`like`) and the \nfiltering part (`select`).\n\nIn the first case, the operand of `like` is dispatched to a **model**, which converts this into a **vector**.\nThe **vector** is compared to previously saved outputs of the same or a paired **model** (multi-modal).\nThe most similar `ids` are retrieved. The `select` part of the query is then transformed to \na similar query which searches within the retrieved `ids`. The full set of results are returned\nto the client.\n\nRead [here](../tutorials/vector_search.md) about setting up and detailed usage of vector-search.\n",
  "# Architecture\n\nHere is a schematic of the Superduper design.\n\n![](/img/light.png)\n\n### Explanation\n\n1. Superduper expects data and components to be added/ updated from a range of client-side mechanisms: **scripts**, **apps**, **notebooks** or **third-party database clients** (possibly non-python).\n\n1. Users and programs can add **components** (**models**, data **encoders**, **vector-indexes** and more) from the client-side. These large items are stored in the **artifact-store** and are tracked via the **meta-data** store.\n\n1. If data is inserted to the **databackend** the **change-data-capture (CDC)** component captures these changes as they stream in.\n\n1. **(CDC)** triggers **work** to be performed in response to these changes, depending on which **components** are present in the system.\n\n1. The **work** is submitted to the **workers** via the **scheduler**. Together the **scheduler** and **workers** make up the **compute** layer.\n\n1. **workers** write their outputs back to the **databackend** and trained models to the **artifact-store**\n\n1. The **compute**, **databackend**, **metadata-store**, **artifact-store** collectively make up the **datalayer**\n\n1. The **datalayer** may be queried from client-side, including hybrid-queries or **compound-select** queries, which synthesizes classical **selects** with **vector-searches**",
  "# Class hierarchy of user-facing classes\n\n![](/img/class-hierarchy.png)\n\n## `superduper`\n\n`superduper` is the entry point to connect and \nbe able to use key functionality. It returns a built `Datalayer`.\n\n## `Datalayer`\n\nThe `Datalayer` class, an instance of which we refer to throughout this \ndocumentation as `db`, is the key entrypoint via which developers\nmay connect to their data-infrastructure and additional connect\nAI functionality to their data-infrastructure:\n\nThe `Datalayer` connects to data, with the [`superduper` function](../core_api/connect).\n\n***`.apply`***\n\nAI `Component` instances may be applied to the built `Datalayer` [with `.apply`](../core_api/apply).\n\n***`.execute`***\n\nThe data and AI outputs are accessible with queries and AI models \nusing the `.execute` method. This can include standard database queries,\nvector-search queries (which include model inference) and pure model computations.\nSee [here](../core_api/execute).\n\n## `Component`\n\nAI functionality is packaged as a `Component`. Key implementations \nare `Model`, `Listener` and `VectorIndex`.\n\n## `Document`\n\n`Document` is a wrapper around standard Python `dict` instances, \nbut which can encode their contained fields as a mixture of JSON\nand pure `bytes`. This mechanism can in principle handle any information \nwhich Python can handle.\n\nSince most databases can handle this type of information, this makes\n`Document` a crucial piece in connecting AI (which operates over a range of information)\nand the database.\n\n## `_BaseEncodable`\n\nThis is the base class, which allows `superduper` to decide how to save \"special\" data.\n\n## `Serializable`\n\nAn extension of Python `dataclasses`, but easier to get the original class back \nfrom the serialized dictionary form. This is the base class underlying \nall `superduper` queries and predictions as well as mixing into `Component`.\n\n## `Job`\n\n`Component` instances applied with `Datalayer.apply` create compute-jobs \nin response to incoming data, and on initialization via the `Job` class.\n\nThe interface on `Component` is `Component.schedule_jobs`.",
  "# Snowflake\n\n## Installation\n\n```bash\npip install superduper_ibis\npip install snowflake-sqlalchemy\n```\n\n## API\n\n```python\nfrom superduper import superduper\n\ndb = superduper('snowflake://<snowflake-uri>')\n```",
  "---\nsidebar_position: 1\n---\n\n# Community support\n\nIn order to specify the action of models on the data, we provide an interface to pythonic ecosystem query APIs.\nIn particular, we provide wrappers to these projects to create database queries:\n\n- [`pymongo`](https://pymongo.readthedocs.io/en/stable/) for MongoDB\n- [`ibis`](https://ibis-project.org/) for SQL databases\n\n`ibis` also allows users to use raw SQL in their workflows.\n\nQueries in these two-worlds can be built by importing the table/collection class from \neach data backend. With `pymongo`, one can write:\n\n```python\nquery = db['products'].find({'brand': 'Nike'}, {'_id': 1}).limit(10)\n```\n\nIn `ibis`, one would write:\n\n```python\nquery = db['products'].filter(products.brand == 'Nike').select('id').limit(10)\n```\n\n## Hybrid API\n\nOn top of the native features of `pymongo` and `ibis`, `superduper` builds several novel features:\n\n- Additional ways to query the database with the outputs of machine learning models\n  - Query model-outputs directly\n  - Vector-search\n- Ways to encode and query more sophisticated data-types using the `Document`-`Encoder` pattern.",
  "# MySQL\n\n## Installation\n\n```bash\npip install superduper_ibis\n```\n\n## API\n\n```python\nfrom superduper import superduper\n\ndb = superduper('mysql://<mysql-uri>')\n```",
  "# Data integrations\n\nSuperduper integrates with 3 types of data-backend:\n\n- NoSQL\n    - [MongoDB Community Edition](https://www.mongodb.com/try/download/community)\n    - [MongoDB Atlas](https://www.mongodb.com/products/platform/atlas-database)\n- SQL\n    - [MySQL](https://www.mysql.com/)\n    - [PostgreSQL](https://www.postgresql.org/)\n    - [Oracle](https://www.oracle.com/database/)\n    - [SQLite](https://www.sqlite.org/)\n    - [Snowflake](https://www.snowflake.com/en/)\n    - [DuckDB](https://duckdb.org/)\n    - [Clickhouse](https://clickhouse.com/)\n- In-memory tabular formats\n    - [Pandas](https://pandas.pydata.org/docs/)\n\nAlthough these data-backends provide very different functionality, \nwith Superduper they are accessible via a uniform API.\n\nHowever, developers should bear in mind that there are a few \ndifferences between MongoDB and SQL data-backends.\n\n\n",
  "---\nsidebar_position: 3\n---\n\n# SQL\n\n`superduper` supports SQL databases via the [`ibis` project](https://ibis-project.org/).\nWith `superduper`, queries may be built which conform to the `ibis` API, with additional \nsupport for complex data-types and vector-searches.\n\n## Installation\n\n```bash\npip install superduper_ibis\n```\n\n## Inserting data\n\nTable data must correspond to the `Schema` for that table.\nEither [create a `Schema` and `Table`](../execute_api/data_encodings_and_schemas.md#create-a-table-with-a-schema)\nor use [an auto-detected `Schema`](../execute_api/auto_data_types.md). Once you've \ngot a `Schema`, all data inserted must conform to that `Schema`:\n\n```python\nimport pandas\n\npandas.DataFrame([\n    PIL.Image.open('image.jpg'), 'some text', 4,\n    PIL.Image.open('other_image.jpg'), 'some other text', 3,\n])\n\nt.insert(dataframe.to_dict(orient='records'))\n```\n\n## Selecting data\n\n`superduper` supports selecting data via the `ibis` query API.\nFor example:\n\n```python\ndb['my_table'].filter(t.rating > 3).limit(5).select(t.image).execute()\n```\n\n### Vector-search\n\nVector-searches are supported via the `like` operator:\n\n```python\n(\n    db['my_table']\n    .like({'text': 'something like this'}, vector_index='my-index')\n    .filter(t.rating > 3)\n    .limit(5)\n    .select(t.image, t.id)\n).execute()\n```\n\nVector-searches are either first or last in a chain of operations:\n\n```python\n(\n    db['my_table']\n    t.filter(t.rating > 3)\n    .limit(5)\n    .select(t.image, t.id)\n    .like({'text': 'something like this'}, vector_index='my-index')\n).execute()\n```\n\n## Updating data\n\nUpdates are not covered for `superduper` SQL integrations.\n\n## Deleting data\n\n```python\ndb.databackend.drop_table('my-table')\n```\n",
  "# DuckDB\n\n## Installation\n\n```bash\npip install superduper_ibis\n```\n\n## API\n\n```python\nfrom superduper import superduper\n\ndb = superduper('duckdb://<path-to-db>.dbb')\n```",
  "# Pandas\n\n## Installation\n\n```bash\npip install superduper_ibis\n```\n\n## API\n\nAlthough `pandas` is not a database, it came come in very handy for testing.\nTo connect, one specifies a list of `.csv` files:\n\n```python\nimport glob\nfrom superduper import superduper\n\ndb = superduper(glob.glob('*.csv'))\n```",
  "# PostgreSQL\n\n## Installation\n\n```bash\npip install ibis-framework[postgres]\npip install superduper_ibis\n```\n\n## API\n\n```python\nfrom superduper import superduper\n\ndb = superduper('postgres://<post-gres-uri>')\n```",
  "# SQLite\n\n## Installation\n\n```bash\npip install superduper_ibis\n```\n\n## API\n\n```python\nfrom superduper import superduper\n\ndb = superduper('sqlite://<path-to-db>.db')\n```",
  "---\nsidebar_position: 2\n---\n\n# MongoDB \n\n## Installation\n\n```bash\npip install superduper_mongodb\n```\n\n## API\n\nIn general the MongoDB query API works exactly as per `pymongo`, with the exception that:\n\n- inputs are wrapped in `Document`\n- additional support for vector-search is provided\n- queries are executed lazily\n\n### Inserts\n\n```python\ndb['my-collection'].insert_many([{'my-field': ..., ...}\n    for _ in range(20)\n]).execute()\n```\n\n### Updates\n\n```python\ndb['my-collection'].update_many(\n    {'<my>': '<filter>'},\n    {'$set': ...},\n).execute()\n```\n\n### Selects\n\n```python\ndb['my-collection'].find({}, {'_id': 1}).limit(10).execute()\n```\n\n### Vector-search\n\nVector-searches may be integrated with `.find`.\n\n```python\ndb['my-collection'].like({'img': <my_image>}, vector_index='my-index-name').find({}, {'img': 1}).execute()\n```\n\nRead more about vector-search [here](../fundamentals/vector_search_algorithm.md).\n\n### Deletes\n\n```python\ndb['my-collection'].delete_many({}).execute()\n```",
  "---\nsidebar_label: Build text embedding model\nfilename: build_text_embedding_model.md\n---\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n\n<!-- TABS -->\n# Build text embedding model\n\n\n<Tabs>\n    <TabItem value=\"OpenAI\" label=\"OpenAI\" default>\n        ```python\n        !pip install openai\n        from superduper_openai import OpenAIEmbedding\n        \n        embedding_model = OpenAIEmbedding(identifier='text-embedding-ada-002')        \n        ```\n    </TabItem>\n    <TabItem value=\"JinaAI\" label=\"JinaAI\" default>\n        ```python\n        import os\n        from superduper_jina import JinaEmbedding\n        \n        os.environ[\"JINA_API_KEY\"] = \"jina_xxxx\"\n         \n        # define the model\n        embedding_model = JinaEmbedding(identifier='jina-embeddings-v2-base-en')        \n        ```\n    </TabItem>\n    <TabItem value=\"Sentence-Transformers\" label=\"Sentence-Transformers\" default>\n        ```python\n        !pip install sentence-transformers\n        from superduper import vector\n        import sentence_transformers\n        from superduper_sentence_transformers import SentenceTransformer\n        \n        embedding_model = SentenceTransformer(\n            identifier=\"embedding\",\n            object=sentence_transformers.SentenceTransformer(\"BAAI/bge-small-en\"),\n            datatype=vector(shape=(1024,)),\n            postprocess=lambda x: x.tolist(),\n            predict_kwargs={\"show_progress_bar\": True},\n        )        \n        ```\n    </TabItem>\n    <TabItem value=\"Transformers\" label=\"Transformers\" default>\n        ```python\n        from superduper import vector\n        from superduper.components.model import Model, ensure_initialized, Signature\n        from transformers import AutoTokenizer, AutoModel\n        import torch\n        \n        class TransformerEmbedding(Model):\n            signature: Signature = 'singleton'\n            pretrained_model_name_or_path : str\n        \n            def init(self):\n                self.tokenizer = AutoTokenizer.from_pretrained(self.pretrained_model_name_or_path)\n                self.model = AutoModel.from_pretrained(self.pretrained_model_name_or_path)\n                self.model.eval()\n        \n            @ensure_initialized\n            def predict(self, x):\n                return self.predict([x])[0]\n                \n            @ensure_initialized\n            def predict(self, dataset):\n                encoded_input = self.tokenizer(dataset, padding=True, truncation=True, return_tensors='pt')\n                # Compute token embeddings\n                with torch.no_grad():\n                    model_output = self.model(**encoded_input)\n                    # Perform pooling. In this case, cls pooling.\n                    sentence_embeddings = model_output[0][:, 0]\n                # normalize embeddings\n                sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n                return sentence_embeddings.tolist()\n        \n        \n        embedding_model = TransformerEmbedding(identifier=\"embedding\", pretrained_model_name_or_path=\"BAAI/bge-small-en\", datatype=vector(shape=(384, )))        \n        ```\n    </TabItem>\n</Tabs>\n```python\nprint(len(embedding_model.predict(\"What is superduper\")))\n```\n\n",
  "---\nsidebar_label: Create Listener\nfilename: create_listener.md\n---\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n\n<!-- TABS -->\n# Create Listener\n\n## Two ways to define listener\n\n\n<Tabs>\n    <TabItem value=\"Listener\" label=\"Listener\" default>\n        ```python\n        from superduper import Listener\n        db.apply(\n            Listener(\n                key='key_name',\n                model=model,\n                select=select,\n            )\n        )        \n        ```\n    </TabItem>\n    <TabItem value=\"model.to_linstener\" label=\"model.to_linstener\" default>\n        ```python\n        db.apply(model.to_listener(key='key_name', select=select))        \n        ```\n    </TabItem>\n</Tabs>\n## Data passed into the model\n\n\n<Tabs>\n    <TabItem value=\"Single Field\" label=\"Single Field\" default>\n        ```python\n        # Model predict function definition: model.predict(x)\n        # Data example in database: {\"key_name\": 10}\n        # Then the listener will call model.predict(10)\n        from superduper import Listener\n        db.apply(\n            Listener(\n                key='key_name',\n                model=model,\n                select=select,\n            )\n        )        \n        ```\n    </TabItem>\n    <TabItem value=\"Multiple fields(*args)\" label=\"Multiple fields(*args)\" default>\n        ```python\n        # Model predict function definition: model.predict(x1, x2)\n        # Data example in database: {\"key_name_1\": 10, \"key_name_2\": 100}\n        # Then the listener will call model.predict(10, 100)\n        from superduper import Listener\n        db.apply(\n            Listener(\n                key=['key_name_1', 'key_name_2'],\n                model=model,\n                select=select,\n            )\n        )        \n        ```\n    </TabItem>\n    <TabItem value=\"Multiple fields(*kwargs)\" label=\"Multiple fields(*kwargs)\" default>\n        ```python\n        # Model predict function definition: model.predict(x1, x2)\n        # Data example in database: {\"key_name_1\": 10, \"key_name_2\": 100}\n        # Then the listener will call model.predict(x1=10, x2=100)\n        from superduper import Listener\n        db.apply(\n            Listener(\n                key={\"key_name_1\": \"x1\", \"key_name_2\": \"x2\"},\n                model=model,\n                select=select,\n            )\n        )        \n        ```\n    </TabItem>\n</Tabs>\n",
  "---\nsidebar_label: Setup tables or collections\nfilename: setup_tables_or_collections.md\n---\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n\n<!-- TABS -->\n# Setup tables or collections\n\n```python\nfrom superduper.components.table import Table\nfrom superduper import Schema\n\nschema = Schema(identifier=\"schema\", fields={\"x\": datatype})\ntable_or_collection = Table(\"documents\", schema=schema)\ndb.apply(table_or_collection)\n```\n\n",
  "---\nsidebar_label: Build LLM\nfilename: build_llm.md\n---\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n\n<!-- TABS -->\n# Build LLM\n\n\n<Tabs>\n    <TabItem value=\"OpenAI\" label=\"OpenAI\" default>\n        ```python\n        !pip install openai\n        from superduper_openai import OpenAIChatCompletion\n        \n        llm = OpenAIChatCompletion(identifier='llm', model='gpt-3.5-turbo')        \n        ```\n    </TabItem>\n    <TabItem value=\"Anthropic\" label=\"Anthropic\" default>\n        ```python\n        !pip install anthropic\n        from superduper_anthropic import AnthropicCompletions\n        import os\n        \n        os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-xxx\"\n        \n        predict_kwargs = {\n            \"max_tokens\": 1024,\n            \"temperature\": 0.8,\n        }\n        \n        llm = AnthropicCompletions(identifier='llm', model='claude-2.1', predict_kwargs=predict_kwargs)        \n        ```\n    </TabItem>\n    <TabItem value=\"vLLM\" label=\"vLLM\" default>\n        ```python\n        !pip install vllm\n        from superduper_vllm import VllmModel\n        \n        predict_kwargs = {\n            \"max_tokens\": 1024,\n            \"temperature\": 0.8,\n        }\n        \n        \n        llm = VllmModel(\n            identifier=\"llm\",\n            model_name=\"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\",\n            vllm_kwargs={\n                \"gpu_memory_utilization\": 0.7,\n                \"max_model_len\": 1024,\n                \"quantization\": \"awq\",\n            },\n            predict_kwargs=predict_kwargs,\n        )        \n        ```\n    </TabItem>\n    <TabItem value=\"Transformers\" label=\"Transformers\" default>\n        ```python\n        !pip install transformers datasets bitsandbytes accelerate\n        from superduper_transformers import LLM\n        \n        llm = LLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", load_in_8bit=True, device_map=\"cuda\", identifier=\"llm\", predict_kwargs=dict(max_new_tokens=128))        \n        ```\n    </TabItem>\n    <TabItem value=\"Llama.cpp\" label=\"Llama.cpp\" default>\n        ```python\n        !pip install llama_cpp_python\n        # !huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.2-GGUF mistral-7b-instruct-v0.2.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\n        \n        from superduper_llama_cpp.model import LlamaCpp\n        llm = LlamaCpp(identifier=\"llm\", model_name_or_path=\"mistral-7b-instruct-v0.2.Q4_K_M.gguf\")        \n        ```\n    </TabItem>\n</Tabs>\n```python\n# test the llm model\nllm.predict(\"Tell me about the superduper\")\n```\n\n",
  "---\nsidebar_label: Compute features\nfilename: compute_features.md\n---\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n\n<!-- TABS -->\n# Compute features\n\n\n<Tabs>\n    <TabItem value=\"Text\" label=\"Text\" default>\n        ```python\n        key = 'txt'\n        import sentence_transformers\n        from superduper import vector, Listener\n        from superduper_sentence_transformers import SentenceTransformer\n        \n        superdupermodel = SentenceTransformer(\n            identifier=\"embedding\",\n            object=sentence_transformers.SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\"),\n            postprocess=lambda x: x.tolist(),\n        )\n        \n        jobs, listener = db.apply(\n            Listener(\n                model=superdupermodel,\n                select=select,\n                key=key,\n                identifier=\"features\"\n            )\n        )        \n        ```\n    </TabItem>\n    <TabItem value=\"Image\" label=\"Image\" default>\n        ```python\n        key = 'image'\n        import torchvision.models as models\n        from torchvision import transforms\n        from superduper_torch import TorchModel\n        from superduper import Listener\n        from PIL import Image\n        \n        class TorchVisionEmbedding:\n            def __init__(self):\n                # Load the pre-trained ResNet-18 model\n                self.resnet = models.resnet18(pretrained=True)\n                \n                # Set the model to evaluation mode\n                self.resnet.eval()\n                \n            def preprocess(self, image):\n                # Preprocess the image\n                preprocess = preprocess = transforms.Compose([\n                    transforms.Resize(256),\n                    transforms.CenterCrop(224),\n                    transforms.ToTensor(),\n                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                ])\n                tensor_image = preprocess(image)\n                return tensor_image\n                \n        model = TorchVisionEmbedding()\n        superdupermodel = TorchModel(identifier='my-vision-model-torch', object=model.resnet, preprocess=model.preprocess, postprocess=lambda x: x.numpy().tolist())\n        \n        jobs, listener = db.apply(\n            Listener(\n                model=superdupermodel,\n                select=select,\n                key=key,\n                identifier=\"features\"\n            )\n        )        \n        ```\n    </TabItem>\n    <TabItem value=\"Text-And-Image\" label=\"Text-And-Image\" default>\n        ```python\n        import torch\n        import clip\n        from torchvision import transforms\n        from superduper import ObjectModel\n        from superduper import Listener\n        \n        import torch\n        import clip\n        from PIL import Image\n        \n        key={'txt': 'txt', 'image': 'image'}\n        \n        class CLIPModel:\n            def __init__(self):\n                # Load the CLIP model\n                self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n                self.model, self.preprocess = clip.load(\"RN50\", device=self.device)\n        \n            def __call__(self, text, image):\n                with torch.no_grad():\n                    text = clip.tokenize([text]).to(self.device)\n                    image = self.preprocess(Image.fromarray(image.astype(np.uint8))).unsqueeze(0).to(self.device)\n                    image_features = self.model.encode_image(image)[0].numpy().tolist()\n                    text_features = self.model.encode_text(text)[0].numpy().tolist()\n                return [image_features, text_features]\n                \n        model = CLIPModel()\n        \n        superdupermodel = ObjectModel(identifier=\"clip\", object=model, signature=\"**kwargs\", flatten=True, model_update_kwargs={\"document_embedded\": False})\n        \n        jobs, listener = db.apply(\n            Listener(\n                model=superdupermodel,\n                select=select,\n                key=key\n                identifier=\"features\"\n            )\n        )\n        \n        ```\n    </TabItem>\n    <TabItem value=\"Random\" label=\"Random\" default>\n        ```python\n        \n        key = 'random'\n        \n        import numpy as np\n        from superduper import superduper, ObjectModel, Listener\n        \n        def random(*args, **kwargs):\n            return np.random.random(1024).tolist()\n        \n        superdupermodel = ObjectModel(identifier=\"random\", object=random)\n        \n        jobs, listener = db.apply(\n            Listener(\n                model=superdupermodel,\n                select=select,\n                key=key,\n                identifier=\"features\"\n            )\n        )        \n        ```\n    </TabItem>\n    <TabItem value=\"Custom\" label=\"Custom\" default>\n        ```python\n        import numpy as np\n        from superduper import superduper, ObjectModel, Listener\n        \n        key = 'custom'\n        \n        # Define any feature calculation function\n        def calc_fake_feature(input_data):\n            fake_feature = list(range(10))\n            return fake_feature\n        \n        superdupermodel = ObjectModel(identifier=\"fake_feature\", object=calc_fake_feature)\n        \n        jobs, listener = db.apply(\n            Listener(\n                model=superdupermodel,\n                select=select,\n                # key of input_data\n                key=key,\n                identifier=\"features\"\n            )\n        )        \n        ```\n    </TabItem>\n</Tabs>\n",
  "---\nsidebar_label: Get useful sample data\nfilename: get_useful_sample_data.md\n---\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n\n<!-- TABS -->\n# Get useful sample data\n\n\n<Tabs>\n    <TabItem value=\"Text\" label=\"Text\" default>\n        ```python\n        !curl -O https://superduper-public-demo.s3.amazonaws.com/text.json\n        import json\n        \n        with open('text.json', 'r') as f:\n            data = json.load(f)        \n        ```\n    </TabItem>\n    <TabItem value=\"Text-Classification\" label=\"Text-Classification\" default>\n        ```python\n        !curl -O https://superduper-public-demo.s3.amazonaws.com/text_classification.json\n        import json\n        \n        with open(\"text_classification.json\", \"r\") as f:\n            data = json.load(f)\n        num_classes = 2        \n        ```\n    </TabItem>\n    <TabItem value=\"PDF\" label=\"PDF\" default>\n        ```python\n        !curl -O https://superduper-public-demo.s3.amazonaws.com/pdfs.zip && unzip -o pdfs.zip\n        import os\n        \n        data = [f'pdfs/{x}' for x in os.listdir('./pdfs') if x.endswith('.pdf')]        \n        ```\n    </TabItem>\n    <TabItem value=\"Image\" label=\"Image\" default>\n        ```python\n        !curl -O https://superduper-public-demo.s3.amazonaws.com/images.zip && unzip images.zip\n        import os\n        from PIL import Image\n        \n        data = [f'images/{x}' for x in os.listdir('./images') if x.endswith(\".png\")][:200]\n        data = [ Image.open(path) for path in data]        \n        ```\n    </TabItem>\n    <TabItem value=\"Image-Classification\" label=\"Image-Classification\" default>\n        ```python\n        !curl -O https://superduper-public-demo.s3.amazonaws.com/images_classification.zip && unzip images_classification.zip\n        import json\n        from PIL import Image\n        \n        with open('images/images.json', 'r') as f:\n            data = json.load(f)\n        \n        data = [{'x': Image.open(d['image_path']), 'y': d['label']} for d in data]\n        num_classes = 2        \n        ```\n    </TabItem>\n    <TabItem value=\"Video\" label=\"Video\" default>\n        ```python\n        !curl -O https://superduper-public-demo.s3.amazonaws.com/videos.zip && unzip videos.zip\n        import os\n        \n        data = [f'videos/{x}' for x in os.listdir('./videos')]\n        sample_datapoint = data[-1]\n        \n        from superduper.ext.pillow import pil_image\n        chunked_model_datatype = pil_image        \n        ```\n    </TabItem>\n    <TabItem value=\"Audio\" label=\"Audio\" default>\n        ```python\n        # !curl -O https://superduper-public-demo.s3.amazonaws.com/audio.zip && unzip audio.zip\n        import os\n        \n        data = [f'audios/{x}' for x in os.listdir('./audio')]\n        sample_datapoint = data[-1]        \n        ```\n    </TabItem>\n</Tabs>\n",
  "---\nsidebar_label: Create datatype\nfilename: create_datatype.md\n---\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n\n<!-- TABS -->\n# Create datatype\n\nSuperduperDB supports automatic data conversion, so users don’t need to worry about the compatibility of different data formats (`PIL.Image`, `numpy.array`, `pandas.DataFrame`, etc.) with the database.\n\nIt also supports custom data conversion methods for transforming data, such as defining the following Datatype.\n\n\n<Tabs>\n    <TabItem value=\"Vector\" label=\"Vector\" default>\n        ```python\n        from superduper import vector\n        \n        datatype = vector(shape=(3, ))        \n        ```\n    </TabItem>\n    <TabItem value=\"Tensor\" label=\"Tensor\" default>\n        ```python\n        from superduper_torch import tensor\n        import torch\n        \n        datatype = tensor(torch.float, shape=(32, 32, 3))        \n        ```\n    </TabItem>\n    <TabItem value=\"Array\" label=\"Array\" default>\n        ```python\n        from superduper.ext.numpy import array\n        import numpy as np\n        \n        datatype = array(dtype=\"float64\", shape=(32, 32, 3))        \n        ```\n    </TabItem>\n    <TabItem value=\"Text\" label=\"Text\" default>\n        ```python\n        datatype = 'str'        \n        ```\n    </TabItem>\n    <TabItem value=\"PDF\" label=\"PDF\" default>\n        ```python\n        from superduper import DataType\n        \n        # By creating a datatype and setting its encodable attribute to “file” for saving PDF files, \n        # all datatypes encoded as “file” will have their corresponding files uploaded to the artifact store. \n        # References will be recorded in the database, and the files will be downloaded locally when needed. \n        \n        datatype = DataType('pdf', encodable='file')        \n        ```\n    </TabItem>\n    <TabItem value=\"Image\" label=\"Image\" default>\n        ```python\n        from superduper.ext.pillow import pil_image\n        import PIL.Image\n        \n        datatype = pil_image        \n        ```\n    </TabItem>\n    <TabItem value=\"URI\" label=\"URI\" default>\n        ```python\n        \n        datatype = None        \n        ```\n    </TabItem>\n    <TabItem value=\"Audio\" label=\"Audio\" default>\n        ```python\n        from superduper.ext.numpy import array\n        from superduper import DataType\n        import scipy.io.wavfile\n        import io\n        \n        \n        def encoder(data):\n            buffer = io.BytesIO()\n            fs = data[0]\n            content = data[1]\n            scipy.io.wavfile.write(buffer, fs, content)\n            return buffer.getvalue()\n        \n        \n        def decoder(data):\n            buffer = io.BytesIO(data)\n            content = scipy.io.wavfile.read(buffer)\n            return content\n        \n        \n        datatype = DataType(\n            'wav',\n            encoder=encoder,\n            decoder=decoder,\n            encodable='artifact',\n        )        \n        ```\n    </TabItem>\n    <TabItem value=\"Video\" label=\"Video\" default>\n        ```python\n        from superduper import DataType\n        \n        # Create an instance of the Encoder with the identifier 'video_on_file' and load_hybrid set to False\n        datatype = DataType(\n            identifier='video_on_file',\n            encodable='file',\n        )        \n        ```\n    </TabItem>\n    <TabItem value=\"Encodable\" label=\"Encodable\" default>\n        ```python\n        from superduper import DataType\n        import pandas as pd\n        \n        def encoder(x, info=None):\n            return x.to_json()\n        \n        def decoder(x, info):\n            return pd.read_json(x)\n            \n        datatype = DataType(\n            identifier=\"pandas\",\n            encoder=encoder,\n            decoder=decoder\n        )        \n        ```\n    </TabItem>\n    <TabItem value=\"Artifact\" label=\"Artifact\" default>\n        ```python\n        from superduper import DataType\n        import numpy as np\n        import pickle\n        \n        \n        def pickle_encode(object, info=None):\n            return pickle.dumps(object)\n        \n        def pickle_decode(b, info=None):\n            return pickle.loads(b)\n        \n        \n        datatype = DataType(\n            identifier=\"VectorSearchMatrix\",\n            encoder=pickle_encode,\n            decoder=pickle_decode,\n            encodable='artifact',\n        )        \n        ```\n    </TabItem>\n</Tabs>\n",
  "# Create vector-index\n\n\n```python\nvector_index_name = 'my-vector-index'\n```\n\n\n```python\n# <tab: 1-Modality>\nfrom superduper import VectorIndex, Listener\n\njobs, _ = db.apply(\n    VectorIndex(\n        vector_index_name,\n        indexing_listener=Listener(\n            key=indexing_key,      # the `Document` key `model` should ingest to create embedding\n            select=select,       # a `Select` query telling which data to search over\n            model=embedding_model,         # a `_Predictor` how to convert data to embeddings\n        )\n    )\n)\n```\n\n\n```python\n# <tab: 2-Modalities>\nfrom superduper import VectorIndex, Listener\n\njobs, _ = db.apply(\n    VectorIndex(\n        vector_index_name,\n        indexing_listener=Listener(\n            key=indexing_key,      # the `Document` key `model` should ingest to create embedding\n            select=select,       # a `Select` query telling which data to search over\n            model=embedding_model,         # a `_Predictor` how to convert data to embeddings\n        ),\n        compatible_listener=Listener(\n            key=compatible_key,      # the `Document` key `model` should ingest to create embedding\n            model=compatible_model,         # a `_Predictor` how to convert data to embeddings\n            active=False,\n            select=None,\n        )\n    )\n)\n```\n\n\n```python\nquery_table_or_collection = select.table_or_collection\n```\n",
  "---\nsidebar_label: Build multimodal embedding models\nfilename: build_multimodal_embedding_models.md\n---\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n\n<!-- TABS -->\n# Build multimodal embedding models\n\nSome embedding models such as [CLIP](https://github.com/openai/CLIP) come in pairs of `model` and `compatible_model`.\nOtherwise:\n\n\n<Tabs>\n    <TabItem value=\"Text\" label=\"Text\" default>\n        ```python\n        from superduper_sentence_transformers import SentenceTransformer\n        \n        # Load the pre-trained sentence transformer model\n        model = SentenceTransformer(\n            identifier='all-MiniLM-L6-v2',\n            postprocess=lambda x: x.tolist(),\n        )        \n        ```\n    </TabItem>\n    <TabItem value=\"Image\" label=\"Image\" default>\n        ```python\n        from torchvision import transforms\n        import torch\n        import torch.nn as nn\n        import torchvision.models as models\n        \n        import warnings\n        \n        # Import custom modules\n        from superduper_torch import TorchModel, tensor\n        \n        # Define a series of image transformations using torchvision.transforms.Compose\n        t = transforms.Compose([\n            transforms.Resize((224, 224)),   # Resize the input image to 224x224 pixels (must same as here)\n            transforms.CenterCrop((224, 224)),  # Perform a center crop on the resized image\n            transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize the tensor with specified mean and standard deviation\n        ])\n        \n        # Define a preprocess function that applies the defined transformations to an input image\n        def preprocess(x):\n            try:\n                return t(x)\n            except Exception as e:\n                # If an exception occurs during preprocessing, issue a warning and return a tensor of zeros\n                warnings.warn(str(e))\n                return torch.zeros(3, 224, 224)\n        \n        # Load the pre-trained ResNet-50 model from torchvision\n        resnet50 = models.resnet50(pretrained=True)\n        \n        # Extract all layers of the ResNet-50 model except the last one\n        modules = list(resnet50.children())[:-1]\n        resnet50 = nn.Sequential(*modules)\n        \n        # Create a TorchModel instance with the ResNet-50 model, preprocessing function, and postprocessing lambda\n        model = TorchModel(\n            identifier='resnet50',\n            preprocess=preprocess,\n            object=resnet50,\n            postprocess=lambda x: x[:, 0, 0],  # Postprocess by extracting the top-left element of the output tensor\n            datatype=tensor(dtype='float', shape=(2048,))  # Specify the encoder configuration\n        )        \n        ```\n    </TabItem>\n    <TabItem value=\"Text-Image\" label=\"Text-Image\" default>\n        ```python\n        !pip install git+https://github.com/openai/CLIP.git\n        import clip\n        from superduper import vector\n        from superduper_torch import TorchModel\n        \n        # Load the CLIP model and obtain the preprocessing function\n        model, preprocess = clip.load(\"ViT-B/32\", device='cpu')\n        \n        # Define a vector with shape (1024,)\n        \n        output_datatpye = vector(shape=(1024,))\n        \n        # Create a TorchModel for text encoding\n        compatible_model = TorchModel(\n            identifier='clip_text', # Unique identifier for the model\n            object=model, # CLIP model\n            preprocess=lambda x: clip.tokenize(x)[0],  # Model input preprocessing using CLIP \n            postprocess=lambda x: x.tolist(), # Convert the model output to a list\n            datatype=output_datatpye,  # Vector encoder with shape (1024,)\n            forward_method='encode_text', # Use the 'encode_text' method for forward pass \n        )\n        \n        # Create a TorchModel for visual encoding\n        model = TorchModel(\n            identifier='clip_image',  # Unique identifier for the model\n            object=model.visual,  # Visual part of the CLIP model    \n            preprocess=preprocess, # Visual preprocessing using CLIP\n            postprocess=lambda x: x.tolist(), # Convert the output to a list \n            datatype=output_datatpye, # Vector encoder with shape (1024,)\n        )        \n        ```\n    </TabItem>\n    <TabItem value=\"Audio\" label=\"Audio\" default>\n        ```python\n        !pip install librosa\n        import librosa\n        import numpy as np\n        from superduper import ObjectModel\n        from superduper import vector\n        \n        def audio_embedding(audio_file):\n            # Load the audio file\n            MAX_SIZE= 10000\n            y, sr = librosa.load(audio_file)\n            y = y[:MAX_SIZE]\n            mfccs = librosa.feature.mfcc(y=y, sr=44000, n_mfcc=1)\n            mfccs =  mfccs.squeeze().tolist()\n            return mfccs\n        \n        if not get_chunking_datatype:\n            e =  vector(shape=(1000,))\n        else:\n            e = get_chunking_datatype(1000)\n        \n        model= ObjectModel(identifier='my-model-audio', object=audio_embedding, datatype=e)        \n        ```\n    </TabItem>\n</Tabs>\n",
  "---\nsidebar_label: Setup simple tables or collections\nfilename: setup_simple_tables_or_collections.md\n---\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n\n<!-- TABS -->\n# Setup simple tables or collections\n\n\n<Tabs>\n    <TabItem value=\"MongoDB\" label=\"MongoDB\" default>\n        ```python\n        # If data is in a format natively supported by MongoDB, we don't need to do anything.\n        # However to manually specify datatypes, do as below\n        from superduper import Schema, Document\n        from superduper_pillow import pil_image\n        from superduper.components.datatype import pickle_serializer\n        \n        fields = {\n            'serialized_content': pickle_serializer,\n            'img_content': pil_image,\n        }\n        \n        schema = Schema(identifier=\"my-schema\", fields=fields)\n        \n        # Add schema to system\n        db.apply(schema)\n        \n        # Now assert `Document` instances, specifying this schema\n        db['documents'].insert_many([\n            Document({\n                'serialized_content': item,\n                'img_content': img,\n            }, schema='my-schema')\n            for item, img in data\n        ])        \n        ```\n    </TabItem>\n    <TabItem value=\"SQL\" label=\"SQL\" default>\n        ```python\n        # If data is in a format natively supported by MongoDB, we don't need to do anything.\n        # However to manually specify datatypes, do as below\n        from superduper import Schema\n        from superduper_pillow import pil_image_hybrid\n        from superduper.components.datatype import pickle_serializer\n        \n        fields = {\n            'serialized_content': pickle_serializer,\n            'img_content': pil_image_hybrid,\n        }\n        \n        schema = Schema(identifier=\"my-schema\", fields=fields)\n        db.apply(schema)\n        \n        # Now assert `Document` instances, specifying this schema\n        db['documents'].insert_many([\n            Document({\n                'serialized_content': item,\n                'img_content': img,\n            }, schema='my-schema')\n            for item, img in data\n        ])        \n        ```\n    </TabItem>\n</Tabs>\n",
  "---\nsidebar_label: Get LLM Finetuning Data\nfilename: get_llm_finetuning_data.md\n---\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n\n<!-- TABS -->\n# Get LLM Finetuning Data\n\nThe following are examples of training data in different formats.\n\n\n<Tabs>\n    <TabItem value=\"Text\" label=\"Text\" default>\n        ```python\n        from datasets import load_dataset\n        from superduper.base.document import Document\n        dataset_name = \"timdettmers/openassistant-guanaco\"\n        dataset = load_dataset(dataset_name)\n        \n        train_dataset = dataset[\"train\"]\n        eval_dataset = dataset[\"test\"]\n        \n        train_documents = [\n            Document({**example, \"_fold\": \"train\"})\n            for example in train_dataset\n        ]\n        eval_documents = [\n            Document({**example, \"_fold\": \"valid\"})\n            for example in eval_dataset\n        ]\n        \n        datas = train_documents + eval_documents        \n        ```\n    </TabItem>\n    <TabItem value=\"Prompt-Response\" label=\"Prompt-Response\" default>\n        ```python\n        from datasets import load_dataset\n        from superduper.base.document import Document\n        dataset_name = \"mosaicml/instruct-v3\"\n        dataset = load_dataset(dataset_name)\n        \n        train_dataset = dataset[\"train\"]\n        eval_dataset = dataset[\"test\"]\n        \n        train_documents = [\n            Document({**example, \"_fold\": \"train\"})\n            for example in train_dataset\n        ]\n        eval_documents = [\n            Document({**example, \"_fold\": \"valid\"})\n            for example in eval_dataset\n        ]\n        \n        datas = train_documents + eval_documents        \n        ```\n    </TabItem>\n    <TabItem value=\"Chat\" label=\"Chat\" default>\n        ```python\n        from datasets import load_dataset\n        from superduper.base.document import Document\n        dataset_name = \"philschmid/dolly-15k-oai-style\"\n        dataset = load_dataset(dataset_name)['train'].train_test_split(0.9)\n        \n        train_dataset = dataset[\"train\"]\n        eval_dataset = dataset[\"test\"]\n        \n        train_documents = [\n            Document({**example, \"_fold\": \"train\"})\n            for example in train_dataset\n        ]\n        eval_documents = [\n            Document({**example, \"_fold\": \"valid\"})\n            for example in eval_dataset\n        ]\n        \n        datas = train_documents + eval_documents        \n        ```\n    </TabItem>\n</Tabs>\nWe can define different training parameters to handle this type of data.\n\n\n<Tabs>\n    <TabItem value=\"Text\" label=\"Text\" default>\n        ```python\n        # Function for transformation after extracting data from the database\n        transform = None\n        key = ('text')\n        training_kwargs=dict(dataset_text_field=\"text\")        \n        ```\n    </TabItem>\n    <TabItem value=\"Prompt-Response\" label=\"Prompt-Response\" default>\n        ```python\n        # Function for transformation after extracting data from the database\n        def transform(prompt, response):\n            return {'text': prompt + response + \"</s>\"}\n        \n        key = ('prompt', 'response')\n        training_kwargs=dict(dataset_text_field=\"text\")        \n        ```\n    </TabItem>\n    <TabItem value=\"Chat\" label=\"Chat\" default>\n        ```python\n        # Function for transformation after extracting data from the database\n        transform = None\n        \n        key = ('messages')\n        training_kwargs=None        \n        ```\n    </TabItem>\n</Tabs>\nExample input_text and output_text\n\n\n<Tabs>\n    <TabItem value=\"Text\" label=\"Text\" default>\n        ```python\n        data = datas[0]\n        input_text, output_text = data[\"text\"].rsplit(\"### Assistant: \", maxsplit=1)\n        input_text += \"### Assistant: \"\n        output_text = output_text.rsplit(\"### Human:\")[0]\n        print(\"Input: --------------\")\n        print(input_text)\n        print(\"Response: --------------\")\n        print(output_text)        \n        ```\n    </TabItem>\n    <TabItem value=\"Prompt-Response\" label=\"Prompt-Response\" default>\n        ```python\n        data = datas[0]\n        input_text = data[\"prompt\"]\n        output_text = data[\"response\"]\n        print(\"Input: --------------\")\n        print(input_text)\n        print(\"Response: --------------\")\n        print(output_text)        \n        ```\n    </TabItem>\n    <TabItem value=\"Chat\" label=\"Chat\" default>\n        ```python\n        data = datas[0]\n        messages = data[\"messages\"]\n        input_text = messages[:-1]\n        output_text = messages[-1][\"content\"]\n        print(\"Input: --------------\")\n        print(input_text)\n        print(\"Response: --------------\")\n        print(output_text)        \n        ```\n    </TabItem>\n</Tabs>\n",
  "---\nsidebar_label: Create Vector Search Model\nfilename: create_vector_search_model.md\n---\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n\n<!-- TABS -->\n# Create Vector Search Model\n\n```python\nitem = {indexing_key: '<var:query>'}\n```\n\n```python\nfrom superduper.components.model import QueryModel\n\nvector_search_model = QueryModel(\n    identifier=\"VectorSearch\",\n    select=query_table_or_collection.like(item, vector_index=vector_index_name, n=5).select(),\n    # The _source is the identifier of the upstream data, which can be used to locate the data from upstream sources using `_source`.\n    postprocess=lambda docs: [{\"text\": doc[indexing_key], \"_source\": doc[\"_source\"]} for doc in docs],\n    db=db\n)\n```\n\n```python\nvector_search_model.predict(query=query)\n```\n\n",
  "---\nsidebar_label: Build simple select queries\nfilename: build_simple_select_queries.md\n---\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n\n<!-- TABS -->\n# Build simple select queries\n\n\n<Tabs>\n    <TabItem value=\"MongoDB\" label=\"MongoDB\" default>\n        ```python\n        \n        select = db['<table-name>'].select()        \n        ```\n    </TabItem>\n    <TabItem value=\"SQL\" label=\"SQL\" default>\n        ```python\n        \n        select = db['<table-name>'].select()        \n        ```\n    </TabItem>\n</Tabs>\n",
  "---\nsidebar_label: Insert data\nfilename: insert_data.md\n---\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n\n<!-- TABS -->\n# Insert data\n\nIn order to create data, we need to create a `Schema` for encoding our special `Datatype` column(s) in the databackend.\n\n\n<Tabs>\n    <TabItem value=\"MongoDB\" label=\"MongoDB\" default>\n        ```python\n        from superduper import Document, DataType\n        \n        def do_insert(data, schema = None):\n            \n            if schema is None and (datatype is None or isinstance(datatype, str)):\n                data = [Document({'x': x['x'], 'y': x['y']}) if isinstance(x, dict) and 'x' in x and 'y' in x else Document({'x': x}) for x in data]\n                db.execute(table_or_collection.insert_many(data))\n            elif schema is None and datatype is not None and isinstance(datatype, DataType):\n                data = [Document({'x': datatype(x['x']), 'y': x['y']}) if isinstance(x, dict) and 'x' in x and 'y' in x else Document({'x': datatype(x)}) for x in data]\n                db.execute(table_or_collection.insert_many(data))\n            else:\n                data = [Document({'x': x['x'], 'y': x['y']}) if isinstance(x, dict) and 'x' in x and 'y' in x else Document({'x': x}) for x in data]\n                db.execute(table_or_collection.insert_many(data, schema=schema))\n        \n        ```\n    </TabItem>\n    <TabItem value=\"SQL\" label=\"SQL\" default>\n        ```python\n        from superduper import Document\n        \n        def do_insert(data):\n            db.execute(table_or_collection.insert([Document({'id': str(idx), 'x': x['x'], 'y': x['y']}) if isinstance(x, dict) and 'x' in x and 'y' in x else Document({'id': str(idx), 'x': x}) for idx, x in enumerate(data)]))\n        \n        ```\n    </TabItem>\n</Tabs>\n",
  "---\nsidebar_label: Build and train classifier\nfilename: build_and_train_classifier.md\n---\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n\n<!-- TABS -->\n# Build and train classifier\n\n\n<Tabs>\n    <TabItem value=\"Scikit-Learn\" label=\"Scikit-Learn\" default>\n        ```python\n        from superduper_sklearn import Estimator, SklearnTrainer\n        from sklearn.svm import SVC\n        \n        model = Estimator(\n            identifier=\"my-model\",\n            object=SVC(),\n            trainer=SklearnTrainer(\n                \"my-trainer\",\n                key=(input_key, \"label\"),\n                select=training_select,\n            ),\n        )        \n        ```\n    </TabItem>\n    <TabItem value=\"Torch\" label=\"Torch\" default>\n        ```python\n        import torch\n        from torch import nn\n        from superduper_torch.model import TorchModel\n        from superduper_torch.training import TorchTrainer\n        from torch.nn.functional import cross_entropy\n        \n        \n        class SimpleModel(nn.Module):\n            def __init__(self, input_size=16, hidden_size=32, num_classes=3):\n                super(SimpleModel, self).__init__()\n                self.fc1 = nn.Linear(input_size, hidden_size)\n                self.relu = nn.ReLU()\n                self.fc2 = nn.Linear(hidden_size, num_classes)\n        \n            def forward(self, x):\n                out = self.fc1(x)\n                out = self.relu(out)\n                out = self.fc2(out)\n                return out\n        \n        preprocess = lambda x: torch.tensor(x)\n        \n        # Postprocess function for the model output    \n        def postprocess(x):\n            return int(x.topk(1)[1].item())\n        \n        def data_transform(features, label):\n            return torch.tensor(features), label\n        \n        # Create a Logistic Regression model\n        # feature_length is the input feature size\n        model = SimpleModel(feature_size, num_classes=num_classes)\n        model = TorchModel(\n            identifier='my-model',\n            object=model,         \n            preprocess=preprocess,\n            postprocess=postprocess,\n            trainer=TorchTrainer(\n                key=(input_key, 'label'),\n                identifier='my_trainer',\n                objective=cross_entropy,\n                loader_kwargs={'batch_size': 10},\n                max_iterations=1000,\n                validation_interval=100,\n                select=select,\n                transform=data_transform,\n            ),\n        )        \n        ```\n    </TabItem>\n</Tabs>\n",
  "---\nsidebar_label: Apply a chunker for search\nfilename: apply_a_chunker_for_search.md\n---\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n\n<!-- TABS -->\n# Apply a chunker for search\n\n:::note\nNote that applying a chunker is ***not*** mandatory for search.\nIf your data is already chunked (e.g. short text snippets or audio) or if you\nare searching through something like images, which can't be chunked, then this\nwon't be necessary.\n:::\n\n\n<Tabs>\n    <TabItem value=\"Text\" label=\"Text\" default>\n        ```python\n        from superduper import model\n        \n        CHUNK_SIZE = 200\n        \n        @model(flatten=True, model_update_kwargs={'document_embedded': False})\n        def chunker(text):\n            text = text.split()\n            chunks = [' '.join(text[i:i + CHUNK_SIZE]) for i in range(0, len(text), CHUNK_SIZE)]\n            return chunks        \n        ```\n    </TabItem>\n    <TabItem value=\"PDF\" label=\"PDF\" default>\n        ```python\n        !pip install -q \"unstructured[pdf]\"\n        from superduper import model\n        from unstructured.partition.pdf import partition_pdf\n        \n        CHUNK_SIZE = 500\n        \n        @model(flatten=True)\n        def chunker(pdf_file):\n            elements = partition_pdf(pdf_file)\n            text = '\\n'.join([e.text for e in elements])\n            chunks = [text[i:i + CHUNK_SIZE] for i in range(0, len(text), CHUNK_SIZE)]\n            return chunks        \n        ```\n    </TabItem>\n    <TabItem value=\"Video\" label=\"Video\" default>\n        ```python\n        !pip install opencv-python\n        import cv2\n        import tqdm\n        from PIL import Image\n        from superduper.ext.pillow import pil_image\n        from superduper import model, Schema\n        \n        \n        @model(\n            flatten=True,\n            model_update_kwargs={'document_embedded': False},\n        )\n        def chunker(video_file):\n            # Set the sampling frequency for frames\n            sample_freq = 10\n            \n            # Open the video file using OpenCV\n            cap = cv2.VideoCapture(video_file)\n            \n            # Initialize variables\n            frame_count = 0\n            fps = cap.get(cv2.CAP_PROP_FPS)\n            extracted_frames = []\n            progress = tqdm.tqdm()\n        \n            # Iterate through video frames\n            while True:\n                ret, frame = cap.read()\n                if not ret:\n                    break\n                \n                # Get the current timestamp based on frame count and FPS\n                current_timestamp = frame_count // fps\n                \n                # Sample frames based on the specified frequency\n                if frame_count % sample_freq == 0:\n                    extracted_frames.append({\n                        'image': Image.fromarray(frame[:,:,::-1]),  # Convert BGR to RGB\n                        'current_timestamp': current_timestamp,\n                    })\n                frame_count += 1\n                progress.update(1)\n            \n            # Release resources\n            cap.release()\n            cv2.destroyAllWindows()\n            \n            # Return the list of extracted frames\n            return extracted_frames        \n        ```\n    </TabItem>\n    <TabItem value=\"Audio\" label=\"Audio\" default>\n        ```python\n        from superduper import model, Schema\n        \n        CHUNK_SIZE = 10  # in seconds\n        \n        @model(\n            flatten=True,\n            model_update_kwargs={'document_embedded': False},\n            output_schema=Schema(identifier='output-schema', fields={'audio': datatype}),\n        )\n        def chunker(audio):\n            chunks = []\n            for i in range(0, len(audio), CHUNK_SIZE):\n                chunks.append(audio[1][i: i + CHUNK_SIZE])\n            return [(audio[0], chunk) for chunk in chunks]        \n        ```\n    </TabItem>\n</Tabs>\nNow we apply this chunker to the data by wrapping the chunker in `Listener`:\n\n```python\nfrom superduper import Listener\n\nupstream_listener = Listener(\n    model=chunker,\n    select=select,\n    key='x',\n    uuid=\"chunk\",\n)\n\ndb.apply(upstream_listener)\n```\n\n",
  "---\nsidebar_label: Visualize Results\nfilename: visualize_results.md\n---\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n\n<!-- TABS -->\n# Visualize Results\n\n\n<Tabs>\n    <TabItem value=\"Text\" label=\"Text\" default>\n        ```python\n        from IPython.display import Markdown, display\n        \n        def visualize(item, source):\n            display(Markdown(item))\n            \n        def show(results, output_key, get_original_callable=None):\n            for result in results:\n                source = None\n                if '_source' in result:\n                    \n                    source = get_original_callable(result['_source'])\n                visualize(result[output_key], source)        \n        ```\n    </TabItem>\n    <TabItem value=\"Image\" label=\"Image\" default>\n        ```python\n        from IPython.display import display\n        \n        def visualize(item, source):\n            display(item)        # item is a PIL.Image\n        \n        def show(results, output_key, get_original_callable=None):\n            for result in results:\n                source = None\n                if '_source' in result:\n                    source = get_original_callable(result['_source'])\n                visualize(result[output_key].x, source)        \n        ```\n    </TabItem>\n    <TabItem value=\"Audio\" label=\"Audio\" default>\n        ```python\n        from IPython.display import Audio, display\n        \n        def visualize(item, source):\n            display(Audio(item[1], fs=item[0]))\n        \n        def show(results, output_key, get_original_callable=None):\n            for result in results:\n                source = None\n                if '_source' in result:\n                    \n                    source = get_original_callable(result['_source'])\n                visualize(result[output_key], source)        \n        ```\n    </TabItem>\n    <TabItem value=\"PDF\" label=\"PDF\" default>\n        ```python\n        from IPython.display import IFrame, display\n        \n        def visualize(item, source):\n            display(item)\n        \n        \n        def show(results, output_key, get_original_callable=None):\n            for result in results:\n                source = None\n                if '_source' in result:\n                    \n                    source = get_original_callable(result['_source'])\n                visualize(result[output_key], source)        \n        ```\n    </TabItem>\n    <TabItem value=\"Video\" label=\"Video\" default>\n        ```python\n        from IPython.display import display, HTML\n        \n        def visualize(uri, source):\n            timestamp = source    # increment to the frame you want to start at\n            \n            # Create HTML code for the video player with a specified source and controls\n            video_html = f\"\"\"\n            <video width=\"640\" height=\"480\" controls>\n                <source src=\"{uri}\" type=\"video/mp4\">\n            </video>\n            <script>\n                // Get the video element\n                var video = document.querySelector('video');\n                \n                // Set the current time of the video to the specified timestamp\n                video.currentTime = {timestamp};\n                \n                // Play the video automatically\n                video.play();\n            </script>\n            \"\"\"\n            \n            display(HTML(video_html))\n        \n        \n        def show(results, output_key, get_original_callable=None):\n            # show only the first video\n            for result in results:\n                source = result['_source']\n                result = result[output_key]\n                timestamp = result['current_timestamp']\n                uri = get_original_callable(source)['x']\n                print(uri, timestamp)\n                visualize(uri, timestamp)\n                break        \n        ```\n    </TabItem>\n</Tabs>\n",
  "```python\n# <tab: PDF>\nfrom PyPDF2 import PdfReader\n\nfrom superduper import model\n\n\n@model(flatten=True, model_update_kwargs={'document_embedded': False})\ndef text_extraction(file_path):\n    reader = PdfReader(file_path)\n    \n    texts = []\n    for i, page in tqdm(enumerate(reader.pages)):\n        text = page.extract_text() \n        texts.append(text)\n    return texts\n```\n",
  "---\nsidebar_label: Perform a vector search\nfilename: perform_a_vector_search.md\n---\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n\n<!-- TABS -->\n# Perform a vector search\n\n```python\nfrom superduper import Document\n\ndef get_sample_item(key, sample_datapoint, datatype=None):\n    if not isinstance(datatype, DataType):\n        item = Document({key: sample_datapoint})\n    else:\n        item = Document({key: datatype(sample_datapoint)})\n\n    return item\n\nif compatible_key:\n    item = get_sample_item(compatible_key, sample_datapoint, None)\nelse:\n    item = get_sample_item(indexing_key, sample_datapoint, datatype=datatype)\n```\n\nOnce we have this search target, we can execute a search as follows:\n\n\n<Tabs>\n    <TabItem value=\"MongoDB\" label=\"MongoDB\" default>\n        ```python\n        select = query_table_or_collection.like(item, vector_index=vector_index_name, n=10).find()        \n        ```\n    </TabItem>\n    <TabItem value=\"SQL\" label=\"SQL\" default>\n        ```python\n        select = query_table_or_collection.like(item, vector_index=vector_index_name, n=10).limit(10)        \n        ```\n    </TabItem>\n</Tabs>\n```python\nresults = db.execute(select)\n```\n\n",
  "---\nsidebar_label: Build image embedding model\nfilename: build_image_embedding_model.md\n---\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n\n<!-- TABS -->\n# Build image embedding model\nConstruct a neural network architecture to project high-dimensional image data into a lower-dimensional, dense vector representation\n(embedding) that preserves relevant semantic and visual information within a learned latent space.\n\n```python\n!wget https://raw.githubusercontent.com/openai/CLIP/main/CLIP.png\n```\n\n```python\nimage_path = \"CLIP.png\"\n```\n\n\n<Tabs>\n    <TabItem value=\"TorchVision\" label=\"TorchVision\" default>\n        ```python\n        \n        import torchvision.models as models\n        from torchvision import transforms\n        from superduper_torch import TorchModel\n        \n        class TorchVisionEmbedding:\n            def __init__(self):\n                # Load the pre-trained ResNet-18 model\n                self.resnet = models.resnet18(pretrained=True)\n                \n                # Set the model to evaluation mode\n                self.resnet.eval()\n                \n            def preprocess(self, image):\n                # Preprocess the image\n                preprocess = preprocess = transforms.Compose([\n                    transforms.Resize(256),\n                    transforms.CenterCrop(224),\n                    transforms.ToTensor(),\n                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                ])\n                tensor_image = preprocess(image)\n                return tensor_image\n                \n        embedding_model = TorchVisionEmbedding()\n        superdupermodel = TorchModel(identifier='my-vision-model-torch', object=embedding_model.resnet, preprocess=embedding_model.preprocess)        \n        ```\n    </TabItem>\n    <TabItem value=\"CLIP-multimodal\" label=\"CLIP-multimodal\" default>\n        ```python\n        import torch\n        import clip\n        from torchvision import transforms\n        from superduper_torch import TorchModel\n        \n        class CLIPVisionEmbedding:\n            def __init__(self):\n                # Load the CLIP model\n                self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n                self.model, self.preprocess = clip.load(\"RN50\", device=self.device)\n                \n            def preprocess(self, image):\n                # Load and preprocess the image\n                image = self.preprocess(image).unsqueeze(0).to(self.device)\n                return image\n                \n        embedding_model = CLIPVisionEmbedding()\n        superdupermodel = TorchModel(identifier='my-vision-model-clip', object=model.model, preprocess=model.preprocess, forward_method='encode_image')        \n        ```\n    </TabItem>\n    <TabItem value=\"HuggingFace (ViT)\" label=\"HuggingFace (ViT)\" default>\n        ```python\n        import torch\n        from transformers import AutoImageProcessor, AutoModel, AutoFeatureExtractor\n        import torchvision.transforms as T\n        from superduper_torch import TorchModel\n        \n        \n        class HuggingFaceEmbeddings(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                model_ckpt = \"nateraw/vit-base-beans\"\n                processor = AutoImageProcessor.from_pretrained(model_ckpt)\n                self.extractor = AutoFeatureExtractor.from_pretrained(model_ckpt)\n                self.model = AutoModel.from_pretrained(model_ckpt)\n        \n            def forward(self, x):\n                return self.model(pixel_values=x).last_hidden_state[:, 0].cpu()\n                \n                \n        class Preprocessor:\n            def __init__(self, extractor):\n                self.device = 'cpu'\n                # Data transformation chain.\n                self.transformation_chain = T.Compose(\n                    [\n                        # We first resize the input image to 256x256 and then we take center crop.\n                        T.Resize(int((256 / 224) * extractor.size[\"height\"])),\n                        T.CenterCrop(extractor.size[\"height\"]),\n                        T.ToTensor(),\n                        T.Normalize(mean=extractor.image_mean, std=extractor.image_std),\n                    ]\n                )\n            def __call__(self, image):\n                return self.transformation_chain(image).to(self.device)\n        \n            \n        embedding_model = HuggingFaceEmbeddings()\n        superdupermodel = TorchModel(identifier='my-vision-model-huggingface', object=embedding_model, preprocess=Preprocessor(embedding_model.extractor))        \n        ```\n    </TabItem>\n</Tabs>\n```python\nembedding_model.predict(Image.open(image_path))\n```\n\n",
  "# Select outputs of upstream listener\n\n:::note\nThis is useful if you have performed a first step, such as pre-computing \nfeatures, or chunking your data. You can use this query to \noperate on those outputs.\n:::\n\n\n```python\nindexing_key = upstream_listener.outputs_key\nselect = upstream_listener.outputs_select\n```\n",
  "---\nsidebar_label: Connecting listeners\nfilename: connecting_listeners.md\n---\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n\n<!-- TABS -->\n# Connecting listeners\n\n",
  "---\nsidebar_label: Create Model Output Type\nfilename: create_model_output_type.md\n---\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n\n<!-- TABS -->\n# Create Model Output Type\n\n\n<Tabs>\n    <TabItem value=\"MongoDB\" label=\"MongoDB\" default>\n        ```python\n        chunked_model_datatype = None        \n        ```\n    </TabItem>\n    <TabItem value=\"SQL\" label=\"SQL\" default>\n        ```python\n        from superduper_ibis.field_types import dtype\n        chunked_model_datatype = dtype('str')        \n        ```\n    </TabItem>\n</Tabs>\n",
  "---\nsidebar_label: Connect to superduper\nfilename: connect_to_superduper.md\n---\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n\n<!-- TABS -->\n# Connect to superduper\n\n:::note\nNote that this is only relevant if you are running superduper in development mode.\nOtherwise refer to \"Configuring your production system\".\n:::\n\n\n<Tabs>\n    <TabItem value=\"MongoDB\" label=\"MongoDB\" default>\n        ```python\n        from superduper import superduper\n        \n        db = superduper('mongodb://localhost:27017/documents')        \n        ```\n    </TabItem>\n    <TabItem value=\"SQLite\" label=\"SQLite\" default>\n        ```python\n        from superduper import superduper\n        db = superduper('sqlite://my_db.db')        \n        ```\n    </TabItem>\n    <TabItem value=\"MySQL\" label=\"MySQL\" default>\n        ```python\n        from superduper import superduper\n        \n        user = 'superduper'\n        password = 'superduper'\n        port = 3306\n        host = 'localhost'\n        database = 'test_db'\n        \n        db = superduper(f\"mysql://{user}:{password}@{host}:{port}/{database}\")        \n        ```\n    </TabItem>\n    <TabItem value=\"Oracle\" label=\"Oracle\" default>\n        ```python\n        from superduper import superduper\n        \n        user = 'sa'\n        password = 'Superduper#1'\n        port = 1433\n        host = 'localhost'\n        \n        db = superduper(f\"mssql://{user}:{password}@{host}:{port}\")        \n        ```\n    </TabItem>\n    <TabItem value=\"PostgreSQL\" label=\"PostgreSQL\" default>\n        ```python\n        !pip install psycopg2\n        from superduper import superduper\n        \n        user = 'postgres'\n        password = 'postgres'\n        port = 5432\n        host = 'localhost'\n        database = 'test_db'\n        db_uri = f\"postgres://{user}:{password}@{host}:{port}/{database}\"\n        \n        db = superduper(db_uri, metadata_store=db_uri.replace('postgres://', 'postgresql://'))        \n        ```\n    </TabItem>\n    <TabItem value=\"Snowflake\" label=\"Snowflake\" default>\n        ```python\n        from superduper import superduper\n        \n        user = \"superduperuser\"\n        password = \"superduperpassword\"\n        account = \"XXXX-XXXX\"  # ORGANIZATIONID-USERID\n        database = \"FREE_COMPANY_DATASET/PUBLIC\"\n        \n        snowflake_uri = f\"snowflake://{user}:{password}@{account}/{database}\"\n        \n        db = superduper(\n            snowflake_uri, \n            metadata_store='sqlite:///your_database_name.db',\n        )        \n        ```\n    </TabItem>\n    <TabItem value=\"Clickhouse\" label=\"Clickhouse\" default>\n        ```python\n        from superduper import superduper\n        \n        user = 'default'\n        password = ''\n        port = 8123\n        host = 'localhost'\n        \n        db = superduper(f\"clickhouse://{user}:{password}@{host}:{port}\", metadata_store=f'mongomock://meta')        \n        ```\n    </TabItem>\n    <TabItem value=\"DuckDB\" label=\"DuckDB\" default>\n        ```python\n        from superduper import superduper\n        \n        db = superduper('duckdb://mydb.duckdb')        \n        ```\n    </TabItem>\n    <TabItem value=\"Pandas\" label=\"Pandas\" default>\n        ```python\n        from superduper import superduper\n        \n        db = superduper(['my.csv'], metadata_store=f'mongomock://meta')        \n        ```\n    </TabItem>\n    <TabItem value=\"MongoMock\" label=\"MongoMock\" default>\n        ```python\n        from superduper import superduper\n        \n        db = superduper('mongomock:///test_db')        \n        ```\n    </TabItem>\n</Tabs>\n",
  "---\nsidebar_label: Build A Trainable LLM\nfilename: build_a_trainable_llm.md\n---\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n\n<!-- TABS -->\n# Build A Trainable LLM\n\n**Create an LLM Trainer for training**\n\nThe parameters of this LLM Trainer are basically the same as `transformers.TrainingArguments`, but some additional parameters have been added for easier training setup.\n\n```python\nfrom superduper_transformers import LLM, LLMTrainer\n\ntrainer = LLMTrainer(\n    identifier=\"llm-finetune-trainer\",\n    output_dir=\"output/finetune\",\n    overwrite_output_dir=True,\n    num_train_epochs=3,\n    save_total_limit=3,\n    logging_steps=10,\n    evaluation_strategy=\"steps\",\n    save_steps=100,\n    eval_steps=100,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    max_seq_length=512,\n    key=key,\n    select=select,\n    transform=transform,\n    training_kwargs=training_kwargs,\n)\n```\n\n\n<Tabs>\n    <TabItem value=\"Lora\" label=\"Lora\" default>\n        ```python\n        trainer.use_lora = True        \n        ```\n    </TabItem>\n    <TabItem value=\"QLora\" label=\"QLora\" default>\n        ```python\n        trainer.use_lora = True\n        trainer.bits = 4        \n        ```\n    </TabItem>\n    <TabItem value=\"Deepspeed\" label=\"Deepspeed\" default>\n        ```python\n        !pip install deepspeed\n        deepspeed = {\n            \"train_batch_size\": \"auto\",\n            \"train_micro_batch_size_per_gpu\": \"auto\",\n            \"gradient_accumulation_steps\": \"auto\",\n            \"zero_optimization\": {\n                \"stage\": 2,\n            },\n        }\n        trainer.use_lora = True\n        trainer.bits = 4\n        trainer.deepspeed = deepspeed        \n        ```\n    </TabItem>\n    <TabItem value=\"Multi-GPUS\" label=\"Multi-GPUS\" default>\n        ```python\n        trainer.use_lora = True\n        trainer.bits = 4\n        trainer.num_gpus = 2        \n        ```\n    </TabItem>\n</Tabs>\nCreate a trainable LLM model and add it to the database, then the training task will run automatically.\n\n```python\nllm = LLM(\n    identifier=\"llm\",\n    model_name_or_path=model_name,\n    trainer=trainer,\n    model_kwargs=model_kwargs,\n    tokenizer_kwargs=tokenizer_kwargs,\n)\n\ndb.apply(llm)\n```\n\n# Load the trained model\nThere are two methods to load a trained model:\n\n- **Load the model directly**: This will load the model with the best metrics (if the transformers' best model save strategy is set) or the last version of the model.\n- **Use a specified checkpoint**: This method downloads the specified checkpoint, then initializes the base model, and finally merges the checkpoint with the base model. This approach supports custom operations such as resetting flash_attentions, model quantization, etc., during initialization.\n\n\n<Tabs>\n    <TabItem value=\"Load Trained Model Directly\" label=\"Load Trained Model Directly\" default>\n        ```python\n        llm = db.load(\"model\", \"llm\")        \n        ```\n    </TabItem>\n    <TabItem value=\"Use a specified checkpoint\" label=\"Use a specified checkpoint\" default>\n        ```python\n        from superduper_transformers import LLM\n        experiment_id = db.show(\"checkpoint\")[-1]\n        version = None # None means the last checkpoint\n        checkpoint = db.load(\"checkpoint\", experiment_id, version=version)\n        llm = LLM(\n            identifier=\"llm\",\n            model_name_or_path=model_name,\n            adapter_id=checkpoint,\n            model_kwargs=dict(load_in_4bit=True)\n        )        \n        ```\n    </TabItem>\n</Tabs>\n",
  "---\nsidebar_label: Answer question with LLM\nfilename: answer_question_with_llm.md\n---\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n\n<!-- TABS -->\n# Answer question with LLM\n\n\n<Tabs>\n    <TabItem value=\"No-context\" label=\"No-context\" default>\n        ```python\n        \n        llm.predict(query)        \n        ```\n    </TabItem>\n    <TabItem value=\"Prompt\" label=\"Prompt\" default>\n        ```python\n        from superduper import model\n        from superduper.components.graph import Graph, input_node\n        \n        @model\n        def build_prompt(query):\n            return f\"Translate the sentence into German: {query}\"\n        \n        in_ = input_node('query')\n        prompt = build_prompt(query=in_)\n        answer = llm(prompt)\n        prompt_llm = answer.to_graph(\"prompt_llm\")\n        prompt_llm.predict(query)[0]        \n        ```\n    </TabItem>\n    <TabItem value=\"Context\" label=\"Context\" default>\n        ```python\n        from superduper import model\n        from superduper.components.graph import Graph, input_node\n        \n        prompt_template = (\n            \"Use the following context snippets, these snippets are not ordered!, Answer the question based on this context.\\n\"\n            \"{context}\\n\\n\"\n            \"Here's the question: {query}\"\n        )\n        \n        \n        @model\n        def build_prompt(query, docs):\n            chunks = [doc[\"text\"] for doc in docs]\n            context = \"\\n\\n\".join(chunks)\n            prompt = prompt_template.format(context=context, query=query)\n            return prompt\n            \n        \n        in_ = input_node('query')\n        vector_search_results = vector_search_model(query=in_)\n        prompt = build_prompt(query=in_, docs=vector_search_results)\n        answer = llm(prompt)\n        context_llm = answer.to_graph(\"context_llm\")\n        context_llm.predict(query)        \n        ```\n    </TabItem>\n</Tabs>\n",
  "---\nsidebar_label: Insert simple data\nfilename: insert_simple_data.md\n---\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n\n<!-- TABS -->\n# Insert simple data\n\nAfter turning on auto_schema, we can directly insert data, and superduper will automatically analyze the data type, and match the construction of the table and datatype.\n\n```python\nfrom superduper import Document\n\ntable_or_collection = db['documents']\n\nids = db.execute(table_or_collection.insert([Document(data) for data in datas]))\nselect = table_or_collection.select()\n```\n\n",
  "---\ndescription: Find detailed technical documentation for Superduper's AI and database integration solutions. Access comprehensive guides, API references, and tutorials to effectively implement and utilize SuperDuper technologies in your projects. (Formerly SuperDuperDB)\n---\n\n<head>\n  <title>Docs - Superduper</title>\n</head>\n\n# Welcome to Superduper!\n\nHi 👋 and welcome to the open-source Superduper project! \n\nSuperduper is a framework for building **AI-data** applications which are highly flexible, compositional and declarative, and deployable directly on major databases.\n\n:::info\nAn **AI-data** application is a new generation of application involving cross talk between AI models and data, which is updated \ndynamically in response to changing data, and supports a range of data queries, including queries involving AI model inferences.\n\nBasic examples include:\n\n- *Retrieval augmented generation*\n- *Data dependent retraining protocols*\n- *Semantic multimodal product search with text, image and product JSON formats.*\n\nThere is a whole world of AI-data applications out there waiting to be built.\nSuperduper is the perfect framework with which to get started!\n:::\n\n## What problem does Superduper solve?\n\nAI-data applications are highly complex. They require:\n\n- Maintaining a highly intricate state\n- Integrating with production data in databases\n- Deploying and maintaining many endpoints\n- Taking care of the life-cycle of AI models:\n  - Feature computations\n  - Training and re-training\n  - Computation and caching of outputs\n- A range of infrastructual work, from deploying custom hardware, triggering auto-scaling,\n  to deploying specialized solutions such as vector-search engines, model repositories and more\n\nThe fact that all of this is necessary explains the existence of the MLOPs, AIOps, LLMOps fields of engineering.\n\nWhat if none of this was necessary? What if AI engineers, data-scientist and decision makers, \ncould simply \"apply\" AI to the data in their data deployments? For example, the framework required would allow this type of command:\n\n```python\n<framework> apply ./path_to_app.zip your://database-uri\n```\n\nThat \"framework\" is `superduper`:\n\n```python\nsuperduper apply ./path_to_app.zip your://database-uri\n```\n\n## How does Superduper work?\n\nSuperduper is an AI-data application builder which is:\n\n- declarative \n- composable\n\n## Declarative paradigm\n\nSuperduper's main building block is class called a `Component`, which allows developers\nto build applications which \"declare\" the state they want the system to reach:\n\n- Which outputs should be kept up-to-date with which database input queries?\n- Which models should be deployed as endpoints?\n- Which models should be fine-tuned on which data prior to use?\n- Which outputs should be synchronized as vector-indexes?\n- Which computations should be run on a schedule?\n- Much, much more...\n\nThis aspect of the system is referred to as a declarative programming paradigm.\n\n## Compositionality\n\nSuperduper includes a range of `Component` types which may be easily subclassed and interchanged.\nFor example, if a developer has prototyped his model using an OpenAI LLM implementation, the LLM may be trivially exchanged for an on-premise, self-hosted Llama-3 implementation with a simple and predictable toggle in the codebase.\n\nFrom version 0.0.4 onwards, Superduper includes a range of plugins which developers may pick and choose from in open source, as well as a clear path for developers to build their own plugins. This plugin\narchitecture plays well with the compositional nature of the project.\n\n## Datalayer\n\nApplications are deployed on databases using a virtual AI-datalayer, referred to everywhere in this documentation as `db`.\n\nThis layer is composed of several important components, encompassing primary data, meta-data, artifacts and computation, which can be configured independently by the developer.\n\n## Superduper is open-sourced in Python under the Apache 2.0 license\n\nWe want to make Superduper the most inclusive and flexible AI-data framework around.\n\n:::note\nSuperduper pledges to retain the Apache 2.0 license as a key cornerstone \nof its philosophy and community ethos.\n:::\n\n### Superduper can handle classical AI/ machine learning paradigms...\n\n- classification\n- regression\n- forecasting\n- clustering\n- *and much, more more...*\n\n### As well as the most update to date techniques...\n\n- generative AI\n- LLMs\n- retrieval augmented generation (RAG)\n- computer vision\n- multimodal AI\n- *and much, more more...*\n\n![](/img/superduper.gif)\n\n## How can developers use Superduper?\n\nSuperduper boils down to 3 key patterns:\n\n#### 1. Connect to your data\n\n```python\nfrom superduper import superduper\n\ndb = superduper('<your-database-uri>')\n```\n\n#### 2. Apply AI to your data\n\n```python\n\ncomponent = ...   # build your AI with anything from the \n                  # python ecosystem\n\ndb.apply(component)\n```\n\nEquivalently from the command line:\n\n```bash\nsuperduper apply <path-component>.zip <uri-of-database>\n```\n\n#### 3. Query your data to obtain predictions, select data or perform vector-searches\n\n```python\nquery = db['<table-name>'].select()\nquery.execute()\n```\n\n### What does apply AI to data mean?\n\n\"Applying AI\" to data can mean numerous things, which developers \nare able to determine themselves. Any of these things is possible:\n\n- Compute outputs on incoming data\n- Train a model on database data\n- Configure vector-search on database\n- Measure the performance of models\n- Configure models to work together\n\n### Why is the \"DB\" so important in AI?\n\nSuperduper uses the fact that AI development always starts with data, ends with data, and interfaces \nwith data from conception, to productionized deployment. Any environment which has a chance of uniting \nthe diverse tools and stakeholders involved in AI development, needs a single way \nfor AI models and algorithms to be connected to data. ***That way is Superduper***.\n\n:::important\nBy integrating AI directly at data's source, Superduper enables developers to avoid implementing MLops.\n:::\n\n### What integrations does Superduper include?\n\n#### Data\n\n- MongoDB\n- PostgreSQL\n- SQLite\n- Snowflake\n- MySQL\n- Oracle\n- MSSQL\n- Clickhouse\n- Pandas\n\n#### AI frameworks\n\n- OpenAI\n- Cohere\n- Anthropic\n- PyTorch\n- Sklearn\n- Transformers\n- Sentence-Transformers\n\n### What important additional aspects does Superduper include?\n\nDevelopers may:\n\n- Choose whether to deploy Superduper in single blocking process or in scalable, non-blocking mode via `ray`\n- Choose whether to use their own self-programmed home grown models, or integrate AI APIs and open-source frameworks\n- Choose which type of data they use, including images, videos, audio, or custom datatypes\n- Automatically version and track all functionality they use\n- Keep control over which data is exposed to API services (if any) by leveraging model self-hosting\n\n### Key Features:\n\n- **[Integration of AI with your existing data infrastructure](https://docs.superduper.io/docs/docs/walkthrough/apply_models):** Integrate any AI models and APIs with your databases in a single scalable deployment without the need for additional pre-processing steps, ETL, or boilerplate code.\n- **[Streaming Inference](https://docs.superduper.io/docs/docs/walkthrough/daemonizing_models_with_listeners):** Have your models compute outputs automatically and immediately as new data arrives, keeping your deployment always up-to-date.\n- **[Scalable Model Training](https://docs.superduper.io/docs/docs/walkthrough/training_models):** Train AI models on large, diverse datasets simply by querying your training data. Ensured optimal performance via in-build computational optimizations.\n- **[Model Chaining](https://docs.superduper.io/docs/docs/walkthrough/linking_interdependent_models/)**: Easily set up complex workflows by connecting models and APIs to work together in an interdependent and sequential manner.\n- **[Simple, but Extendable Interface](https://docs.superduper.io/docs/docs/fundamentals/procedural_vs_declarative_api)**: Add and leverage any function, program, script, or algorithm from the Python ecosystem to enhance your workflows and applications. Drill down to any layer of implementation, including the inner workings of your models, while operating Superduper with simple Python commands.\n- **[Difficult Data Types](https://docs.superduper.io/docs/docs/walkthrough/encoding_special_data_types/)**: Work directly in your database with images, video, audio, and any type that can be encoded as `bytes` in Python.\n- **[Feature Storing](https://docs.superduper.io/docs/docs/walkthrough/encoding_special_data_types):** Turn your database into a centralized repository for storing and managing inputs and outputs of AI models of arbitrary data types, making them available in a structured format and known environment.\n- **[Vector Search](https://docs.superduper.io/docs/docs/walkthrough/vector_search):** No need to duplicate and migrate your data to additional specialized vector databases - turn your existing battle-tested database into a fully-fledged multi-modal vector-search database, including easy generation of vector embeddings and vector indexes of your data with preferred models and APIs.",
  "# `Plugin`\n\n- Supports a plugin system that dynamically loads Python modules and packages at runtime.\n- Supports functioning as subcomponents, providing dependency support for custom models and other local code.\n- Capable of applying to a database and storing in the artifact_store, exporting as a `superduper` format package for sharing with others.\n- Supports automatic installation of dependencies listed in the requirements.txt file under the plugin.\n\n***Usage pattern***\n\nCreate plugin\n\n```python\nfrom superduper.components.plugin import Plugin\nplugin_path = 'path/to/my_module.py'\nmy_plugin = Plugin(path=plugin_path)\n```\n\nPip install without python code\n\n```python\nfrom superduper.components.plugin import Plugin\n# If there is only one requirements file, the path must be a file that ends with requirements.txt.\nplugin = Plugin(path=\"deploy/installations/testenv_requirements.txt\")\n```\n\nPython package with requirements\n```python\nfrom superduper.components.plugin import Plugin\nplugin_path = 'path/to/my_package'\n# |_my_package\n#    |_requirements.txt\n#    |_file1.py\n#    |_file2.py\n#    |_sub_module\n#       |_file_a.py\nmy_plugin = Plugin(path=plugin_path)\n```\n\nPython module with requirements\n\n> If you want to add requirements to a Python file, you can create a requirement_plugin as a submodule of this module. \n> Then, the requirement_plugin will be loaded prior to the Python code.\n```python\nfrom superduper.components.plugin import Plugin\nrequirements_path = 'path/to/my_requirements.txt'\nrequirement_plugin = Plugin(path=requirements_path)\n\nplugin_path = 'path/to/my_module.py'\nmy_plugin = Plugin(path=plugin_path, plugins=[requirement_plugin])\n```\n\nExport plugin\n\n```python\nfrom superduper.components.plugin import Plugin\nplugin_path = 'plugin_path'\nmy_plugin = Plugin(path=plugin_path)\n\nmy_plugin.export(\"exports/plugin\")\n```\n\nLoad plugin\n\n```python\nfrom superduper.components.plugin import Plugin\nmy_plugin = Plugin.read(\"exports/plugin\")\n```\n\nAs a sub-component\n\n```python\nfrom utils import function\nclass Model:\n  def predict(self, X):\n    return function(X)\n\nfrom superduper.components.plugin import Plugin\nplugin = Plugin(path=\"./utils.py\")\nmodel = Model(identifier=\"test\", plugins=[plugin])\ndb.apply(model)\n\n# Then we can execute db.load(\"model\", \"test\") from any location.\n```\n\n***Explanation***\n\nInitialization and installation\n\n- During plugin initialization, `superduper` loads the component’s Python module or package into `sys.modules`, allowing subsequent use of native import statements for plugin utilization.\n- If the plugin package includes a `requirements.txt`, dependencies are installed prior to loading the Python code.\n- The plugin is installed only once per process; if it detects that the same plugin has already been installed in the current runtime, the installation is skipped.\n\nUsage\n\n- When exported locally, the plugin component saves all necessary dependency files for the plugins into the `superduper` package, allowing for sharing to different locations.\n- When executing `db.apply(plugin)`, the necessary Python dependency environment files for the plugin are saved in the artifact_store. During `db.load(\"plugin\", \"plugin_identifier\")`, these files are downloaded to the local `~/.superduper/plugin/` directory, followed by the initialization and installation of the plugin.\n- As a sub-component, Superduper’s encode and decode logic ensures that the plugin is loaded prior to the parent component to maintain dependency integrity.\n\n\n***See also***\n\n- [superduper.components.plugin](../api/components/plugin.md)",
  "# `Template`\n\n- A `Component` containing placeholders flagged with `<var:?>`\n- A `Template` may be used as the basis for applying multiple `Component` instances\n- `Template` is leveraged by `Application`.\n- Snapshot allows users to know that their validation comparisons are apples-to-apples\n- A `Template` is useful for sharing, migrating and distributing AI components\n- A `Template` may be applied to any Superduper deployment\n\n***Usage pattern***\n\n```python\nfrom superduper import *\n\nm = Listener(\n    model=ObjectModel(\n        object=lambda x: x + 2,\n        identifier='<var:model_id>',\n    ),\n    select=db['=collection'].find(),\n    key='<var:key>',\n)\n\n# optional \"info\" parameter provides details about usage (depends on developer use-case)\nt = Template('my-template', template=m.encode())\n\n# doesn't trigger work/ computations\n# just \"saves\" the template and its artifacts\ndb.apply(t) \n\nlistener = t(key='my_key', collection='my_collection', model_id='my_id')\n\n# this now triggers standard functionality\ndb.apply(listener)\n```\n\n***See also***\n\n- [Application](./application.md)\n",
  "# `Metric`\n\n- Wrapper around a function intended to validate model outputs\n- Function returns scalar value\n- Used in `Validation`, `Model` and `Trainer` to measure `Model` performance\n\n***Usage pattern***\n\n```python\nfrom superduper import Metric\n\ndef example_comparison(x, y):\n    return sum([xx == yy for xx, yy in zip(x, y)]) / len(x)\n\nm = Metric(\n    'accuracy',\n    object=example_comparison,\n)\n\ndb.apply(m)\n```\n\n***See also***\n\n- [Change-data capture](../cluster_mode/change_data_capture)\n- [Validation](./validation.md)",
  "# `Application`\n\n- An `Application` ships a pre-configured functionality in a compact and easy to understand way\n\n***Dependencies***\n\n- [`Template`](./template.md)\n\n***Usage pattern***\n\n(Learn how to build a model [here](model))\n\n```python\nfrom superduper import Application\n\ntemplate = db.load('template', 'my_template')\n\napplication = template(my_variable_1='my_value_1',\n                       my_variable_2='my_value_2')\n\ndb.apply(application.copy())\n```\n",
  "# `Dataset`\n\n- An immutable snapshot of a query saved to `db.artifact_store`\n- Used (currently) for validating model performance\n- Snapshot allows users to know that their validation comparisons are apples-to-apples\n\n***Usage pattern***\n\n(Learn how to build a model [here](model))\n\n```python\nfrom superduper import Listener\n\nds = Dataset(\n    'my-valid-data',\n    select=db['my_table'].select(),   # `.select()` selects whole table\n)\n\ndb.apply(ds)\n```\n\n***Explanation***\n\n- On creation `superduper` queries the data from the `db.databackend` based on the `select` parameter.\n- The data queries like this is saved as a persistent blob in the `db.artifact_store`.\n- When the dataset is reloaded, the `select` query is not executed again, instead the \n  data is reloaded from the `db.artifact_store`. This ensures the `Dataset` is always \"the same\".\n- `Dataset` is handy for making sure model validations are comparable.",
  "# `Model`\n\n- Wrap a standard AI model with functionality necessary for `superduper`\n- Configure validation and training of a model on database data\n\n***Dependencies***\n\n- [`Datatype`](./datatype.md)\n\n***(Optional dependencies)***\n\n- [`Validation`](./validation.md)\n- [`Trainer`](./trainer.md)\n\n***Usage pattern***\n\n:::note\nNote that `Model` is an abstract base class which cannot be called directly.\nTo use `Model` you should call any of its downstream implementations, \nsuch as [`ObjectModel`](../api/components/model.md#model-1) or models in the [AI-integrations](/docs/category/ai-integrations).\n:::\n\n***Important notes***\n\n`Model` instances can output data not-usually supported by your database.\nThis data will be encoded by default by `pickle`, but more control may be added\nby adding the parameters `datatype=...` or `output_schema=...`.\n\n## Implementations\n\nHere are a few `superduper` native implementations:\n\n**`ObjectModel`**\n\nUse a self-built model (`object`) or function with the system:\n\n```python\nfrom superduper import ObjectModel\n\nm = ObjectModel(\n    'my-model',\n    object=lambda x: x + 2,\n)\n\ndb.apply(m)\n```\n\n**`QueryModel`**\n\nUse a `superduper` query to extract data from `db`\n\n```python\nfrom superduper.components.model import QueryModel\n\nquery = ... # build a select query\nm = QueryModel('my-query', select=query, key='<key-to-extract>')\n\ndb.apply(m)\n```\n\n**`APIModel`**\n\nRequest model outputs hosted behind an API:\n\n```python\nfrom superduper.components.model import APIModel\n\nm = APIModel('my-api', url='http://localhost:6666?token={MY_DEV_TOKEN}&model={model}&text={text}')\n\ndb.apply(m)\n```\n\n**`SequentialModel`**\n\nMake predictions on the basis of a sequence of models:\n\n```python\nfrom superduper.components.model import SequentialModel\n\nm = SequentialModel(\n    'my-sequence',\n    models=[\n        model1,\n        model2,\n        model3,\n    ]\n)\n\ndb.apply(m)\n```\n\n***See also***\n\n- [Scikit-learn extension](../ai_integrations/sklearn)\n- [Pytorch extension](../ai_integrations/pytorch)\n- [Transformers extension](../ai_integrations/transformers)\n- [Llama.cpp extension](../ai_integrations/llama_cpp)\n- [Vllm extension](../ai_integrations/vllm)\n- [OpenAI extension](../ai_integrations/openai)\n- [Anthropic extension](../ai_integrations/anthropic)\n- [Cohere extension](../ai_integrations/cohere)\n- [Jina extension](../ai_integrations/jina)\n",
  "# `Validation`\n\n- Validate a `Model` by attaching a `Validation` component\n\n***Dependencies***\n\n- [`Metric`](metric.md)\n- [`Dataset`](./dataset.md)\n\n***Usage pattern***\n\n```python\nfrom superduper import Validation\n\nvalidation = Validation(\n    datasets=[dataset_1, ...],\n    metrics=[metric_1, ...],\n    key=('X', 'y')    # key to use for the comparison\n)\n\nmodel = Model(\n    ...     # standard arguments\n    validation=validation,\n)\n\n# Applying model recognizes `.validation` attribute\n# and validates model on the `.datasets` with `.metrics`\ndb.apply(model)\n```",
  "# Trigger\n\n- Listen for update, inserts and deletes\n- Take a specific action contigent on these changes\n- Can be deployed on Superduper Enterprise\n\n***Usage pattern***\n\n```python\nfrom superduper.components.trigger import Trigger\n\nclass MyTrigger(Trigger):\n    def if_change(self, ids):\n        data = db[self.table].select_ids(ids).execute()\n        for r in data:\n            if 'urgent' in r['title']:\n                db['notifications'].insert_one({\n                    'status': 'urgent',\n                    'msg': r['msg'],\n                }).execute()\n\nmy_trigger = MyTrigger('urgent', on='insert')\n```",
  "# `Listener`\n\n- apply a `model` to compute outputs on a query\n- outputs are refreshed every-time new data are added\n- outputs are saved to the `db.databackend`\n\n***dependencies***\n\n- [`Model`](./model.md)\n\n***usage pattern***\n\n(learn how to build a model [here](model))\n\n```python\nfrom superduper import Listener\nm = ...  # build a model\nq = ... # build a select query\n\n# either...\nlistener = Listener(\n    mode=m,\n    select=q,\n    key='x',\n)\n\n# or...\nlistener = m.to_listener(select=q, key='x')\n\ndb.apply(listener)\n```\n\n:::info\n*how do i choose the `key` parameter?*\n`key` refers to the field, or fields which \nwill be fed into the `model` as `*args` and `**kwargs`\n\nthe following forms are possible:\n- `key='x'`, \n- `key=('x','y')`, \n- `key={'x': 'x', 'y': 'y'}`, \n- `key=(('x',), {'y': 'y'})`,\n:::\n\n***see also***\n\n- [change-data capture](../cluster_mode/change_data_capture)",
  "# `Schema`\n\n- Apply a dictionary of `FieldType` and `DataType` to encode columnar data\n- Mostly relevant to SQL databases, but can also be used with MongoDB\n- `Schema` leverages encoding functionality of contained `DataType` instances\n\n***Dependencies***\n\n- [`DataType`](./datatype.md)\n\n***Usage pattern***\n\n(Learn how to build a `DataType` [here](datatype))\n\n*Vanilla usage*\n\nTable can potentially include\nmore columns which don't need encoding:\n\n```python\nfrom superduper import Schema\n\nschema = Schema(\n    'my-schema',\n    fields={\n        'img': dt_1,   # A `DataType`\n        'video': dt_2,   # Another `DataType`\n    }\n)\n\ndb.apply(schema)\n```\n\n*Usage with SQL*\n\nAll columns should be flagged with either `DataType` or `dtype`:\n\n```python\nfrom superduper.backends.ibis import dtype\n\nschema = Schema(\n    'my-schema',\n    fields={\n        'img': dt_1,   # A `DataType`\n        'video': dt_2,   # Another `DataType`\n        'txt', dtype('str'),\n        'numer', dtype('int'),\n    }\n)\n\ndb.apply(schema)\n```\n\n*Usage with MongoDB*\n\nIn MongoDB, the non-`DataType` columns/ fields can be omitted:\n\n```python\nschema = Schema(\n    'my-schema',\n    fields={\n        'img': dt_1,   # A `DataType`\n        'video': dt_2,   # Another `DataType`\n    }\n)\n\ndb.apply(schema)\n```\n\n*Usage with `Model` descendants (MongoDB only)*\n\nIf used together with `Model`, the model is assumed to emit `tuple` outputs, and these \nneed differential encoding. The `Schema` is applied to the columns of output, \nto get something which can be saved in the `db.databackend`.\n\n```python\nfrom superduper import ObjectModel\n\nm = Model(\n    'my-model',\n    object=my_object,\n    output_schema=schema\n)\n\ndb.apply(m)    # adds model and schema\n```\n\n***See also***\n\n- [Change-data capture](../cluster_mode/change_data_capture)",
  "# `DataType`\n\n- Convert objects which should be added to the database or model outputs to encoded `bytes`\n- `DataType` encodings and decodings are fully configurable and can be written as functions\n- Users may choose to encode `bytes` to strings with `base64` encoding\n\n***Usage pattern***\n\nDefault `DataTypes`, called \"serializers\":\n\n```python\nfrom superduper.components.datatype import serializers\n\npickle_serializer = serializers['pickle']\n```\n\nBuild your own `DataType` which saves data directly in the database:\n\n```python\nfrom superduper import DataType\n\ndt = DataType(\n    'my-datatype',\n    encoder=function_from_object_to_bytes,\n    decoder=function_from_bytes_to_object,\n    encodable='encodable',\n)\n\ndb.apply(dt)\n```\n\n:::info\n*How do I choose the `encodable` parameter?*\n\n\n| Value | Usage | \n| --- | --- |\n| `\"encodable\"` | `dt` adds object encoded as `bytes` directly to the `db.databackend` |\n| `\"artifact\"`  | `dt` saves object encoded as `bytes` to the `db.artifact_store` and a reference to `db.databackend` |\n| `\"lazy_artifact\"` | as per `\"artifact\"` but `bytes` must be actively loaded when needed |\n| `\"file\"` | `dt` simply saves a reference to a file in `db.artifact_store`; user handles loading |\n:::\n\n***See also***\n\n- [Encoding-difficult data](../advanced_usage/encoding_difficult_data)",
  "# `Trainer`\n\n- Train a `Model` by attaching a `Trainer` component\n\n***Usage pattern***\n\n(Learn how to build a `Model` [here](model))\n\n```python\nfrom superduper.ext.<extension> import <ExtensionTrainer>\n\ntrainer = <ExtensionTrainer>(\n    'my-trainer',\n    select=train_query,   # data to use for training\n    key=('X', 'y'),       # the columns/keys to use for training\n    **training_params,    # can vary greatly from framework to framework\n)\n\nmodel = Model(\n    ...     # standard arguments\n    validation=validation,   # validation will be executed after training\n    trainer=trainer,\n)\n\n# Applying model recognizes `.trainer` attribute\n# and trains model on the `.trainer.select` attribute\ndb.apply(model)\n```",
  "# `Table`\n\n- Use a table in your databackend database, which optionally has a `Schema` attached\n- Table can be a `MongoDB` collection or an SQL table.\n\n***Dependencies***\n\n- [`Schema`](./schema.md)\n\n***Usage pattern***\n\n(Learn how to build a `Schema` [here](schema))\n\n```python\nfrom superduper.backends.ibis import Table\n\ntable = Table(\n    'my-table',\n    schema=my_schema\n)\n\ndb.apply(table)\n```\n\nIn MongoDB, the attached `schema` will be used as the default `Schema` for that `Table` (collection).",
  "# Apply API\n\n:::info\nIn this section we re-use the datalayer variable `db` without further explanation.\nRead more about how to build it [here](../core_api/connect) and what it is [here](../fundamentals/datalayer_overview).\n:::\n\nAI functionality in superduper revolves around creating AI models, \nand configuring them to interact with data via the datalayer.\n\nThere are many decisions to be made and configured; for this superduper\nprovides the `Component` abstraction.\n\nThe typical process is:\n\n### 1. Create a component\n\nBuild your components, potentially including other subcomponents.\n\n```python\nfrom superduper import <ComponentClass>\n\ncomponent = <ComponentClass>(\n    'identifier',\n    **kwargs   # can include other components\n)\n```\n\n### 2. Apply the component to the datalayer\n\n\"Applying\" the component the `db` datalayer, also\napplies all sub-components. So only 1 call is needed.\n\n```python\ndb.apply(component)\n```\n\n### 3. Reload the component (if necessary)\n\nThe `.apply` command saves everything necessary to reload the component\nin the superduper system.\n\n```python\nreloaded = db.load('type_id', 'identifier')   # `type_id`\n```\n\n### 4. Export the component (to share/ migrate)\n\nThe `.export` command saves the entirety of the component's parameters, \ninline code and artifacts in a directory:\n\n```python\ncomponent.export('my_component')\n```\n\nThe directory structure looks like this.\nIt contains the meta-data of the component as\nwell as a \"mini-artifact-store\". Together\nthese items make the export completely portable.\n\n```\nmy_component\n|_component.json // meta-data and imports of component\n|_blobs   // directory of blobs used in the component\n| |_234374364783643764\n| |_574759874238947320\n|_files   // directory of files used in the component\n  |_182372983721897389\n  |_982378978978978798\n```\n\nYou can read about the serialization mechanism [here](../production/superduper_protocol.md).\n\n## Read more\n\n```mdx-code-block\nimport DocCardList from '@theme/DocCardList';\n\n<DocCardList />\n```\n",
  "# `DataInit`\n\n- Used to automatically insert initialization data during application build.\n\n***Usage pattern***\n\n```python\nfrom superduper.components.dataset import DataInit\ndata = [{\"x\": i, \"y\": [1, 2, 3]} for i in range(10)]\ndata_init = DataInit(data=data, table=\"documents\", identifier=\"test_data_init\")\n\ndb.apply(data_init)\n```\n\n***Explanation***\n\n- When db.apply(data_init) is executed, DataInit inserts data into the specified table.",
  "# `Checkpoint`\n\n- Save intermediate results of training via `superduper`\n- Load a different point of the training process by specifying `Checkpoint` explicitly\n- Particularly useful with deep-learning models\n\n***Usage pattern***\n\n```python\nfrom superduper import Model\nfrom superduper.components.training import Checkpoint\n\nclass MyModel(Model):\n    checkpoints: t.List[Checkpoint]\n    best_checkpoint: t.Optional[int] = None\n\n    def __post_init__(self, db, artifacts):\n        super().__post_init__(db, artifacts)\n\n        if self.best_checkpoint is not None:\n            self.load_weights(self.checkpoints[self.best_checkpoint])\n\n    def load_weights(self):\n        ... # custom load logic\n\nmy_model = MyModel('my-model')\n\nmy_model.checkpoints.append('path/to/model/weights-0.pt')\nmy_model.checkpoints.append('path/to/model/weights-1.pt')\nmy_model.best_checkpoint = 1\n\n# saves model as well as checkpoints to db.artifact_store\ndb.apply(my_model)     \n\n# loads `self.checkpoints[1]`\nm = db.load('model', 'my-model')\n```",
  "# `Cron Job`\n\n- Iterate computations, queries and actions on a crontab\n- Can be deployed on Superduper Enterprise\n\n***Usage pattern***\n\nCron-job can take any actions relating to `db`\nwhich is loaded as an attribute of the `Component`.\n\n```python\nimport datetime\nfrom superduper.components.cron_job import CronJob\n\nclass MyCronJob(CronJob):\n    table: str\n\n    # overwriting this function defines actions to be \n    # taken on a schedule\n    def run(self):\n        results = list(self.db[self.table].select())\n\n        date = str(datetime.now())\n\n        with open(f'{date}.bak', 'wb') as f:\n            json.dump(results)\n        \n        # for example, backing up a collection every day\n        os.system(f'aws s3 cp {date}.bak s3://my-bucket/{date}.bak')\n\ncron_job = MyCronJob(table='documents', schedule='0 0 * * * *')\n\ndb.apply(cron_job)\n```",
  "# `VectorIndex`\n\n- Wrap a `Listener` so that outputs are searchable\n- Can optionally take a second `Listener` for multimodal search\n- Applies to `Listener` instances containing `Model` instances which output vectors, arrays or tensors\n- Maybe leveraged in superduper queries with the `.like` operator\n\n***Dependencies***\n\n- [`Listener`](./listener.md)\n\n***Usage pattern***\n\n```python\nfrom superduper import VectorIndex\n\nvi = VectorIndex(\n    'my-index',\n    indexing_listener=listener_1  # defined earlier calculates searchable vectors\n)\n\n# or directly from a model\nvi = model_1.to_vector_index(select=q, key='x')\n\n# or may use multiple listeners\nvi = VectorIndex(\n    'my-index',\n    indexing_listener=listener_1\n    compatible_listener=listener_2 # this listener can have `listener_2.active = False`\n)\n\ndb.apply(vi)\n```\n\n***See also***\n\n- [vector-search queries](../query_api/vector_search)\n- [vector-search service](../cluster_mode/vector_comparison_service)",
  "---\nsidebar_position: 18\n---\n\n# Applying `Model` instances to `db`\n\nThere are 4 key AI `Model` sub classes, see [here](../apply_api/model) for detailed usage:\n\n| Path | Description |\n| --- | ---\n| `superduper.components.model.ObjectModel` | Wraps a Python object to compute outputs |\n| `superduper.components.model.APIModel` | Wraps a model hosted behind an API to compute outputs |\n| `superduper.components.model.QueryModel` | Maps a Database select query with a free variable over inputs |\n| `superduper.components.model.SequentialModel` | Computes outputs sequentially for a sequence of `Model` instances |\n\nAs well as these key sub-classes, we have classes in the `superduper.ext.*` subpackages:\nSee [here](../ai_integrations/) for more information.\n\nWhenever one of these `Model` descendants is instantiated, and `db.apply(model)` is called, \nseveral things can (do) happen:\n\n1. The `Model`'s metadata is saved in the `db.metadata_store`.\n2. It's associated data (e.g.) model is saved in the `db.artifact_store`.\n3. (Optional) if the `Model` has a `Trainer` attached, then the `Model` is trained/ fit on the specified data.\n4. (Optional) if the `Model` has an `Evaluation` method attached, then the `Model` is evaluated on the specified data.\n\n<!-- ### Scikit-Learn\n\n```python\nfrom superduper.ext.sklearn import Estimator\nfrom sklearn.svm import SVC\n\ndb.add(Estimator(SVC()))\n```\n\n### Transformers\n\n```pytho\nfrom superduper.ext.transformers import Pipeline\nfrom superduper import superduper\n\ndb.add(Pipeline(task='sentiment-analysis'))\n```\n\nThere is also support for building the pipeline in separate stages with a high degree of customization.\nThe following is a speech-to-text model published by [facebook research](https://arxiv.org/abs/2010.05171) and shared [on Hugging-Face](https://huggingface.co/facebook/s2t-small-librispeech-asr):\n\n```python\nfrom superduper.ext.transformers import Pipeline\nfrom transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n\nmodel = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\nprocessor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n\ntranscriber = Pipeline(\n    identifier='transcription',\n    object=model,\n    preprocess=processor,\n    preprocess_kwargs={'sampling_rate': SAMPLING_RATE, 'return_tensors': 'pt', 'padding': True}, # Please replace the placeholder `SAMPLING_RATE` with the appropriate value in your context.\n    postprocess=lambda x: processor.batch_decode(x, skip_special_tokens=True),\n    predict_method='generate',\n    preprocess_type='other',\n)\n\ndb.add(transcriber)\n```\n\n### PyTorch\n\n```python\nimport torch\nfrom superduper.ext.torch import Module\n\nmodel = Module(\n    identifier='my-classifier',\n    preprocess=lambda x: torch.tensor(x),\n    object=torch.nn.Linear(64, 512),\n    postprocess=lambda x: x.topk(1)[0].item(),\n)\n\ndb.add(model)\n```\n\n### Important Parameters, Common to All Models\n  \n| Name | Function |\n| --- | --- |\n| `identifier` | A unique name for `superduper`, for later use and recall |\n| `object` | The model-object, including parameters and hyper-parameters providing heavy lifting |\n| `preprocess` | `Callable` which processes individual rows/records/fields from the database prior to passing to the model |\n| `postprocess` | `Callable` applied to individual rows/items or output |\n| `encoder` | An `Encoder` instance applied to the model output to save that output in the database |\n| `schema` | A `Schema` instance applied to a model's output, whose rows are dictionaries |\n\n\n## Using AI APIs \n\nIn superduper, developers are able to interact with popular AI API providers, in a way very similar to \n[integrating with AI open-source or home-grown models](./ai_models.md). Instantiating a model from \nthese providers is similar to instantiating a `Model`:\n\n### OpenAI\n\n**Supported**\n\n| Description | Class-name |\n| --- | --- |\n| Embeddings | `OpenAIEmbedding` |\n| Chat models | `OpenAIChatCompletion` |\n| Image generation models | `OpenAIImageCreation` |\n| Image edit models | `OpenAIImageEdit` |\n| Audio transcription models | `OpenAIAudioTranscription` |\n\n**Usage**\n\n```python\nfrom superduper.ext.openai import OpenAI<ModelType> as ModelCls\n\ndb.add(Modelcls(identifier='my-model', **kwargs))\n```\n\n### Cohere\n\n**Supported**\n\n| Description | Class-name |\n| --- | --- |\n| Embeddings | `CohereEmbedding` |\n| Chat models | `CohereChatCompletion` |\n\n**Usage**\n\n```python\nfrom superduper.ext.cohere import Cohere<ModelType> as ModelCls\n\ndb.add(Modelcls(identifier='my-model', **kwargs))\n```\n\n### Anthropic\n\n**Supported**\n\n| Description | Class-name |\n| --- | --- |\n| Chat models | `AnthropicCompletions` |\n\n**Usage**\n\n```python\nfrom superduper.ext.anthropic import Anthropic<ModelType> as ModelCls\n\ndb.add(Modelcls(identifier='my-model', **kwargs))\n```\n\n### Jina\n\n**Supported**\n\n| Description | Class-name |\n| --- | --- |\n| Embeddings | `JinaEmbedding` |\n\n**Usage**\n\n```python\nfrom superduper.ext.jina import JinaEmbedding\n\ndb.add(JinaEmbedding(identifier='jina-embeddings-v2-base-en', api_key='JINA_API_KEY')) # You can also set JINA_API_KEY as environment variable\n``` -->",
  "# LLMs\n\nSuperduper allows users to work with LLM services and models\n\nHere is a table of LLMs supported in Superduper:\n\n| Class | Description |\n| --- | --- |\n| `superduper.ext.transformers.LLM` | Useful for trying and fine-tuning a range of open-source LLMs |\n| `superduper.ext.vllm.vLLM` | Useful for fast self-hosting of LLM models with CUDA |\n| `superduper.ext.llamacpp.LlamaCpp` | Useful for fast self-hosting of LLM models without requiring CUDA |\n| `superduper.ext.openai.OpenAIChatCompletion` | Useful for getting started quickly |\n| `superduper.ext.anthropic.AnthropicCompletion` | Useful alternative for getting started quickly |\n\nYou can find the snippets [here](../reusable_snippets/build_llm)\n\n:::tip\nConnect your LLM to data and vector-search using `SequentialModel` or `GraphModel`.\n:::\n",
  "# Training models directly on your datastore\n\n`Model` instances may be trained if a `trainer` is set on the `Model` when `db.apply` is called.\nWhen models are trained, if `CFG.cluster.compute` has been configured with a `ray` scheduler, then `superduper` deploys [a job on the connected `ray` cluster](../production_features/non_blocking_ray_jobs).\n\n## Basic pattern\n\n```python\nfrom superduper.ext.<framework> import <Framework>Trainer\nfrom superduper.ext.<framework> import <ModelCls>\n\ndb.apply(\n    <ModelCls>(\n        *args, \n        trainer=<Framework>Trainer(**trainer_kwargs),\n        **kwargs,\n    )\n)\n```\n\n## Fitting/ training models by framework\n\nNot all `Model` types are trainable. We support training for the following frameworks:\n\n| Framework | Training Link |\n| --- | --- |\n| Scikit-Learn | [link](../ai_integrations/sklearn#training) |\n| PyTorch | [link](../ai_integrations/pytorch#training) |\n| Transformers | [link](../ai_integrations/transformers#training) |\n\n<!-- ### Scikit-learn\n\nSee [here]\n\n```python\nfrom superduper.ext.sklearn import Estimator\nfrom sklearn.svm import SVC\n\nm = Estimator(SVC(C=0.05))\n\nm.fit(\n    X='<input-col>',\n    y='<target-col>',\n    select=<query>,  # MongoDB, Ibis or SQL query\n    db=db,\n)\n```\n\n### Transformers\n\n```python\nfrom superduper.ext.transformers import Pipeline\nfrom superduper import superduper\n\nm = Pipeline(task='sentiment-analysis')\n\nm.fit(\n    X='<input-col>',\n    y='<target-col>',\n    db=db,\n    select=<query>,   # MongoDB, Ibis or SQL query\n    dataloader_num_workers=4,   # **kwargs are passed to `transformers.TrainingArguments`\n)\n```\n\n### PyTorch\n\n```python\nimport torch\nfrom superduper.ext.torch import Module\n\nmodel = Module(\n    'my-classifier',\n    preprocess=lambda x: torch.tensor(x),\n    object=torch.nn.Linear(64, 512),\n    postprocess=lambda x: x.topk(1)[0].item(),\n)\n\nmodel.fit(\n    X='<input>',\n    db=db,\n    select=<query>,  # MongoDB, Ibis or SQL query\n    batch_size=100,  # any **kwargs supported by `superduper.ext.torch.TorchTrainerConfiguration`\n    num_workers=4,\n)\n``` -->",
  "# Models\n\nA key `Component` type in Superduper is `Model` and its descendants.\nThe intended usage is that `Model` wraps classical AI and machine learning models, \nAI APIs, as well as important processing steps involved in building such models, \nsuch as feature-computation.\n\nSee [here](../apply_api/model) for basic usage. This section gives detailed\nusage information as well as information about building your own model types.\n\n## Read more\n\n```mdx-code-block\nimport DocCardList from '@theme/DocCardList';\n\n<DocCardList />\n```\n",
  "# Configuring models to ingest features from other models\n\nThere are two ways to connect models in Superduper:\n\n- via interdependent `Listeners`\n- via the `Graph` component\n\nIn both cases, the first step is define the computation graph using \na simple formalism.\n\n## Building a computation graph\n\nHere is an example of building a graph with 3 members:\n\n```python\nfrom superduper.components.graph import document_node\nfrom superduper import ObjectModel\n\nm1 = ObjectModel('m1', object=lambda x: x + 1)\nm2 = ObjectModel('m2', object=lambda x: x + 2)\nm3 = ObjectModel('m3', object=lambda x, y: x * y)\n\ninput = document_node('x1', 'x2')\n\n# `outputs` specifies in which field the outputs will be cached/ saved\nout1 = m1(x=input['x1'], outputs='o1')\nout2 = m2(x=input['x2'], outputs='o2')\nout3 = m3(x=out1, y=out2, outputs='o3')\n```\n\nThe variable `out3` now contains the computation graph in `out3.parent_graph`.\n\nIn order to use this graph, developers may choose between creating a `Model`\ninstance which passes inputs recursively through the graph:\n\n```python\n>>> graph_model = out3.to_graph('my_graph_model')\n>>> graph_model.predict({'x1': 1, 'x2': 2})\n6\n```\n\nand creating a `Stack` of `Listener` instances which can be applied with `db.apply`\nwhere intermediate outputs are cached in `db.databackend`.\nThe order in which these listeners are applied respects \nthe graph topology.\n\n```python\nq = db['my_documents'].find()\nstack = out3.to_listeners(q, 'my_stack')\ndb.apply(stack)\n```\n",
  "# Creating embeddings\n\nSuperduper supports a number of embedding models which may be used to create\nvectors ready for downstream tasks, including vector-search\n\nHere is an overview of pre-packaged embedding models:\n\n| Class | \n| --- | \n| `superduper.ext.sentence_transformers.SentenceTransformer` |\n| `superduper.ext.openai.OpenAIEmbeddings` | \n| `superduper.ext.cohere.CohereEmbeddings` |",
  "# Key methods of `Model`\n\nAll usage in `superduper` proceeds by changing or setting the attributes of a `Component`\nand then calling `db.apply`. \n\nHowever it may be useful to know that the following methods specific to `Model` play a key role.\nSee [here](../apply_api/overview#key-methods) for an overview of key-methods specific to `Component`.\n\n| Method | Description | Optional |\n| --- | --- | --- |\n| `Model.predict` | Predict on a single data-point | `FALSE` | \n| `Model.predict_batches` | Predict on batches of data-points | `FALSE` |\n| `Model.predict_in_db` | Predict and save predictions in `db` | `FALSE` |\n| `Model.predict_in_db_job` | `predict_in_db` as compute job | `FALSE` |\n| `Model.validate` | Validate on datasets with metrics | `FALSE` |\n| `Model.validate_in_db` | Validate on datasets with metrics and save in `db` | `FALSE` |\n| `Model.validate_in_db_job` | `validate_in_db` as job | `FALSE` |\n| `Model.fit` | Fit on datasets | `TRUE` |\n| `Model.fit_in_db` | Fit on data in `db` | `TRUE` |\n| `Model.fit_in_db_job` | `.fit_in_db` as job | `TRUE` |\n",
  "# Computing model outputs with listeners\n\n## Usage\n\n```python\ndb.apply(\n    Listener(\n        model=my_model,\n        key='my-key',\n        select=<query>,\n        predict_kwargs={**<model_dot_predict_kwargs>},\n    )\n)\n```\n\n## Outcome\n\nIf a `Listener` has been created, whenever new data is added to `db`, \nthe `Model` instance is loaded and outputs as computed with `Model.predict` are evaluated on the inserted data.\n\n:::info\nIf [change-data-capture (CDC)](../production/change_data_capture.md) has been configured, \ndata may even be inserted from third-party clients such as `pymongo`, and is nonetheless still processed\nby configured `Listeners` via the CDC service.\n:::",
  "# Evaluating models\n\nSee [here](../apply_api/validation.md).",
  "# Bring your own models\n\nThere are two ways to bring your own computations\nand models to Superduper.\n\n1. Wrap your own Python functions\n2. Write your own `Model` sub-classes\n\n## Wrap your own Python functions\n\nThis serializes a Python object or class:\n\n```python\nfrom superduper import model\n\n@model\ndef my_model(x, y):\n    return x + y\n```\n\nAdditional arguments may be provided to the decorator from `superduper.components.model.ObjectModel`:\n\n```python\n@model(num_workers=4)\ndef my_model(x, y):\n    return x + y\n```\n\nSimilarly the following snippet saves the source code of a python object instead of serializing the object:\n\n```python\nfrom superduper import code\n\n@code\ndef my_other_model(x, y):\n    return x * y\n```\n\nThese decorators may also be applied to `callable` classes.\nIf your class has important state which should be serialized with the class, \nthen use `model` otherwise you can use `codemodel`:\n\n```python\nfrom superduper import ObjectModel, model\n\n@model\nclass MyClass:\n    def __call__(self, x):\n        return x + 2\n\nm = MyClass()\n\nassert isinstance(m, ObjectModel)\nassert m.predict(2) == 4\n```\n\nAs before, additional arguments can be supplied to the decorator:\n\n```python\nfrom superduper import vector, DataType, model\n\n@model(datatype=vector(shape=(32, )))\nclass MyClass:\n    def __call__(self, x):\n        return [x + 2] * 32\n\nm = MyClass()\n\nassert isinstance(m.datatype, DataType)\n```\n\n## Import classes and functions directly\n\nThis may be more intuitive to some developers, \nand allows functionality to be invoked \"directly\" \nfrom third-party packages:\n\n```python\nfrom superduper.ext.auto.sklearn.svm import SVC\n```\n\n## Create your own `Model` subclasses\n\nDevelopers may create their own `Model` sub-classes, and deploy these directly to `superduper`.\nThe key methods the developers need to create are:\n\n- `predict`\n- Optionally `predict_batches`, to speed up batching\n- Optionally `fit`\n\n### Minimal example with `prediction`\n\nHere is a simple sub-class of `Model`:\n\n```python\nfrom superduper.components.model import Model\nimport typing as t\n\nclass CustomModel(Model):\n    signature: t.ClassVar[str] = '**kwargs'\n    my_argument: int = 1\n\n    def predict(self, x, y):\n        return x + y + self.my_argument\n```\n\nThe addition of `signature = **kwargs` controls how the individual datapoints in the dataset \nare emitted, for consumption by the internal workings of the model\n\n### Including datablobs which can't be converted to JSON\n\nIf your model contains large data-artifacts or non-JSON-able content, then \nthese items should be labelled with [a `DataType`](../apply_api/datatype).\n\nOn saving, this will allow Superduper to encode their values and save the result\nin `db.artifact_store`.\n\nHere is an example which includes a `numpy.array`:\n\n```python\nimport numpy as np\nfrom superduper.ext.numpy import array\n\n\nclass AnotherModel(Model):\n    _artifacts: t.ClassVar[t.Any] = [\n        ('my_array', array)\n    ]\n    signature: t.ClassVar[str] = '**kwargs'\n    my_argument: int = 1\n    my_array: np.ndarray\n\n    def predict(self, x, y):\n        return x + y + self.my_argument + self.my_array\n\nmy_array = numpy.random.randn(100000, 20)\nmy_array_type = array('my_array', shape=my_array.shape, encodable='lazy_artifact')\ndb.apply(my_array_type)\n\nm = AnotherModel(\n    my_argument=2,\n    my_array=my_array,\n    artifacts={'my_array': my_array_type},\n)\n```\n\nWhen `db.apply` is called, `m.my_array` will be converted to `bytes` with `numpy` functionality\nand a reference to these `bytes` will be saved in the `db.metadata`.\nIn principle any `DataType` can be used to encode such an object.\n",
  "# Deleting data\n\n:::note\nThis functionality is only supported for MongoDB `db.databackend` implementations.\nFor SQL databases, users should drop unwanted tables or use native clients\nto delete data.\n:::\n\nDelete queries follow exactly the same [pattern as insert queries](./basic_insertion). For example:\n\n```python\ndeleted_ids, jobs = db.execute(my_collection.delete_many({}))\n```",
  "# Setting up vector-search\n\nSetting up vector-search involves \"applying\" a `VectorIndex` component to the datalayer `db`.\nRead [here](../apply_api/vector_index.md) for information about how to do this.",
  "# Execute API\n\nSuperduper implements 2 main classes of `db.databackend`:\n\n- [MongoDB](../data_integrations/mongodb)\n- [SQL backends](../data_integrations/sql)\n\nCorrespondingly, Superduper currently has 2 flavours of query API:\n\n- [`pymongo`](https://pymongo.readthedocs.io/en/stable/)\n- [`ibis`](https://ibis-project.org/)\n\n## Base\n\nA few commands are shared in common by all supported databackends:\n\n- `db[\"table_name\"].insert(data)`\n- `db[\"table_name\"].select()`\n\nFor more specific commands, one should use one of the two following APIs.\n\n## PyMongo\n\n`pymongo` is the official MongoDB client for Python. It supports \ncompositional queries, leveraging the BSON format for encoding \nand retrieving data.\n\n## Ibis\n\n`ibis` is a Python library with a uniform compositional approach to building\nSQL queries.",
  "# Vector-search\n\nSuperduper aims to provide first-class support for \nvector-search, including embedding computation in preparation\nand run-time, as well as executing the fast vector-comparison \nand returning results in a way compatible with standard database\nqueries.\n\n## Read more\n\n```mdx-code-block\nimport DocCardList from '@theme/DocCardList';\n\n<DocCardList />\n```\n",
  "# Selecting data\n\nSelecting data involves building a compositional query \nstaring with a table of collection, and repeatingly calling\nmethods to build a complex query:\n\n```python\nq = db['table_name'].method_1(*args_1, **kwargs_1).method_2(*args_2, **kwargs_2)....\n```\n\nAs usual, the query is executed with:\n\n```\nq.execute()\n```\n\n## Read more\n\n```mdx-code-block\nimport DocCardList from '@theme/DocCardList';\n\n<DocCardList />\n```",
  "# Updating data\n\n:::note\nThis functionality is only supported for MongoDB `db.databackend` implementations\n:::\n\nUpdate queries follow exactly the same [pattern as insert queries](./basic_insertion). For example:\n\n```python\nupdated_ids, jobs = db.execute(my_collection.update_many({}, {'$set': {'brand': 'Adidas'}}))\n```",
  "# Vector search queries\n\nVector search queries are built with the `.like` operator.\nThis allows developers to combine standard database with vector-search queries.\nThe philosophy is that developers do not need to convert their inputs \ninto vector's themselves. Rather, this is taken care by the specified \n[`VectorIndex` component](../apply_api/vector_index).\n\nThe basic schematic for vector-search queries is:\n\n```python\ntable_or_collection\n    .like(Document(<dict-to-search-with>), vector_index='<my-vector-index>')      # the operand is vectorized using registered models\n    .filter_results(*args, **kwargs)            # the results of vector-search are filtered\n```\n\n***or...***\n\n```python\ntable_or_collection\n    .filter_results(*args, **kwargs)            # the results of vector-search are filtered\n    .like(Document(<dict-to-search-with>),\n          vector_index='<my-vector-index>')      # the operand is vectorized using registered models\n```\n\n## MongoDB\n\n```python\nfrom superduper.ext.pillow import pil_image\nfrom superduper import Document\n\nmy_image = PIL.Image.open('test/material/data/test_image.png')\n\nq = db['my_collection'].find({'brand': 'Nike'}).like(Document({'img': pil_image(my_image)}), \n                                               vector_index='<my-vector-index>')\n\nresults = q.execute()\n```\n\n## SQL\n\n```python\nt = db['my_table']\nt.filter(t.brand == 'Nike').like(Document({'img': pil_image(my_image)}))\n\nresults = db.execute(q)\n```\n\n",
  "# Basic insertion\n\nSuperduper supports inserting data wrapped as dictionaries in Python.\nThese dictionaries may contain basic JSON-compatible data, but also \nother data-types to be handled with `DataType` components. All data inserts are wrapped with the `Document` wrapper:\n\n```python\ndata = ... # an iterable of dictionaries\n```\n\nFor example, first get some [sample data](../reusable_snippets/get_useful_sample_data.md):\n\n```bash\n!curl -O https://superduper-public-demo.s3.amazonaws.com/text.json\n```\n\nThen load the data:\n\n```python\nwith open('./text.json') as f:\n    data = json.load(f)\n```\n\n## Usage pattern\n\n```python\nids, jobs = db['collection-name'].insert(data).execute()\n```\n\n## MongoDB\n\n```python\nids, jobs = db['collection-name'].insert_many(data).execute()\n```\n\nA `Schema` which differs from the standard `Schema` used by `\"collection-name\"` may \nbe used with:\n\n```python\nids, jobs = db['collection-name'].insert_many(data).execute(schema=schema_component)\n```\n\nRead about this here `Schema` [here](../apply_api/schema.md).\n\n## SQL\n\n```python\nids, jobs = db['table-name'].insert(data)\n```\nIf no `Schema` has been set-up for `table-name\"` a `Schema` is auto-inferred.\nData not handled by the `db.databackend` is encoded by default with `pickle`.\n\n## Monitoring jobs\n\nThe second output of this command gives a reference to the job computations \nwhich are triggered by the `Component` instances already applied with `db.apply(...)`.\n\nIf users have configured a `ray` cluster, the jobs may be monitored at the \nfollowing uri:\n\n```python\nfrom superduper import CFG\n\nprint(CFG.cluster.compute.uri)\n```",
  "# MongoDB select queries\n\nSuperduper supports the `pymongo` query API to build select queries.\nThere is one slight difference however, since queries built with `pymongo`'s formalism\nare executed lazily:\n\nWhereas in `pymongo` one might write:\n\n```python\nclient.my_db.my_collection.find_one()\n```\n\nwith `superduper` one would write:\n\n```python\nresult = db['my_collection'].find_one().execute()\n```\n",
  "# Data inserts\n\nSuperduper allows developers to insert data from a variety of sources, \nencoding and decoding objects, such as images and videos, not usually handled \nexplicitly by the `db.databackend`.\n\n## Read more\n\n```mdx-code-block\nimport DocCardList from '@theme/DocCardList';\n\n<DocCardList />\n```",
  "---\nsidebar_position: 15\n---\n\n# Working with external data sources\n\nSuperduper supports data added from external data-sources.\nWhen doing this, Superduper supports:\n\n- web URLs\n- URIs of objects in `s3` buckets\n\nThe trick is to pass the `uri` parameter to an encoder, instead of the raw-data.\nHere is an example where we add a `.pdf` file directly from a location \non the public internet.\n\n```python\nimport io\nfrom PyPDF2 import PdfReader\n\ndef load_pdf(bytes):\n    text = []\n    for page in PdfReader(io.BytesIO(bytes)).pages:\n        text.append(page.extract_text())\n    return '\\n----NEW-PAGE----\\n'.join(text)\n\n# no `encoder=...` parameter required since text is not converted to `.pdf` format\npdf_enc = Encoder('my-pdf-encoder', decoder=load_pdf)\n\nPDF_URI = (\n    'https://papers.nips.cc/paper_files/paper/2012/file/'\n    'c399862d3b9d6b76c8436e924a68c45b-Paper.pdf'\n)\n\n# This command inserts a record which refers to this URI\n# and also downloads the content from the URI and saves\n# it in the record\ndb['pdf-files'].insert_one(Document({'txt': pdf_enc(uri=PDF_URI)})).execute()\n```\n\nNow when the data is loaded from the database, it is loaded as text:\n\n```python\n>>> r = collection.find_one().execute()\n>>> print(r['txt'])\n```",
  "# Using a database's native vector search\n\nDatabases increasingly include their own vector-comparison and search operations \n(comparing one vector with others). In order to use this, include \nthis configuration in your configuration setup:\n\n```yaml\ncluster:\n  vector_search:\n    type: native\n```\n\n***or***\n\n```bash\nexport SUPERDUPER_CLUSTER_VECTOR_SEARCH_TYPE=native\n```\n\nIf `superduper` detects this configuration, it uses the inbuilt mechanism \nof your `db.databackend` to perform the vector-comparison.\n\nCurrently Superduper supports the native implementation of these databases:\n\n- MongoDB Atlas\n\nMore integrations are on the way.",
  "# (Optional) Setting up tables and encodings\n\nSuperduper has flexible support for data-types. In both MongoDB and SQL databases,\none can uses `superduper.DataType` to define one's own data-types.\n\nIf no-datatypes are provided, `superduper` [uses fallbacks](./auto_data_types.md) to encode and decode data.\nTo gain more-control, developers may use the `DataType` and `Schema` components.\n\n## `DataType` abstraction\n\nThe `DataType` class requires two functions which allow the user to go to-and-from `bytes`.\nHere is an `DataType` which encodes `numpy.ndarray` instances to `bytes`:\n\n```python\nimport numpy\nfrom superduper import DataType\n\nmy_array = DataType(\n    'my-array',\n    encoder=lambda x: memoryview(x).tobytes(),\n    decode=lambda x: numpy.frombuffer(x),\n)\n```\n\nHere's a more interesting `DataType` which encoders audio from `numpy.array` format to `.wav` file `bytes`:\n\n```python\nimport librosa\nimport io\nimport soundfile\n\ndef decoder(x):\n    buffer = io.BytesIO(x)\n    return librosa.load(buffer)\n\ndef encoder(x):\n    buffer = io.BytesIO()\n    soundfile.write(buffer)\n    return buffer.getvalue()\n\naudio = DataType('audio', encoder=encoder, decoder=decoder)\n```\n\nIt's completely open to the user how exactly the `encoder` and `decoder` arguments are set.\n\nYou may include these `DataType` instances in models, data-inserts and more. You can also directly \nregister the `DataType` instances in the system, using:\n\n```python\ndb.apply(my_array)\ndb.apply(audio)\n```\n\nTo reload (for instance in another session) do:\n\n```python\nmy_array_reloaded = db.load('datatype', 'my_array')\naudio_reloaded = db.load('datatype', 'audio')\n```\n\n:::tip\nMany of the `superduper` extensions come with their own pre-built `DataType` instances.\nFor example:\n\n- `superduper.ext.pillow.pil_image`\n- `superduper.ext.numpy.array`\n- `superduper.ext.torch.tensor`\n:::\n\nRead more about `DataType` [here](../apply_api/datatype).\n\n## Create a `Schema`\n\nThe `Schema` component wraps several columns of standard data or `DataType` encoded data; it \nmay be used with MongoDB and SQL databases, but is only necessary for SQL.\n\nHere is a `Schema` with three columns, one of the columns is a standard data-type \"str\".\nThe other 2 are given by the `DataType` instances defined above.\n\n```python\nfrom superduper import Schema\nfrom superduper.ext.pillow import pil_image\n\nmy_schema = Schema(\n    'my-schema',\n    fields={'txt': 'str', 'audio': audio, 'img': pil_image}\n)\n\n# save this for later use\ndb.apply(my_schema)\n```\n\n### Create a table with a `Schema`\n\nIf a `Table` is created with a `Schema`, all data inserted to this \ntable will use that `Schema`.\n\n```python\nfrom superduper import Table\n\ndb.apply(Table('my-table', schema=my_schema))\n```\n\nIn MongoDB this `Table` refers to a MongoDB collection, otherwise\nto an SQL table.\n\nThen when data is inserted, it will use this `my_schema` object:\n\n```python\ndb['my-table'].insert[_many](data).execute()\n```",
  "# Predictions\n\nModel predictions may be deployed by calling `Model.predict_batches` or `Model.predict` directly.\n\n```python\nm = db.load('model', 'my-model')\n\n# *args, **kwargs depend on model implementation\nresults = m.predict(*args, **kwargs)\n```\n\nAn alternative is to construct a prediction \"query\" as follows:\n\n```python\n# *args, **kwargs depend on model implementation\ndb['my-model'].predict(*args, **kwargs).execute()\n```\n\nThe results should be the same for both versions.\n",
  "# Automatic data-types\n\nA major challenge in uniting classical databases with AI, \nis that the types of data used in AI are often not supported by your database.\n\nTo solve this problem, `superduper` has the abstractions [`DataType`](../apply_api/datatype.md) and [`Schema`](../apply_api/schema.md).\n\nTo save developers time, by default, `superduper` recognizes the type of data and constructs a `Schema` based on this inference.\nTo learn more about setting these up manually read [the following page](./data_encodings_and_schemas.md).\n\n## Basic usage\n\nTo learn about this feature, try these lines of code, based on sample image data we've prepared.\n\n```bash\ncurl -O https://superduper-public-demo.s3.amazonaws.com/images.zip && unzip images.zip\n```\n\n```python\nimport os\nimport PIL.Image\n\nfrom superduper import superduper\n\ndb = superduper('mongomock://test')\n\nimages = [PIL.Image.open(f'images/{x}') for x in os.listdir('images') if x.endswith('.png')]\n\n# inserts the images into `db` recognizing datatypes automatically\ndb['images'].insert_many([{'img': img} for img in images]).execute()\n```\n\nNow if you inspect which components are available, you will see that 2 components have been added to \nthe system:\n\n```python\ndb.show()\n```\n\n<details>\n    <summary>Outputs</summary>\n    <pre>\n        ```\n        [{'identifier': 'pil_image', 'type_id': 'datatype'},\n         {'identifier': 'AUTO:img=pil_image', 'type_id': 'schema'}]\n        ```\n    </pre>\n</details>\n\nTo verify that the data types were correctly inferred, we can retrieve a single record.\nThe record is a `Document` which wraps a dictionary with important information:\n\n```python\nr = db['images'].find_one().execute()\nr\n```\n\n<details>\n    <summary>Outputs</summary>\n    <pre>\n        ```\n        Document({'img': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=500x338 at 0x128394190>, '_fold': 'train', '_schema': 'AUTO:img=pil_image', '_id': ObjectId('6658610912e50a99219ba587')})\n        ```\n    </pre>\n</details>\n\n\nBy calling the `.unpack()` method, the original data is decoded and unwrapped from the `Document`.\nThe result in this case is a Python `pillow` image, which may be used as direct input \nto functions from, for instance, `torchvision` or `transformers`.\n\n```python\nr.unpack()['img']\n```\n\n<details>\n    <summary>Outputs</summary>\n    <div>\n        ![](/listening/31_0.png)\n    </div>\n</details>",
  "---\nsidebar_position: 16\n---\n\n# Working with and inserting large pieces of data\n\nSome applications require large data-blobs and objects, which are either larger than the objects which are supported by the underlying database, or which will degrade performance of the database if stored directly. For example:\n\n- large images\n- large audio\n- videos\n\nIn order to handle such data, Superduper provides a few options when \ncreating a `DataType` via the `encodable` parameter.\n\n## Artifact store reference with `encodable='artifact'`\n\nWhen creating a `DataType` with `encodable='artifact'`, \nthe data encoded by the `DataType` is saved to the `db.artifact_store` \nand a reference in saved in the `db.databackend`\n\nFor example, if you try the following snippet:\n\n```python\nimport pickle\nimport uuid\nfrom superduper import DataType, Document, superduper, Table, Schema\n\ndb = superduper('mongomock://test', artifact_store='filesystem://./artifacts')\n\ndt = DataType(\n    'my-artifact',\n    encoder=lambda x, info: pickle.dumps(x),\n    decoder=lambda x, info: pickle.loads(x),\n    encodable='artifact',\n)\n\nschema = Schema(identifier='schema', fields={'x': dt})\ntable = Table('my_collection', schema=schema)\n\ndb.apply(table)\n\nmy_id = str(uuid.uuid4())\n\ndb['my_collection'].insert_one(Document({'id': my_id, 'x': 'This is a test'})).execute()\n```\n\nIf you now reload the data with this query:\n\n```python\n>>> r = db.execute(db['my_collection'].find_one({'id': my_id}))\n>>> r\nDocument({'id': 'a9a01284-f391-4aaa-9391-318fc38303bb', 'x': 'This is a test', '_fold': 'train', '_id': ObjectId('669fae8ccdaeae826dec4784')})\n```\n\nYou will see that `r['x']` is exactly `'This is a test'`, however, \nwith a native MongoDB query, you will find the data for `'x'` missing:\n\n```python\n>>> db.databackend.conn.test.my_collection.find_one() \n{'id': 'a9a01284-f391-4aaa-9391-318fc38303bb',\n 'x': '&:blob:866cf8526595d3620d6045172fb16d1efefac4b1',\n '_fold': 'train',\n '_schema': 'schema',\n '_builds': {},\n '_files': {},\n '_blobs': {},\n '_id': ObjectId('669fae8ccdaeae826dec4784')}\n```\n\nThis is because the data is stored in the filesystem/ artifact store `./artifacts`.\nYou may verify that with this command:\n\n```bash\niconv -f ISO-8859-1 -t UTF-8 artifacts/866cf8526595d3620d6045172fb16d1efefac4b1\n```\n\nThe Superduper query reloads the data and passes it to the query result, \nwithout any user intervention.\n\n## Just-in-time loading with `encodable='lazy_artifact'`:\n\nIf you specify `encodable='lazy_artifact'`, then the data \nis only loaded when a user calls the `.unpack()` method.\nThis can be useful if the datapoints are very large, \nand should only be loaded when absolutely necessary.\n\nTry replacing the creation of `dt` with this command:\n\n```python\ndt = DataType(\n    'my-artifact',\n    encoder=lambda x, info: pickle.dumps(x),\n    decoder=lambda x, info: pickle.loads(x),\n    encodable='lazy_artifact',\n)\n```\n\nand then execute the same lines as before.\nYou will find that:\n\n```python\n>>> r = db.execute(my_collection.find_one({'id': my_id}))\n>>> r\nDocument({'id': 'b2a248c7-e023-4cba-9ac9-fdc92fa77ae3', 'x': LazyArtifact(identifier='', uuid='c0db12ad-2684-4e39-a2ba-2748bd20b193', datatype=DataType(identifier='my-artifact', uuid='6d72b346-b5ec-4d8b-8cba-cddec86937a3', upstream=None, plugins=None, encoder=<function <lambda> at 0x125e33760>, decoder=<function <lambda> at 0x125c4e320>, info=None, shape=None, directory=None, encodable='lazy_artifact', bytes_encoding='Bytes', intermediate_type='bytes', media_type=None), uri=None, x=<EMPTY>), '_fold': 'train', '_id': ObjectId('669faf9dcdaeae826dec4789')})\n>>> r['x'].x\n<EMPTY>\n```\n\nHowever, after calling `.unpack(db)`:\n\n```python\n>>> r = r.unpack()\n>>> r['x']\n'This is a test'\n```\n\nThis allows `superduper` to build efficient data-loaders and model loading mechanisms.\nFor example, when saving model data to the artifact-store, the default `encodable` is `'lazy_artifact'`.\n\n## Saving files and directories to the artifact store\n\nThere is an additional mechanism for working with large files. This works \nbetter in certain contexts, such as flexibly saving the results of model training.\nThe following lines copy the file to the `db.artifact_store`.\nWhen data is loaded, the data is copied back over from the artifact-store to \nthe local file-system:\n\n```bash\ncp -r test test_copy\n```\n\n```python\nschema = Schema(identifier='schema', fields={'x': dt})\ntable = Table('my_collection', schema=schema)\n\ndb.apply(table)\nmy_id = str(uuid.uuid4())\ndb.execute(db['my_collection'].insert_one(Document({'id': my_id, 'x': './test_copy'})))\n```\n\nWhen reloading data, you will see that only a reference to the data in the artifact-store\nis loaded:\n\n```python\n>>> db.execute(db['my_collection'].find_one({'id': my_id})).unpack()\n{'id': '93eaae04-a48b-4632-94cf-123cdb2c9517',\n 'x': './artifacts/d537309c8e5be28f91b90b97bbb229984935ba4a/test_copy',\n '_fold': 'train',\n '_id': ObjectId('669fb091cdaeae826dec4797')}\n\n```\n\nDownstream `Model` instances may then explicitly handle the local file from the file \nreference.",
  "# Sidecar vector-comparison integration\n\nFor databases which don't have their own vector-search implementation, Superduper offers \n2 integrations:\n\n- In memory vector-search\n- Lance vector-search\n\nTo configure these, add one of the following options to your configuration:\n\n```yaml\ncluster:\n  vector_search:\n    type: in_memory|lance\n```\n\n***or***\n\n```bash\nexport SUPERDUPER_CLUSTER_VECTOR_SEARCH_TYPE='in_memory|lance'\n```\n\nIn this case, whenever a developer executes a vector-search query including `.like`, \nexecution of the similarity and sorting computations of vectors is outsourced to \na sidecar implementation which is managed by `superduper`.",
  "# SQL select queries\n\nIn order to support as many data-backends as possible, Superduper supports the `ibis` query API to build SQL queries.\n\nWith Superduper one would write:\n\n```python\nt = db['my_table']\nresult = t.filter(t.brand == 'Nike').execute()\n```\n",
  "---\nsidebar_position: 1\n---\n\n# First steps\n\nFollow these steps to get started with Superduper:\n\n1. **Get setup**\n     \n    1. [Install Superduper](./installation.md)\n    2. [Configure Superduper](./configuration.md)\n    3. [Try some of the basic tutorials](../tutorials/intro.md)\n\n2. **Try some of our code-snippets and use-cases**\n\n    1. The [code-snippets](../code_snippets) section provides building blocks to create functionality with Superduper\n    2. The [use-cases](../use_cases) section gives selective examples of how to build complex functionality with Superduper.\n\n3. **Read more about usage-patterns and key functionality of Superduper**\n\n    The API reference ([overview](../core_api/), [connect](../connect_api/), [apply](../apply_api/), [execute](../execute_api/)) gives\n    the central usage patterns and their purpose.\n\n4. **Read about production features**\n\n    Superduper was designed with production specifically in mind. Read more about this [here](../production/).\n\n5. **Build understanding**\n\n    To understand more about how Superduper works, read through the [`Fundamentals`](../fundamentals/glossary) and refer to the [`API References`](https://docs.superduper.io/apidocs/source/superduper.html) for detailed information on API usage.\n\n6. **Create your own Superduper components**\n\n    Superduper makes it easy to write your own models and functionality, with its `Model` base class. Learn how to write\n    a custom `Model` [here](../create_functionality).\n\n7. **Engage with the Community**\n\n    If you encounter challenges, join our [Slack Channels](https://join.slack.com/t/superduper/shared_invite/zt-1zuojj0k0-RjAYBs1TDsvEa7yaFGa6QA) for assistance. Report bugs and share feature requests [by raising an issue]((https://github.com/superduper/superduper/issues).). Our community is here to support you.\n\n    You are welcome to join the conversation on our [discussions forum](https://github.com/superduper/superduper/discussions) and follow our open-source roadmap [here](https://github.com/orgs/superduper/projects/1/views/10).\n\n8. **Contribute and Share**\n\n    Contribute to the Superduper community by sharing your solutions and experiences. \n    Help us grow by promoting Superduper to your peers and the wider world. Your involvement is valuable to us! Don't forget to give us a star ⭐!\n\n    Superduper is a community effort and licensed under Apache 2.0. We encourage enthusiastic developers to contribute to the project. Read more [here](../setup/contributing) and [on GitHub](https://github.com/superduper/superduper/) about getting setup and ways you can contribute.\n\n\nimport DocCardList from '@theme/DocCardList';\n\n<DocCardList />",
  "---\nsidebar_position: 3\ntags:\n  - quickstart\n---\n\n# Configure\n\nSuperduper provides a range of configurable options for setting\nup your environment:\n\nConfigurations can either be injected:\n\n- in a YAML file specified by the `SUPERDUPER_CONFIG_FILE` environment variable or\n- through environment variables starting with `SUPERDUPER_`:\n- as `**kwargs` when calling the [`superduper.superduper`](../core_api/connect.md) function (note this is only for development purposes).\n\nHere are the configurable settings and their project defaults \n(remaining configurations can be viewed in [`superduper.base.config`](https://github.com/superduper/superduper/blob/main/superduper/base/config.py)). Note that as much or as little of this configuration can be specified. The remaining \nconfigurations will then take on their default values.\n\n\n```yaml\n# Where large data blobs/ files are saved\nartifact_store: filesystem://<path-to-artifact-store>\n\n# How to encode binary data\nbytes_encoding: Bytes\n# bytes_encoding: Base64\n\n# The base database you would like to connect to\ndata_backend: <databackend-uri>\n\n# Settings for randomly assigning train/valid folds\nfold_probability: 0.05\n\n# Where lance indexes will be saved\nlance_home: .superduper/vector_indices\n\n# Log level to be shown to stdout\nlog_level: INFO\nlogging_type: SYSTEM\n\n# Database to save meta-data in (defaults to `data_backend`)\nmetadata_store: null\n\n# Settings for failed API requests\nretries:\n  stop_after_attempt: 2\n  wait_max: 10.0\n  wait_min: 4.0\n  wait_multiplier: 1.0\n```\n\nAs an example, to reconfigure the URI of the data_backend we have two options:\n\nAn environment variable set with:\n\n```bash\n$ export SUPERDUPER_CONFIG=./config.yaml\n```\nAnd a configuration file in `./config.yaml` with this content only:\n\n```yaml\ndata_backend: mongodb://localhost:27018/documents\n```\n\n... or simply set environment variables directly:\n\n```bash\n$ export SUPERDUPER_DATA_BACKEND='mongodb://localhost:27018/documents'\n```\n\nYou may view the configuration used by the system with:\n\n```bash\npython -m superduper config\n```\n",
  "---\nsidebar_position: 2\n---\n\n# Installation\n\nThere are two ways to get started:\n\n- [A local `pip` installation on your system](#pip).\n- [Running the `superduper` docker image](#docker-image).\n\n## Pip\n\n`superduper` is available on [PyPi.org](https://pypi.org/project/superduper-framework/).\n\n### Prerequisites\n\nBefore you begin the installation process, please make sure you have the following prerequisites in place:\n\n#### Operating System\n\n`superduper` is compatible with several Linux distributions, including:\n\n- MacOS\n- Ubuntu\n- Debian\n\n#### Python Ecosystem\n\nIf you plan to install superduper from source, you'll need the following:\n\n- `python3.10` or `python3.11`\n- `pip` 22.0.4 or later\n\nYour experience with `superduper` on Linux may vary depending on your system and compute requirements.\n\n### Installation\n\nTo install `superduper` on your system using `pip`, open your terminal and run the following command:\n\n```bash\npip install superduper-framework\n```\n\nThis command will install `superduper` along with a minimal set of common dependencies required for running the framework.\nIf you would like to use the `superduper.ext` subpackages (e.g. `superduper.ext.openai`), you may build a requirements file\nwith this command:\n\n```bash\npython3 -m superduper requirements <list-of-extensions> > requirements.txt\n```\n\nFor example, this builds a `requirements.txt` file for `openai` and `torch`:\n\n```bash\npython3 -m superduper requirements openai torch > requirements.txt\n```\n\n## Docker Image\n\n#### Using Pre-built Images\n\nIf you prefer using Docker, you can pull a pre-built Docker image from Docker Hub and run it with Docker version 19.03 or later:\n\n```bash\ndocker run -p 8888:8888 superduperio/superduper:latest\n```\n\nThis command installs the base `superduper` image. If you want to run the ready-to-use examples, you'll need to download the required  dependencies at runtime. \n\n\n#### Building the image yourself\n\nFor more control, you can build the Docker images yourself from the latest GitHub version as follows:\n\nClone the code:\n\n```bash\ngit clone --branch main --single-branch --depth 1 https://github.com/superduper/superduper.git\nmake build_superduper\n```\n\n#### Developer's docker image and environment\n\nIf you wish to use your local copy of code with a docker build, execute the following command:\n\n```bash\nmake build_sandbox\n```\n\nYou will see something like these lines in `docker images`:\n\n```bash\nREPOSITORY                    TAG             IMAGE ID       CREATED        SIZE\nsuperduperio/sandbox          latest          88ddab334d17   5 days ago     2.69GB\n```",
  "# Production features in Superduper\n\nSuperduper was made with productionization in mind. These means several things:\n\n## Modifiable, configurable, flexible\n\nA production ready solution should not come up short, as soon as developers \nencounted a use-case or scenario outside of the norm, or the documented \napplications. In this respect, modifiablility, configurability and flexibility are key.\n\nSuperduper contains fully open-source (Apache 2.0, MIT, BSD 3-part) components for all aspects of the setup.\nThis goes from the AI-models integrated, the databases and client libraries supported, as well as \nthe cluster and compute management. This means that developers are never left hung out \nto dry with regard to taking action in altering and interrogating the source doe, as well \nas adding their own functionality.\n\nIn addition, Superduper may be used and configured in a wide variety of ways.\nIt can be used \"in-process\", with computations blocking (\"developer mode\") and \nit can be operated in a cluster-like architecture, with compute, vector-search,\nchange-data capture and a REST-ful server separated into separate services.\nThis is ideal for teams of developers looking to productionize their AI-data setups.\n\n## Scalability\n\nA production ready solution should scale with the amount of traffic, data\nand computation to the system. Superduper includes a `ray` integration\nwhich allows developers to scale the compute as the amount of data and requests\nto the system grod. Read more [here](./non_blocking_ray_jobs).\n\nIn addition Superduper has the option to separate the vector-comparison and sorting component\ninto a separate service, so this doesn't block or slow down the main program running.\n\n## Interoperability\n\nDue to the [change-data-capture component](./change_data_capture), developers \nare not required to operate their database through Superduper. Third-party \ndatabase clients, and even other programming languages other than Python \nmay be used to add data to the database. Nonetheless, Superduper \nwill still process this data.\n\nIn addition the [REST API](./rest_api) may be easily used to access Superduper\nfrom the web, or from other programming environments.\n\n## Live serving\n\nThe [REST API](./rest_api) service may be used to access Superduper using pure JSON, \nplus references to saved/ uploaded binaries. This gives great flexibility to application\ndevelopers looking to build on top of Superduper from Javascript etc..\n\n## SuperDuper protocol\n\nAll Superduper components may be built using Python, or directly in a YAML/ JSON formalism\nusng the [\"superduper-protocol\"](./superduper_protocol.md).\nThis provides a convenient set of choices for AI engineers, infrastructure engineers \nand beyond to share and communicate their AI-data setups in Superduper",
  "# SuperDuper Protocol\n\nSuperduper includes a protocol allowed developers to switch back and forth from Python and YAML/ JSON formats.\nThe mapping is fairly self-explanatory after reading the examples below.\n\n## Writing in Superduper-protocol directly\n\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n<Tabs>\n    <TabItem value=\"YAML\" label=\"YAML\" default>\n\n        ```yaml\n        _base: \"?my_vector_index\"\n        _leaves:\n          postprocess:\n            _path: superduper/base/code/Code\n            code: '\n              from superduper import code\n\n              @code\n              def postprocess(x):\n                  return x.tolist()\n              '\n          my_vector:\n            _path: superduper.components/vector_index/vector\n            shape: 384\n          sentence_transformer:\n            _path: superduper/ext/sentence_transformers/model/SentenceTransformer\n            datatype: \"?my_vector\"\n            model: \"all-MiniLM-L6-v2\"\n            postprocess: \"?postprocess\"\n          my_query:\n            _path: superduper/backends/mongodb/query/parse_query\n            query: \"documents.find()\"\n          my_listener:\n            _path: superduper.components/listener/Listener\n            model: \"?sentence_transformer\"\n            select: \"?my_query\"\n            key: \"X\"\n          my_vector_index:\n            _path: superduper.components/vector_index/VectorIndex\n            indexing_listener: \"?my_listener\"\n            measure: cosine\n        ```\n\n        Then from the commmand line:\n\n        ```bash\n        superduper apply --manifest='<path_to_config>.yaml'\n        ```\n\n    </TabItem>\n    <TabItem value=\"JSON\" label=\"JSON\" default>\n\n        ```json\n        {\n          \"_base\": \"?my_vector_index\",\n          \"_leaves\": {\n            \"postprocess\": {\n              \"_path\": \"superduper/base/code/Code\",\n              \"code\": \"from superduper import code\\n\\n@code\\ndef postprocess(x):\\n    return x.tolist()\"\n            },\n            \"my_vector\": {\n              \"_path\": \"superduper.components/vector_index/vector\",\n              \"shape\": 384\n            },\n            \"sentence_transformer\": {\n              \"_path\": \"superduper/ext/sentence_transformers/model/SentenceTransformer\",\n              \"datatype\": \"?my_vector\",\n              \"model\": \"all-MiniLM-L6-v2\",\n              \"postprocess\": \"?postprocess\"\n            },\n            \"my_query\": {\n              \"_path\": \"superduper/backends/mongodb/query/parse_query\",\n              \"query\": \"documents.find()\"\n            },\n            \"my_listener\": {\n              \"_path\": \"superduper.components/listener/Listener\",\n              \"model\": \"?sentence_transformer\",\n              \"select\": \"?my_query\"\n            },\n            \"my_vector_index\": {\n              \"_path\": \"superduper.components/vector_index/VectorIndex\",\n              \"indexing_listener\": \"?my_listener\",\n              \"measure\": \"cosine\"\n            }\n          }\n        }\n        ```\n\n        Then from the command line:\n\n        ```bash\n        superduper apply --manifest='<path_to_config>.json'\n        ```\n\n    </TabItem>\n    <TabItem value=\"Python\" label=\"Python\" default>\n\n        ```python\n        from superduper import superduper\n        from superduper.components.vector_index import vector\n        from superduper.ext.sentence_transformers.model import SentenceTransformer\n        from superduper.components.listener import Listener\n        from superduper.components.vector_index import VectorIndex\n        from superduper.base.code import Code\n        from superduper import Stack\n\n\n        db = superduper('mongomock://')\n\n        datatype = vector(shape=384, identifier=\"my-vec\")\n\n\n        def postprocess(x):\n            return x.tolist()\n\n\n        postprocess = Code.from_object(postprocess)\n\n\n        model = SentenceTransformer(\n            identifier=\"test\",\n            datatype=datatype,\n            predict_kwargs={\"show_progress_bar\": True},\n            signature=\"*args,**kwargs\",\n            model=\"all-MiniLM-L6-v2\",\n            device=\"cpu\",\n            postprocess=postprocess,\n        )\n\n        listener = Listener(\n            identifier=\"my-listener\",\n            key=\"txt\",\n            model=model,\n            select=db['documents'].find(),\n            active=True,\n            predict_kwargs={}\n        )\n\n        vector_index = VectorIndex(\n            identifier=\"my-index\",\n            indexing_listener=listener,\n            measure=\"cosine\"\n        )\n\n        db.apply(vector_index)\n        ```\n      \n    </TabItem>\n</Tabs>\n\n## Converting a `Component` to Superduper-protocol\n\nAll components may be converted to *Superduper-protocol* using the `Component.encode` method:\n\n```python\nencoding = vector_index.encode()\n```\n\nThis encoding may be written directly to disk with:\n\n```python\nvector_index.export(zip=True)  # outputs to \"./my-index.zip\"\n```\n\nDevelopers may reload components from disk with `Component.read`\n\n```python\nreloaded = Component.read('./my-index.zip')\n```",
  "**`superduper.misc.special_dicts`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/misc/special_dicts.py)\n\n## `diff` \n\n```python\ndiff(r1,\n     r2)\n```\n| Parameter | Description |\n|-----------|-------------|\n| r1 | Dict |\n| r2 | Dict |\n\nGet the difference between two dictionaries.\n\n```python\n_diff({'a': 1, 'b': 2}, {'a': 2, 'b': 2})\n# {'a': (1, 2)}\n_diff({'a': {'c': 3}, 'b': 2}, {'a': 2, 'b': 2})\n# {'a': ({'c': 3}, 2)}\n```\n\n## `SuperDuperFlatEncode` \n\n```python\nSuperDuperFlatEncode(self,\n     /,\n     *args,\n     **kwargs)\n```\n| Parameter | Description |\n|-----------|-------------|\n| args | *args for `dict` |\n| kwargs | **kwargs for `dict` |\n\nDictionary for representing flattened encoding data.\n\n## `MongoStyleDict` \n\n```python\nMongoStyleDict(self,\n     /,\n     *args,\n     **kwargs)\n```\n| Parameter | Description |\n|-----------|-------------|\n| args | *args for `dict` |\n| kwargs | **kwargs for `dict` |\n\nDictionary object mirroring how MongoDB handles fields.\n\n## `IndexableDict` \n\n```python\nIndexableDict(self,\n     ordered_dict)\n```\n| Parameter | Description |\n|-----------|-------------|\n| ordered_dict | OrderedDict |\n\nIndexableDict.\n\n```python\n# Example:\n# -------\nd = IndexableDict({'a': 1, 'b': 2})\nd[0]\n# 1\n```\n\n```python\nd[1]\n# 2\n```\n\n",
  "**`superduper.misc.hash`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/misc/hash.py)\n\n## `hash_string` \n\n```python\nhash_string(string: str)\n```\n| Parameter | Description |\n|-----------|-------------|\n| string | string to hash |\n\nHash a string.\n\n## `random_sha1` \n\n```python\nrandom_sha1()\n```\nGenerate random sha1 values.\n\n",
  "**`superduper.misc.serialization`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/misc/serialization.py)\n\n## `asdict` \n\n```python\nasdict(obj,\n     *,\n     copy_method=<function copy at 0x100451ee0>) -> Dict[str,\n     Any]\n```\n| Parameter | Description |\n|-----------|-------------|\n| obj | The dataclass instance to |\n| copy_method | The copy method to use for non atomic objects |\n\nConvert the dataclass instance to a dict.\n\nCustom ``asdict`` function which exports a dataclass object into a dict,\nwith a option to choose for nested non atomic objects copy strategy.\n\n",
  "**`superduper.misc.auto_schema`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/misc/auto_schema.py)\n\n## `infer_datatype` \n\n```python\ninfer_datatype(data: Any) -> Union[superduper.components.datatype.DataType,\n     type,\n     NoneType]\n```\n| Parameter | Description |\n|-----------|-------------|\n| data | The data object |\n\nInfer the datatype of a given data object.\n\nIf the data object is a base type, return None,\nOtherwise, return the inferred datatype\n\n## `infer_schema` \n\n```python\ninfer_schema(data: Mapping[str,\n     Any],\n     identifier: Optional[str] = None,\n     ibis=False) -> superduper.components.schema.Schema\n```\n| Parameter | Description |\n|-----------|-------------|\n| data | The data object |\n| identifier | The identifier for the schema, if None, it will be generated |\n| ibis | If True, the schema will be updated for the Ibis backend, otherwise for MongoDB |\n\nInfer a schema from a given data object.\n\n## `register_module` \n\n```python\nregister_module(module_name)\n```\n| Parameter | Description |\n|-----------|-------------|\n| module_name | The module name, e.g. \"superduper.ext.numpy.encoder\" |\n\nRegister a module for datatype inference.\n\nOnly modules with a check and create function will be registered\n\n## `updated_schema_data_for_ibis` \n\n```python\nupdated_schema_data_for_ibis(schema_data) -> Dict[str,\n     superduper.components.datatype.DataType]\n```\n| Parameter | Description |\n|-----------|-------------|\n| schema_data | The schema data |\n\nUpdate the schema data for Ibis backend.\n\nConvert the basic data types to Ibis data types.\n\n## `updated_schema_data_for_mongodb` \n\n```python\nupdated_schema_data_for_mongodb(schema_data) -> Dict[str,\n     superduper.components.datatype.DataType]\n```\n| Parameter | Description |\n|-----------|-------------|\n| schema_data | The schema data |\n\nUpdate the schema data for MongoDB backend.\n\nOnly keep the data types that can be stored directly in MongoDB.\n\n",
  "**`superduper.misc.data`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/misc/data.py)\n\n## `ibatch` \n\n```python\nibatch(iterable: Iterable[~T],\n     batch_size: int) -> Iterator[List[~T]]\n```\n| Parameter | Description |\n|-----------|-------------|\n| iterable | the iterable to batch |\n| batch_size | the number of groups to write |\n\nBatch an iterable into chunks of size `batch_size`.\n\n",
  "**`superduper.misc.runnable.runnable`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/misc/runnable/runnable.py)\n\n## `Event` \n\n```python\nEvent(self,\n     *on_set: Callable[[],\n     NoneType])\n```\n| Parameter | Description |\n|-----------|-------------|\n| on_set | Callbacks to call when the event is set |\n\nAn Event that calls a list of callbacks when set or cleared.\n\nA threading.Event that also calls back to zero or more functions when its state\nis set or reset, and has a __bool__ method.\n\nNote that the callback might happen on some completely different thread,\nso these functions cannot block\n\n## `Runnable` \n\n```python\nRunnable(self)\n```\nA base class for things that start, run, finish, stop and join.\n\nStopping is requesting immediate termination: finishing is saying that\nthere is no more work to be done, finish what you are doing.\n\nA Runnable has two `Event`s, `running` and `stopped`, and you can either\n`wait` on either of these conditions to be true, or add a callback function\n(which must be non-blocking) to either of them.\n\n`running` is not set until the setup for a `Runnable` has finished;\n`stopped` is not set until all the computations in a thread have ceased.\n\nAn Runnable can be used as a context manager:\n\nwith runnable:\n# The runnable is running by this point\ndo_stuff()\n# By the time you get to here, the runnable has completely stopped\n\nThe above means roughly the same as\n\nrunnable.start()\ntry:\ndo_stuff()\nrunnable.finish()\nrunnable.join()\nfinally:\nrunnable.stop()\n\n",
  "**`superduper.misc.runnable.queue_chunker`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/misc/runnable/queue_chunker.py)\n\n## `QueueChunker` \n\n```python\nQueueChunker(self,\n     chunk_size: int,\n     timeout: float,\n     accumulate_timeouts: bool = False) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| chunk_size | Maximum number of entries in a chunk |\n| timeout | Maximum amount of time to block |\n| accumulate_timeouts | If accumulate timeouts is True, then `timeout` is the total timeout allowed over the whole chunk, otherwise the timeout is applied to each item. |\n\nChunk a queue into lists of length at most `chunk_size` within time `timeout`.\n\n",
  "**`superduper.misc.annotations`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/misc/annotations.py)\n\n## `merge_docstrings` \n\n```python\nmerge_docstrings(cls)\n```\n| Parameter | Description |\n|-----------|-------------|\n| cls | Class to merge docstrings for. |\n\nDecorator that merges Sphinx-styled class docstrings.\n\nDecorator merges doc-strings from parent to child classes,\nensuring no duplicate parameters and correct indentation.\n\n## `deprecated` \n\n```python\ndeprecated(f)\n```\n| Parameter | Description |\n|-----------|-------------|\n| f | function to deprecate |\n\nDecorator to mark a function as deprecated.\n\nThis will result in a warning being emitted when the function is used.\n\n## `component` \n\n```python\ncomponent(*schema: Dict)\n```\n| Parameter | Description |\n|-----------|-------------|\n| schema | schema for the component |\n\nDecorator for creating a component.\n\n## `requires_packages` \n\n```python\nrequires_packages(*packages,\n     warn=False)\n```\n| Parameter | Description |\n|-----------|-------------|\n| packages | list of tuples of packages each tuple of the form (import_name, lower_bound/None, upper_bound/None, install_name/None) |\n| warn | if True, warn instead of raising an exception |\n\nRequire the packages to be installed.\n\nE.g. ('sklearn', '0.1.0', '0.2.0', 'scikit-learn')\n\n## `extract_parameters` \n\n```python\nextract_parameters(doc)\n```\n| Parameter | Description |\n|-----------|-------------|\n| doc | Sphinx-styled docstring. Docstring may have multiple lines |\n\nExtracts and organizes parameter descriptions from a Sphinx-styled docstring.\n\n## `replace_parameters` \n\n```python\nreplace_parameters(doc,\n     placeholder: str = '!!!')\n```\n| Parameter | Description |\n|-----------|-------------|\n| doc | Sphinx-styled docstring. |\n| placeholder | Placeholder to replace parameters with. |\n\nReplace parameters in a doc-string with a placeholder.\n\n## `superduperDeprecationWarning` \n\n```python\nsuperduperDeprecationWarning(self,\n     /,\n     *args,\n     **kwargs)\n```\n| Parameter | Description |\n|-----------|-------------|\n| args | *args of `DeprecationWarning` |\n| kwargs | **kwargs of `DeprecationWarning` |\n\nSpecialized Deprecation Warning for fine grained filtering control.\n\n",
  "**`superduper.misc.server`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/misc/server.py)\n\n## `request_server` \n\n```python\nrequest_server(service: str = 'vector_search',\n     data=None,\n     endpoint='add',\n     args={},\n     type='post')\n```\n| Parameter | Description |\n|-----------|-------------|\n| service | Service name |\n| data | Data to send |\n| endpoint | Endpoint to hit |\n| args | Arguments to pass |\n| type | Type of request |\n\nRequest server with data.\n\n## `server_request_decoder` \n\n```python\nserver_request_decoder(x)\n```\n| Parameter | Description |\n|-----------|-------------|\n| x | Object to decode. |\n\nDecodes a request to `SuperDuperApp` service.\n\n",
  "**`superduper.misc.download`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/misc/download.py)\n\n## `download_content` \n\n```python\ndownload_content(db,\n     query: Union[superduper.backends.base.query.Query,\n     Dict],\n     ids: Optional[Sequence[str]] = None,\n     documents: Optional[List[superduper.base.document.Document]] = None,\n     raises: bool = True,\n     n_workers: Optional[int] = None) -> Optional[Sequence[superduper.base.document.Document]]\n```\n| Parameter | Description |\n|-----------|-------------|\n| db | database instance |\n| query | query to be executed |\n| ids | ids to be downloaded |\n| documents | documents to be downloaded |\n| raises | whether to raise errors |\n| n_workers | number of download workers |\n\nDownload content contained in uploaded data.\n\nItems to be downloaded are identifier\nvia the subdocuments in the form exemplified below. By default items are downloaded\nto the database, unless a ``download_update`` function is provided.\n\n```python\nd = {\"_content\": {\"uri\": \"<uri>\", \"encoder\": \"<encoder-identifier>\"}}\ndef update(key, id, bytes):\n... with open(f'/tmp/{key}+{id}', 'wb') as f:\n...     f.write(bytes)\ndownload_content(None, None, ids=[\"0\"], documents=[d]))\n    \n```\n\n## `download_from_one` \n\n```python\ndownload_from_one(r: superduper.base.document.Document)\n```\n| Parameter | Description |\n|-----------|-------------|\n| r | document to download from |\n\nDownload content from a single document.\n\nThis function will find all URIs in the document and download them.\n\n## `gather_uris` \n\n```python\ngather_uris(documents: Sequence[superduper.base.document.Document],\n     gather_ids: bool = True) -> Tuple[List[str],\n     List[str],\n     List[Any],\n     List[str]]\n```\n| Parameter | Description |\n|-----------|-------------|\n| documents | list of dictionaries |\n| gather_ids | if ``True`` then gather ids of documents |\n\nGet the uris out of all documents as denoted by ``{\"_content\": ...}``.\n\n## `timeout` \n\n```python\ntimeout(seconds)\n```\n| Parameter | Description |\n|-----------|-------------|\n| seconds | seconds until timeout |\n\nContext manager to set a timeout.\n\n## `timeout_handler` \n\n```python\ntimeout_handler(signum,\n     frame)\n```\n| Parameter | Description |\n|-----------|-------------|\n| signum | signal number |\n| frame | frame |\n\nTimeout handler to raise an TimeoutException.\n\n## `BaseDownloader` \n\n```python\nBaseDownloader(self,\n     uris: List[str],\n     n_workers: int = 0,\n     timeout: Optional[int] = None,\n     headers: Optional[Dict] = None,\n     raises: bool = True)\n```\n| Parameter | Description |\n|-----------|-------------|\n| uris | list of uris/ file names to fetch |\n| n_workers | number of multiprocessing workers |\n| timeout | set seconds until request times out |\n| headers | dictionary of request headers passed to``requests`` package |\n| raises | raises error ``True``/``False`` |\n\nBase class for downloading files.\n\n## `Downloader` \n\n```python\nDownloader(self,\n     uris,\n     update_one: Optional[Callable] = None,\n     ids: Union[List[str],\n     List[int],\n     NoneType] = None,\n     keys: Optional[List[str]] = None,\n     datatypes: Optional[List[str]] = None,\n     n_workers: int = 20,\n     headers: Optional[Dict] = None,\n     skip_existing: bool = True,\n     timeout: Optional[int] = None,\n     raises: bool = True)\n```\n| Parameter | Description |\n|-----------|-------------|\n| uris | list of uris/ file names to fetch |\n| update_one | function to call to insert data into table |\n| ids | list of ids of rows/ documents to update |\n| keys | list of keys in rows/ documents to insert to |\n| datatypes | list of datatypes of rows/ documents to insert to |\n| n_workers | number of multiprocessing workers |\n| headers | dictionary of request headers passed to``requests`` package |\n| skip_existing | if ``True`` then don't bother getting already present data |\n| timeout | set seconds until request times out |\n| raises | raises error ``True``/``False`` |\n\nDownload files from a list of URIs.\n\n## `Fetcher` \n\n```python\nFetcher(self,\n     headers: Optional[Dict] = None,\n     n_workers: int = 0)\n```\n| Parameter | Description |\n|-----------|-------------|\n| headers | headers to be used for download |\n| n_workers | number of download workers |\n\nFetches data from a URI.\n\n## `TimeoutException` \n\n```python\nTimeoutException(self,\n     /,\n     *args,\n     **kwargs)\n```\n| Parameter | Description |\n|-----------|-------------|\n| args | *args of `Exception` |\n| kwargs | **kwargs of `Exception` |\n\nTimeout exception.\n\n## `Updater` \n\n```python\nUpdater(self,\n     db,\n     query)\n```\n| Parameter | Description |\n|-----------|-------------|\n| db | Datalayer instance |\n| query | query to be executed |\n\nUpdater class to update the artifact.\n\n",
  "**`superduper.cdc.cdc`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/cdc/cdc.py)\n\n## `DatabaseChangeDataCapture` \n\n```python\nDatabaseChangeDataCapture(self,\n     db: 'Datalayer')\n```\n| Parameter | Description |\n|-----------|-------------|\n| db | A superduper datalayer instance. |\n\nDatabaseChangeDataCapture (CDC).\n\nDatabaseChangeDataCapture is a Python class that provides a flexible and\nextensible framework for capturing and managing data changes\nin a database.\n\nThis class is repsonsible for cdc service on the provided `db` instance\nThis class is designed to simplify the process of tracking changes\nto database records,allowing you to monitor and respond to\ndata modifications efficiently.\n\n## `BaseDatabaseListener` \n\n```python\nBaseDatabaseListener(self,\n     db: 'Datalayer',\n     on: Union[ForwardRef('IbisQuery'),\n     ForwardRef('TableOrCollection')],\n     stop_event: superduper.misc.runnable.runnable.Event,\n     identifier: 'str' = '',\n     timeout: Optional[float] = None)\n```\n| Parameter | Description |\n|-----------|-------------|\n| db | A superduper instance. |\n| on | A table or collection on which the listener is invoked. |\n| stop_event | A threading event flag to notify for stoppage. |\n| identifier | A identity given to the listener service. |\n| timeout | A timeout for the listener. |\n\nA Base class which defines basic functions to implement.\n\nThis class is responsible for defining the basic functions\nthat needs to be implemented by the database listener.\n\n## `CDCHandler` \n\n```python\nCDCHandler(self,\n     db: 'Datalayer',\n     stop_event: superduper.misc.runnable.runnable.Event,\n     queue)\n```\n| Parameter | Description |\n|-----------|-------------|\n| db | A superduper instance. |\n| stop_event | A threading event flag to notify for stoppage. |\n| queue | A queue to hold the cdc packets. |\n\nCDCHandler for handling CDC changes.\n\nThis class is responsible for handling the change by executing the taskflow.\nThis class also extends the task graph by adding funcation job node which\ndoes post model executiong jobs, i.e `copy_vectors`.\n\n## `DatabaseListenerFactory` \n\n```python\nDatabaseListenerFactory(self,\n     db_type: str = 'mongodb')\n```\n| Parameter | Description |\n|-----------|-------------|\n| db_type | Database type. |\n\nDatabaseListenerFactory to create listeners for different databases.\n\nThis class is responsible for creating a DatabaseListener instance\nbased on the database type.\n\n## `DatabaseListenerThreadScheduler` \n\n```python\nDatabaseListenerThreadScheduler(self,\n     listener: superduper.cdc.cdc.BaseDatabaseListener,\n     stop_event: superduper.misc.runnable.runnable.Event,\n     start_event: superduper.misc.runnable.runnable.Event) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| listener | A BaseDatabaseListener instance. |\n| stop_event | A threading event flag to notify for stoppage. |\n| start_event | A threading event flag to notify for start. |\n\nDatabaseListenerThreadScheduler to listen to the cdc changes.\n\nThis class is responsible for listening to the cdc changes and\nexecuting the following job.\n\n## `Packet` \n\n```python\nPacket(self,\n     ids: Any,\n     query: Optional[Any] = None,\n     event_type: superduper.cdc.cdc.DBEvent = <DBEvent.insert: 'insert'>) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| ids | Document ids. |\n| query | Query to fetch the document. |\n| event_type | CDC event type. |\n\nPacket to hold the cdc event data.\n\n",
  "**`superduper.backends.sqlalchemy.metadata`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/backends/sqlalchemy/metadata.py)\n\n## `SQLAlchemyMetadata` \n\n```python\nSQLAlchemyMetadata(self,\n     conn: Any,\n     name: Optional[str] = None)\n```\n| Parameter | Description |\n|-----------|-------------|\n| conn | connection to the meta-data store |\n| name | Name to identify DB using the connection |\n\nAbstraction for storing meta-data separately from primary data.\n\n",
  "**`superduper.backends.sqlalchemy.db_helper`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/backends/sqlalchemy/db_helper.py)\n\n## `create_clickhouse_config` \n\n```python\ncreate_clickhouse_config()\n```\nCreate configuration for ClickHouse database.\n\n## `get_db_config` \n\n```python\nget_db_config(dialect)\n```\n| Parameter | Description |\n|-----------|-------------|\n| dialect | The dialect of the database. |\n\nGet the configuration class for the specified dialect.\n\n",
  "**`superduper.backends.mongodb.artifacts`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/backends/mongodb/artifacts.py)\n\n## `upload_folder` \n\n```python\nupload_folder(path,\n     file_id,\n     fs,\n     parent_path='')\n```\n| Parameter | Description |\n|-----------|-------------|\n| path | The path to the folder to upload |\n| file_id | The file_id of the folder |\n| fs | The GridFS object |\n| parent_path | The parent path of the folder |\n\nUpload folder to GridFS.\n\n## `MongoArtifactStore` \n\n```python\nMongoArtifactStore(self,\n     conn,\n     name: str)\n```\n| Parameter | Description |\n|-----------|-------------|\n| conn | MongoDB client connection |\n| name | Name of database to host filesystem |\n\nArtifact store for MongoDB.\n\n",
  "**`superduper.backends.mongodb.metadata`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/backends/mongodb/metadata.py)\n\n## `MongoMetaDataStore` \n\n```python\nMongoMetaDataStore(self,\n     conn: Any,\n     name: Optional[str] = None) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| conn | MongoDB client connection |\n| name | Name of database to host filesystem |\n\nMetadata store for MongoDB.\n\n",
  "**`superduper.backends.mongodb.query`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/backends/mongodb/query.py)\n\n## `DeleteOne` \n\n```python\nDeleteOne(**kwargs)\n```\n| Parameter | Description |\n|-----------|-------------|\n| kwargs | The arguments to pass to the operation. |\n\nDeleteOne operation for MongoDB.\n\n## `InsertOne` \n\n```python\nInsertOne(**kwargs)\n```\n| Parameter | Description |\n|-----------|-------------|\n| kwargs | The arguments to pass to the operation. |\n\nInsertOne operation for MongoDB.\n\n## `ReplaceOne` \n\n```python\nReplaceOne(**kwargs)\n```\n| Parameter | Description |\n|-----------|-------------|\n| kwargs | The arguments to pass to the operation. |\n\nReplaceOne operation for MongoDB.\n\n## `UpdateOne` \n\n```python\nUpdateOne(**kwargs)\n```\n| Parameter | Description |\n|-----------|-------------|\n| kwargs | The arguments to pass to the operation. |\n\nUpdateOne operation for MongoDB.\n\n## `parse_query` \n\n```python\nparse_query(query,\n     documents: Sequence[Dict] = (),\n     db: Optional[ForwardRef('Datalayer')] = None)\n```\n| Parameter | Description |\n|-----------|-------------|\n| query | The query to parse. |\n| documents | The documents to query. |\n| db | The datalayer to use to execute the query. |\n\nParse a string query into a query object.\n\n## `MongoQuery` \n\n```python\nMongoQuery(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     parts: Sequence[Union[Tuple,\n     str]] = ()) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| parts | The parts of the query. |\n\nA query class for MongoDB.\n\nThis class is used to build and execute queries on a MongoDB database.\n\n## `BulkOp` \n\n```python\nBulkOp(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     kwargs: Dict = None) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| kwargs | The arguments to pass to the operation. |\n\nA bulk operation for MongoDB.\n\n## `ChangeStream` \n\n```python\nChangeStream(self,\n     collection: str,\n     args: Sequence = None,\n     kwargs: Dict = None) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| collection | The collection to perform the query on |\n| args | Positional query arguments to ``pymongo.Collection.watch`` |\n| kwargs | Named query arguments to ``pymongo.Collection.watch`` |\n\nChange stream class to watch for changes in specified collection.\n\n",
  "**`superduper.backends.mongodb.data_backend`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/backends/mongodb/data_backend.py)\n\n## `MongoDataBackend` \n\n```python\nMongoDataBackend(self,\n     conn: pymongo.mongo_client.MongoClient,\n     name: str)\n```\n| Parameter | Description |\n|-----------|-------------|\n| conn | MongoDB client connection |\n| name | Name of database to host filesystem |\n\nData backend for MongoDB.\n\n",
  "**`superduper.backends.query_dataset`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/backends/query_dataset.py)\n\n## `query_dataset_factory` \n\n```python\nquery_dataset_factory(**kwargs)\n```\n| Parameter | Description |\n|-----------|-------------|\n| kwargs | Keyword arguments to be passed to the query dataset object. |\n\nCreate a query dataset object.\n\nIf ``data_prefetch`` is set to ``True``, then a ``CachedQueryDataset`` object is\ncreated, otherwise a ``QueryDataset`` object is created.\n\n## `CachedQueryDataset` \n\n```python\nCachedQueryDataset(self,\n     select: superduper.backends.base.query.Query,\n     mapping: Optional[ForwardRef('Mapping')] = None,\n     ids: Optional[List[str]] = None,\n     fold: Optional[str] = 'train',\n     transform: Optional[Callable] = None,\n     db=None,\n     in_memory: bool = True,\n     prefetch_size: int = 100)\n```\n| Parameter | Description |\n|-----------|-------------|\n| select | A select query object which defines the query to be executed. |\n| mapping | A mapping object to be used for the dataset. |\n| ids | A list of ids to be used for the dataset. |\n| fold | The fold to be used for the dataset. |\n| transform | A callable which can be used to transform the dataset. |\n| db | A datalayer instance to be used for the dataset. |\n| in_memory | A boolean flag to indicate if the dataset should be loaded |\n| prefetch_size | The number of documents to prefetch from the database. |\n\nCached Query Dataset for fetching documents from database.\n\nThis class which fetch the document corresponding to the given ``index``.\nThis class prefetches documents from database and stores in the memory.\n\nThis can drastically reduce database read operations and hence reduce the overall\nload on the database.\n\n## `ExpiryCache` \n\n```python\nExpiryCache(self,\n     /,\n     *args,\n     **kwargs)\n```\n| Parameter | Description |\n|-----------|-------------|\n| args | *args for `list` |\n| kwargs | **kwargs for `list` |\n\nExpiry Cache for storing documents.\n\nThe document will be removed from the cache after fetching it from the cache.\n\n## `QueryDataset` \n\n```python\nQueryDataset(self,\n     select: superduper.backends.base.query.Query,\n     mapping: Optional[ForwardRef('Mapping')] = None,\n     ids: Optional[List[str]] = None,\n     fold: Optional[str] = 'train',\n     transform: Optional[Callable] = None,\n     db: Optional[ForwardRef('Datalayer')] = None,\n     in_memory: bool = True)\n```\n| Parameter | Description |\n|-----------|-------------|\n| select | A select query object which defines the query to be executed. |\n| mapping | A mapping object to be used for the dataset. |\n| ids | A list of ids to be used for the dataset. |\n| fold | The fold to be used for the dataset. |\n| transform | A callable which can be used to transform the dataset. |\n| db | A datalayer instance to be used for the dataset. |\n| in_memory | A boolean flag to indicate if the dataset should be loaded in memory. |\n\nQuery Dataset for fetching documents from database.\n\n",
  "**`superduper.backends.local.compute`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/backends/local/compute.py)\n\n## `LocalComputeBackend` \n\n```python\nLocalComputeBackend(self)\n```\nA mockup backend for running jobs locally.\n\n",
  "**`superduper.backends.local.artifacts`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/backends/local/artifacts.py)\n\n## `FileSystemArtifactStore` \n\n```python\nFileSystemArtifactStore(self,\n     conn: Any,\n     name: Optional[str] = None)\n```\n| Parameter | Description |\n|-----------|-------------|\n| conn | root directory of the artifact store |\n| name | subdirectory to use for this artifact store |\n\nAbstraction for storing large artifacts separately from primary data.\n\n",
  "**`superduper.backends.ibis.db_helper`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/backends/ibis/db_helper.py)\n\n## `get_db_helper` \n\n```python\nget_db_helper(dialect) -> superduper.backends.ibis.db_helper.DBHelper\n```\n| Parameter | Description |\n|-----------|-------------|\n| dialect | The dialect of the database. |\n\nGet the insert processor for the given dialect.\n\n## `ClickHouseHelper` \n\n```python\nClickHouseHelper(self,\n     dialect)\n```\n| Parameter | Description |\n|-----------|-------------|\n| dialect | The dialect of the database. |\n\nHelper class for ClickHouse database.\n\nThis class is used to convert byte data to base64 format for storage in the\ndatabase.\n\n## `DBHelper` \n\n```python\nDBHelper(self,\n     dialect)\n```\n| Parameter | Description |\n|-----------|-------------|\n| dialect | The dialect of the database. |\n\nGeneric helper class for database.\n\n",
  "**`superduper.backends.ibis.query`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/backends/ibis/query.py)\n\n## `parse_query` \n\n```python\nparse_query(query,\n     documents: Sequence[Dict] = (),\n     db: Optional[ForwardRef('Datalayer')] = None)\n```\n| Parameter | Description |\n|-----------|-------------|\n| query | The query to parse. |\n| documents | The documents to query. |\n| db | The datalayer to use to execute the query. |\n\nParse a string query into a query object.\n\n## `IbisQuery` \n\n```python\nIbisQuery(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     parts: Sequence[Union[Tuple,\n     str]] = ()) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| parts | The parts of the query. |\n\nA query that can be executed on an Ibis database.\n\n## `RawSQL` \n\n```python\nRawSQL(self,\n     query: str,\n     id_field: str = 'id') -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| query | The raw SQL query |\n| id_field | The field to use as the primary id |\n\nRaw SQL query.\n\n",
  "**`superduper.backends.ibis.data_backend`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/backends/ibis/data_backend.py)\n\n## `IbisDataBackend` \n\n```python\nIbisDataBackend(self,\n     conn: ibis.backends.base.BaseBackend,\n     name: str,\n     in_memory: bool = False)\n```\n| Parameter | Description |\n|-----------|-------------|\n| conn | The connection to the database. |\n| name | The name of the database. |\n| in_memory | Whether to store the data in memory. |\n\nIbis data backend for the database.\n\n",
  "**`superduper.backends.ibis.field_types`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/backends/ibis/field_types.py)\n\n## `dtype` \n\n```python\ndtype(x)\n```\n| Parameter | Description |\n|-----------|-------------|\n| x | The data type e.g int, str, etc. |\n\nIbis dtype to represent basic data types in ibis.\n\n## `FieldType` \n\n```python\nFieldType(self,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     identifier: Union[str,\n     ibis.expr.datatypes.core.DataType]) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | The name of the data type. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n\nField type to represent the type of a field in a table.\n\nThis is a wrapper around ibis.expr.datatypes.DataType to make it\nserializable.\n\n",
  "**`superduper.backends.base.compute`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/backends/base/compute.py)\n\n## `ComputeBackend` \n\n```python\nComputeBackend(self,\n     /,\n     *args,\n     **kwargs)\n```\n| Parameter | Description |\n|-----------|-------------|\n| args | *args for `ABC` |\n| kwargs | *kwargs for `ABC` |\n\nAbstraction for sending jobs to a distributed compute platform.\n\n",
  "**`superduper.backends.base.artifacts`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/backends/base/artifacts.py)\n\n## `ArtifactSavingError` \n\n```python\nArtifactSavingError(self,\n     /,\n     *args,\n     **kwargs)\n```\n| Parameter | Description |\n|-----------|-------------|\n| args | *args for `Exception` |\n| kwargs | **kwargs for `Exception` |\n\nError when saving artifact in artifact store fails.\n\n## `ArtifactStore` \n\n```python\nArtifactStore(self,\n     conn: Any,\n     name: Optional[str] = None)\n```\n| Parameter | Description |\n|-----------|-------------|\n| conn | connection to the meta-data store |\n| name | Name to identify DB using the connection |\n\nAbstraction for storing large artifacts separately from primary data.\n\n",
  "**`superduper.backends.base.metadata`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/backends/base/metadata.py)\n\n## `MetaDataStore` \n\n```python\nMetaDataStore(self,\n     conn: Any,\n     name: Optional[str] = None)\n```\n| Parameter | Description |\n|-----------|-------------|\n| conn | connection to the meta-data store |\n| name | Name to identify DB using the connection |\n\nAbstraction for storing meta-data separately from primary data.\n\n## `NonExistentMetadataError` \n\n```python\nNonExistentMetadataError(self,\n     /,\n     *args,\n     **kwargs)\n```\n| Parameter | Description |\n|-----------|-------------|\n| args | *args for `Exception` |\n| kwargs | **kwargs for `Exception` |\n\nNonExistentMetadataError.\n\n",
  "**`superduper.backends.base.query`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/backends/base/query.py)\n\n## `applies_to` \n\n```python\napplies_to(*flavours)\n```\n| Parameter | Description |\n|-----------|-------------|\n| flavours | The flavours to check against. |\n\nDecorator to check if the query matches the accepted flavours.\n\n## `parse_query` \n\n```python\nparse_query(query: Union[str,\n     list],\n     builder_cls: Optional[Type[superduper.backends.base.query.Query]] = None,\n     documents: Sequence[Any] = (),\n     db: Optional[ForwardRef('Datalayer')] = None)\n```\n| Parameter | Description |\n|-----------|-------------|\n| query | The query to parse. |\n| builder_cls | The class to use to build the query. |\n| documents | The documents to query. |\n| db | The datalayer to use to execute the query. |\n\nParse a string query into a query object.\n\n## `Model` \n\n```python\nModel(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     parts: Sequence[Union[Tuple,\n     str]] = ()) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| parts | The parts of the query. |\n\nA model helper class for create a query to predict.\n\n## `Query` \n\n```python\nQuery(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     parts: Sequence[Union[Tuple,\n     str]] = ()) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| parts | The parts of the query. |\n\nA query object.\n\nThis base class is used to create a query object that can be executed\nin the datalayer.\n\n## `TraceMixin` \n\n```python\nTraceMixin(self,\n     /,\n     *args,\n     **kwargs)\n```\nMixin to add trace functionality to a query.\n\n",
  "**`superduper.backends.base.data_backend`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/backends/base/data_backend.py)\n\n## `BaseDataBackend` \n\n```python\nBaseDataBackend(self,\n     conn: Any,\n     name: str)\n```\n| Parameter | Description |\n|-----------|-------------|\n| conn | The connection to the databackend database. |\n| name | The name of the databackend. |\n\nBase data backend for the database.\n\n",
  "**`superduper.ext.sentence_transformers.model`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/ext/sentence_transformers/model.py)\n\n## `SentenceTransformer` \n\n```python\nSentenceTransformer(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     preferred_devices: 't.Sequence[str]' = ('cuda',\n     'mps',\n     'cpu'),\n     device: str = 'cpu',\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     signature: Literal['*args',\n     '**kwargs',\n     '*args,\n    **kwargs',\n     'singleton'] = 'singleton',\n     datatype: 'EncoderArg' = None,\n     output_schema: 't.Optional[Schema]' = None,\n     flatten: 'bool' = False,\n     model_update_kwargs: 't.Dict' = None,\n     predict_kwargs: 't.Dict' = None,\n     compute_kwargs: 't.Dict' = None,\n     validation: 't.Optional[Validation]' = None,\n     metric_values: 't.Dict' = None,\n     object: Optional[sentence_transformers.SentenceTransformer.SentenceTransformer] = None,\n     model: Optional[str] = None,\n     preprocess: Optional[Callable] = None,\n     postprocess: Optional[Callable] = None) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| signature | The signature of the model. |\n| datatype | DataType instance. |\n| output_schema | Output schema (mapping of encoders). |\n| flatten | Flatten the model outputs. |\n| model_update_kwargs | The kwargs to use for model update. |\n| predict_kwargs | Additional arguments to use at prediction time. |\n| compute_kwargs | Kwargs used for compute backend job submit. Example (Ray backend): compute_kwargs = dict(resources=...). |\n| validation | The validation ``Dataset`` instances to use. |\n| metric_values | The metrics to evaluate on. |\n| object | The SentenceTransformer object to use. |\n| model | The model name, e.g. 'all-MiniLM-L6-v2'. |\n| device | The device to use, e.g. 'cpu' or 'cuda'. |\n| preprocess | The preprocessing function to apply to the input. |\n| postprocess | The postprocessing function to apply to the output. |\n| preferred_devices | A list of devices to prefer, in that order. |\n\nA model for sentence embeddings using `sentence-transformers`.\n\n",
  "**`superduper.ext.cohere.model`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/ext/cohere/model.py)\n\n## `CohereEmbed` \n\n```python\nCohereEmbed(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     signature: str = 'singleton',\n     datatype: 'EncoderArg' = None,\n     output_schema: 't.Optional[Schema]' = None,\n     flatten: 'bool' = False,\n     model_update_kwargs: 't.Dict' = None,\n     predict_kwargs: 't.Dict' = None,\n     compute_kwargs: 't.Dict' = None,\n     validation: 't.Optional[Validation]' = None,\n     metric_values: 't.Dict' = None,\n     model: 't.Optional[str]' = None,\n     max_batch_size: 'int' = 8,\n     client_kwargs: Dict[str,\n     Any] = None,\n     shape: Optional[Sequence[int]] = None,\n     batch_size: int = 100) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| signature | Model signature. |\n| datatype | DataType instance. |\n| output_schema | Output schema (mapping of encoders). |\n| flatten | Flatten the model outputs. |\n| model_update_kwargs | The kwargs to use for model update. |\n| predict_kwargs | Additional arguments to use at prediction time. |\n| compute_kwargs | Kwargs used for compute backend job submit. Example (Ray backend): compute_kwargs = dict(resources=...). |\n| validation | The validation ``Dataset`` instances to use. |\n| metric_values | The metrics to evaluate on. |\n| model | The Model to use, e.g. ``'text-embedding-ada-002'`` |\n| max_batch_size | Maximum  batch size. |\n| client_kwargs | The keyword arguments to pass to the client. |\n| shape | The shape as ``tuple`` of the embedding. |\n| batch_size | The batch size to use for the predictor. |\n\nCohere embedding predictor.\n\n## `CohereGenerate` \n\n```python\nCohereGenerate(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     signature: str = '*args,\n    **kwargs',\n     datatype: 'EncoderArg' = None,\n     output_schema: 't.Optional[Schema]' = None,\n     flatten: 'bool' = False,\n     model_update_kwargs: 't.Dict' = None,\n     predict_kwargs: 't.Dict' = None,\n     compute_kwargs: 't.Dict' = None,\n     validation: 't.Optional[Validation]' = None,\n     metric_values: 't.Dict' = None,\n     model: 't.Optional[str]' = None,\n     max_batch_size: 'int' = 8,\n     client_kwargs: Dict[str,\n     Any] = None,\n     takes_context: bool = True,\n     prompt: str = '') -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| signature | Model signature. |\n| datatype | DataType instance. |\n| output_schema | Output schema (mapping of encoders). |\n| flatten | Flatten the model outputs. |\n| model_update_kwargs | The kwargs to use for model update. |\n| predict_kwargs | Additional arguments to use at prediction time. |\n| compute_kwargs | Kwargs used for compute backend job submit. Example (Ray backend): compute_kwargs = dict(resources=...). |\n| validation | The validation ``Dataset`` instances to use. |\n| metric_values | The metrics to evaluate on. |\n| model | The Model to use, e.g. ``'text-embedding-ada-002'`` |\n| max_batch_size | Maximum  batch size. |\n| client_kwargs | The keyword arguments to pass to the client. |\n| takes_context | Whether the model takes context into account. |\n| prompt | The prompt to use to seed the response. |\n\nCohere realistic text generator (chat predictor).\n\n## `Cohere` \n\n```python\nCohere(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     signature: 'Signature' = '*args,\n    **kwargs',\n     datatype: 'EncoderArg' = None,\n     output_schema: 't.Optional[Schema]' = None,\n     flatten: 'bool' = False,\n     model_update_kwargs: 't.Dict' = None,\n     predict_kwargs: 't.Dict' = None,\n     compute_kwargs: 't.Dict' = None,\n     validation: 't.Optional[Validation]' = None,\n     metric_values: 't.Dict' = None,\n     model: 't.Optional[str]' = None,\n     max_batch_size: 'int' = 8,\n     client_kwargs: Dict[str,\n     Any] = None) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| signature | Model signature. |\n| datatype | DataType instance. |\n| output_schema | Output schema (mapping of encoders). |\n| flatten | Flatten the model outputs. |\n| model_update_kwargs | The kwargs to use for model update. |\n| predict_kwargs | Additional arguments to use at prediction time. |\n| compute_kwargs | Kwargs used for compute backend job submit. Example (Ray backend): compute_kwargs = dict(resources=...). |\n| validation | The validation ``Dataset`` instances to use. |\n| metric_values | The metrics to evaluate on. |\n| model | The Model to use, e.g. ``'text-embedding-ada-002'`` |\n| max_batch_size | Maximum  batch size. |\n| client_kwargs | The keyword arguments to pass to the client. |\n\nCohere predictor.\n\n",
  "**`superduper.ext.utils`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/ext/utils.py)\n\n## `str_shape` \n\n```python\nstr_shape(shape: Sequence[int]) -> str\n```\n| Parameter | Description |\n|-----------|-------------|\n| shape | The shape to convert. |\n\nConvert a shape to a string.\n\n## `format_prompt` \n\n```python\nformat_prompt(X: str,\n     prompt: str,\n     context: Optional[List[str]] = None) -> str\n```\n| Parameter | Description |\n|-----------|-------------|\n| X | The input to format the prompt with. |\n| prompt | The prompt to format. |\n| context | The context to format the prompt with. |\n\nFormat a prompt with the given input and context.\n\n## `get_key` \n\n```python\nget_key(key_name: str) -> str\n```\n| Parameter | Description |\n|-----------|-------------|\n| key_name | The name of the environment variable to get. |\n\nGet an environment variable.\n\n",
  "**`superduper.ext.llamacpp.model`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/ext/llamacpp/model.py)\n\n## `download_uri` \n\n```python\ndownload_uri(uri,\n     save_path)\n```\n| Parameter | Description |\n|-----------|-------------|\n| uri | URI to download |\n| save_path | place to save |\n\nDownload file.\n\n## `LlamaCpp` \n\n```python\nLlamaCpp(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     signature: str = 'singleton',\n     datatype: 'EncoderArg' = None,\n     output_schema: 't.Optional[Schema]' = None,\n     flatten: 'bool' = False,\n     model_update_kwargs: 't.Dict' = None,\n     predict_kwargs: 't.Dict' = None,\n     compute_kwargs: 't.Dict' = None,\n     validation: 't.Optional[Validation]' = None,\n     metric_values: 't.Dict' = None,\n     prompt: str = '{input}',\n     prompt_func: Optional[Callable] = None,\n     max_batch_size: Optional[int] = 4,\n     model_name_or_path: str = 'facebook/opt-125m',\n     model_kwargs: Dict = None,\n     download_dir: str = '.llama_cpp') -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| signature | Model signature. |\n| datatype | DataType instance. |\n| output_schema | Output schema (mapping of encoders). |\n| flatten | Flatten the model outputs. |\n| model_update_kwargs | The kwargs to use for model update. |\n| predict_kwargs | Additional arguments to use at prediction time. |\n| compute_kwargs | Kwargs used for compute backend job submit. Example (Ray backend): compute_kwargs = dict(resources=...). |\n| validation | The validation ``Dataset`` instances to use. |\n| metric_values | The metrics to evaluate on. |\n| prompt | The template to use for the prompt. |\n| prompt_func | The function to use for the prompt. |\n| max_batch_size | The maximum batch size to use for batch generation. |\n| model_name_or_path | path or name of model |\n| model_kwargs | dictionary of init-kwargs |\n| download_dir | local caching directory |\n\nLlama.cpp connector.\n\n## `LlamaCppEmbedding` \n\n```python\nLlamaCppEmbedding(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     signature: str = 'singleton',\n     datatype: 'EncoderArg' = None,\n     output_schema: 't.Optional[Schema]' = None,\n     flatten: 'bool' = False,\n     model_update_kwargs: 't.Dict' = None,\n     predict_kwargs: 't.Dict' = None,\n     compute_kwargs: 't.Dict' = None,\n     validation: 't.Optional[Validation]' = None,\n     metric_values: 't.Dict' = None,\n     prompt: str = '{input}',\n     prompt_func: Optional[Callable] = None,\n     max_batch_size: Optional[int] = 4,\n     model_name_or_path: str = 'facebook/opt-125m',\n     model_kwargs: Dict = None,\n     download_dir: str = '.llama_cpp') -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| signature | Model signature. |\n| datatype | DataType instance. |\n| output_schema | Output schema (mapping of encoders). |\n| flatten | Flatten the model outputs. |\n| model_update_kwargs | The kwargs to use for model update. |\n| predict_kwargs | Additional arguments to use at prediction time. |\n| compute_kwargs | Kwargs used for compute backend job submit. Example (Ray backend): compute_kwargs = dict(resources=...). |\n| validation | The validation ``Dataset`` instances to use. |\n| metric_values | The metrics to evaluate on. |\n| prompt | The template to use for the prompt. |\n| prompt_func | The function to use for the prompt. |\n| max_batch_size | The maximum batch size to use for batch generation. |\n| model_name_or_path | path or name of model |\n| model_kwargs | dictionary of init-kwargs |\n| download_dir | local caching directory |\n\nLlama.cpp connector for embeddings.\n\n",
  "**`superduper.ext.torch.model`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/ext/torch/model.py)\n\n## `create_batch` \n\n```python\ncreate_batch(args)\n```\n| Parameter | Description |\n|-----------|-------------|\n| args | single data point for batching |\n\nCreate a singleton batch in a manner similar to the PyTorch dataloader.\n\n```python\ncreate_batch(3.).shape\n# torch.Size([1])\nx, y = create_batch([torch.randn(5), torch.randn(3, 7)])\nx.shape\n# torch.Size([1, 5])\ny.shape\n# torch.Size([1, 3, 7])\nd = create_batch(({'a': torch.randn(4)}))\nd['a'].shape\n# torch.Size([1, 4])\n```\n\n## `torchmodel` \n\n```python\ntorchmodel(class_obj)\n```\n| Parameter | Description |\n|-----------|-------------|\n| class_obj | Class to decorate |\n\nA decorator to convert a `torch.nn.Module` into a `TorchModel`.\n\nDecorate a `torch.nn.Module` so that when it is invoked,\nthe result is a `TorchModel`.\n\n## `unpack_batch` \n\n```python\nunpack_batch(args)\n```\n| Parameter | Description |\n|-----------|-------------|\n| args | a batch of model outputs |\n\nUnpack a batch into lines of tensor output.\n\n```python\nunpack_batch(torch.randn(1, 10))[0].shape\n# torch.Size([10])\nout = unpack_batch([torch.randn(2, 10), torch.randn(2, 3, 5)])\ntype(out)\n# <class 'list'>\nlen(out)\n# 2\nout = unpack_batch({'a': torch.randn(2, 10), 'b': torch.randn(2, 3, 5)})\n[type(x) for x in out]\n# [<class 'dict'>, <class 'dict'>]\nout[0]['a'].shape\n# torch.Size([10])\nout[0]['b'].shape\n# torch.Size([3, 5])\nout = unpack_batch({'a': {'b': torch.randn(2, 10)}})\nout[0]['a']['b'].shape\n# torch.Size([10])\nout[1]['a']['b'].shape\n# torch.Size([10])\n```\n\n## `TorchModel` \n\n```python\nTorchModel(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     preferred_devices: 't.Sequence[str]' = ('cuda',\n     'mps',\n     'cpu'),\n     device: 't.Optional[str]' = None,\n     trainer: 't.Optional[Trainer]' = None,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     signature: 'Signature' = '*args,\n    **kwargs',\n     datatype: 'EncoderArg' = None,\n     output_schema: 't.Optional[Schema]' = None,\n     flatten: 'bool' = False,\n     model_update_kwargs: 't.Dict' = None,\n     predict_kwargs: 't.Dict' = None,\n     compute_kwargs: 't.Dict' = None,\n     validation: 't.Optional[Validation]' = None,\n     metric_values: 't.Dict' = None,\n     object: 'torch.nn.Module',\n     preprocess: 't.Optional[t.Callable]' = None,\n     preprocess_signature: 'Signature' = 'singleton',\n     postprocess: 't.Optional[t.Callable]' = None,\n     postprocess_signature: 'Signature' = 'singleton',\n     forward_method: 'str' = '__call__',\n     forward_signature: 'Signature' = 'singleton',\n     train_forward_method: 'str' = '__call__',\n     train_forward_signature: 'Signature' = 'singleton',\n     train_preprocess: 't.Optional[t.Callable]' = None,\n     train_preprocess_signature: 'Signature' = 'singleton',\n     collate_fn: 't.Optional[t.Callable]' = None,\n     optimizer_state: 't.Optional[t.Any]' = None,\n     loader_kwargs: 't.Dict' = None) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| signature | Model signature. |\n| datatype | DataType instance. |\n| output_schema | Output schema (mapping of encoders). |\n| flatten | Flatten the model outputs. |\n| model_update_kwargs | The kwargs to use for model update. |\n| predict_kwargs | Additional arguments to use at prediction time. |\n| compute_kwargs | Kwargs used for compute backend job submit. Example (Ray backend): compute_kwargs = dict(resources=...). |\n| validation | The validation ``Dataset`` instances to use. |\n| metric_values | The metrics to evaluate on. |\n| object | Torch model, e.g. `torch.nn.Module` |\n| preprocess | Preprocess function, the function to apply to the input |\n| preprocess_signature | The signature of the preprocess function |\n| postprocess | The postprocess function, the function to apply to the output |\n| postprocess_signature | The signature of the postprocess function |\n| forward_method | The forward method, the method to call on the model |\n| forward_signature | The signature of the forward method |\n| train_forward_method | Train forward method, the method to call on the model |\n| train_forward_signature | The signature of the train forward method |\n| train_preprocess | Train preprocess function, the function to apply to the input |\n| train_preprocess_signature | The signature of the train preprocess function |\n| collate_fn | The collate function for the dataloader |\n| optimizer_state | The optimizer state |\n| loader_kwargs | The kwargs for the dataloader |\n| trainer | `Trainer` object to train the model |\n| preferred_devices | The order of devices to use |\n| device | The device to be used |\n\nTorch model. This class is a wrapper around a PyTorch model.\n\n## `BasicDataset` \n\n```python\nBasicDataset(self,\n     items,\n     transform,\n     signature)\n```\n| Parameter | Description |\n|-----------|-------------|\n| items | items, typically documents |\n| transform | function, typically a preprocess function |\n| signature | signature of the transform function |\n\nBasic database iterating over a list of documents and applying a transformation.\n\n",
  "**`superduper.ext.torch.utils`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/ext/torch/utils.py)\n\n## `device_of` \n\n```python\ndevice_of(module: 'Module') -> 't.Union[_device,\n     str]'\n```\n| Parameter | Description |\n|-----------|-------------|\n| module | PyTorch model |\n\nGet device of a model.\n\n## `eval` \n\n```python\neval(module: 'Module') -> 't.Iterator[None]'\n```\n| Parameter | Description |\n|-----------|-------------|\n| module | PyTorch module |\n\nTemporarily set a module to evaluation mode.\n\n## `to_device` \n\n```python\nto_device(item: 't.Any',\n     device: 't.Union[str,\n     _device]') -> 't.Any'\n```\n| Parameter | Description |\n|-----------|-------------|\n| item | torch.Tensor instance |\n| device | device to which one would like to send |\n\nSend tensor leaves of nested list/ dictionaries/ tensors to device.\n\n## `set_device` \n\n```python\nset_device(module: 'Module',\n     device: '_device')\n```\n| Parameter | Description |\n|-----------|-------------|\n| module | PyTorch module |\n| device | Device to set |\n\nTemporarily set a device of a module.\n\n",
  "**`superduper.ext.torch.training`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/ext/torch/training.py)\n\n## `TorchTrainer` \n\n```python\nTorchTrainer(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     key: 'ModelInputType',\n     select: 'Query',\n     transform: 't.Optional[t.Callable]' = None,\n     metric_values: Dict = None,\n     signature: 'Signature' = '*args',\n     data_prefetch: 'bool' = False,\n     prefetch_size: 'int' = 1000,\n     prefetch_factor: 'int' = 100,\n     in_memory: 'bool' = True,\n     compute_kwargs: 't.Dict' = None,\n     objective: Callable,\n     loader_kwargs: Dict = None,\n     max_iterations: int = 10000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000,\n     no_improve_then_stop: int = 5,\n     download: bool = False,\n     validation_interval: int = 100,\n     listen: str = 'objective',\n     optimizer_cls: str = 'Adam',\n     optimizer_kwargs: Dict = None,\n     optimizer_state: Optional[Dict] = None,\n     collate_fn: Optional[Callable] = None) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| key | Model input type key. |\n| select | Model select query for training. |\n| transform | (optional) transform callable. |\n| metric_values | Metric values |\n| signature | Model signature. |\n| data_prefetch | Boolean for prefetching data before forward pass. |\n| prefetch_size | Prefetch batch size. |\n| prefetch_factor | Prefetch factor for data prefetching. |\n| in_memory | If training in memory. |\n| compute_kwargs | Kwargs for compute backend. |\n| objective | Objective function |\n| loader_kwargs | Kwargs for the dataloader |\n| max_iterations | Maximum number of iterations |\n| no_improve_then_stop | Number of iterations to wait for improvement before stopping |\n| download | Whether to download the data |\n| validation_interval | How often to validate |\n| listen | Which metric to listen to for early stopping |\n| optimizer_cls | Optimizer class |\n| optimizer_kwargs | Kwargs for the optimizer |\n| optimizer_state | Latest state of the optimizer for contined training |\n| collate_fn | Collate function for the dataloader |\n\nConfiguration for the PyTorch trainer.\n\n",
  "**`superduper.ext.torch.encoder`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/ext/torch/encoder.py)\n\n## `tensor` \n\n```python\ntensor(dtype,\n     shape: Sequence,\n     bytes_encoding: Optional[str] = None,\n     encodable: str = 'encodable',\n     db: Optional[ForwardRef('Datalayer')] = None)\n```\n| Parameter | Description |\n|-----------|-------------|\n| dtype | The dtype of the tensor. |\n| shape | The shape of the tensor. |\n| bytes_encoding | The bytes encoding to use. |\n| encodable | The encodable name [\"artifact\", \"encodable\", \"lazy_artifact\", \"file\"]. |\n| db | The datalayer instance. |\n\nCreate an encoder for a tensor of a given dtype and shape.\n\n## `DecodeTensor` \n\n```python\nDecodeTensor(self,\n     dtype,\n     shape)\n```\n| Parameter | Description |\n|-----------|-------------|\n| dtype | The dtype of the tensor, eg. torch.float32 |\n| shape | The shape of the tensor, eg. (3, 4) |\n\nDecode a tensor from bytes.\n\n## `EncodeTensor` \n\n```python\nEncodeTensor(self,\n     dtype)\n```\n| Parameter | Description |\n|-----------|-------------|\n| dtype | The dtype of the tensor, eg. torch.float32 |\n\nEncode a tensor to bytes.\n\n",
  "**`superduper.ext.vllm.model`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/ext/vllm/model.py)\n\n## `VllmAPI` \n\n```python\nVllmAPI(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     api_url: str = '',\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     signature: str = 'singleton',\n     datatype: 'EncoderArg' = None,\n     output_schema: 't.Optional[Schema]' = None,\n     flatten: 'bool' = False,\n     model_update_kwargs: 't.Dict' = None,\n     predict_kwargs: 't.Dict' = None,\n     compute_kwargs: 't.Dict' = None,\n     validation: 't.Optional[Validation]' = None,\n     metric_values: 't.Dict' = None,\n     prompt: str = '{input}',\n     prompt_func: Optional[Callable] = None,\n     max_batch_size: Optional[int] = 4) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| signature | Model signature. |\n| datatype | DataType instance. |\n| output_schema | Output schema (mapping of encoders). |\n| flatten | Flatten the model outputs. |\n| model_update_kwargs | The kwargs to use for model update. |\n| predict_kwargs | Additional arguments to use at prediction time. |\n| compute_kwargs | Kwargs used for compute backend job submit. Example (Ray backend): compute_kwargs = dict(resources=...). |\n| validation | The validation ``Dataset`` instances to use. |\n| metric_values | The metrics to evaluate on. |\n| prompt | The template to use for the prompt. |\n| prompt_func | The function to use for the prompt. |\n| max_batch_size | The maximum batch size to use for batch generation. |\n| api_url | The URL for the API. |\n\nWrapper for requesting the vLLM API service.\n\nAPI Server format, started by `vllm.entrypoints.api_server`.\n\n## `VllmModel` \n\n```python\nVllmModel(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     signature: str = 'singleton',\n     datatype: 'EncoderArg' = None,\n     output_schema: 't.Optional[Schema]' = None,\n     flatten: 'bool' = False,\n     model_update_kwargs: 't.Dict' = None,\n     predict_kwargs: 't.Dict' = None,\n     compute_kwargs: 't.Dict' = None,\n     validation: 't.Optional[Validation]' = None,\n     metric_values: 't.Dict' = None,\n     prompt: str = '{input}',\n     prompt_func: Optional[Callable] = None,\n     max_batch_size: Optional[int] = 4,\n     model_name: str = '',\n     tensor_parallel_size: int = 1,\n     trust_remote_code: bool = True,\n     vllm_kwargs: dict = None,\n     on_ray: bool = False,\n     ray_address: Optional[str] = None,\n     ray_config: dict = None) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| signature | Model signature. |\n| datatype | DataType instance. |\n| output_schema | Output schema (mapping of encoders). |\n| flatten | Flatten the model outputs. |\n| model_update_kwargs | The kwargs to use for model update. |\n| predict_kwargs | Additional arguments to use at prediction time. |\n| compute_kwargs | Kwargs used for compute backend job submit. Example (Ray backend): compute_kwargs = dict(resources=...). |\n| validation | The validation ``Dataset`` instances to use. |\n| metric_values | The metrics to evaluate on. |\n| prompt | The template to use for the prompt. |\n| prompt_func | The function to use for the prompt. |\n| max_batch_size | The maximum batch size to use for batch generation. |\n| model_name | The name of the model to use. |\n| tensor_parallel_size | The number of tensor parallelism. |\n| trust_remote_code | Whether to trust remote code. |\n| vllm_kwargs | Additional arguments to pass to the VLLM |\n| on_ray | Whether to use Ray for parallelism. |\n| ray_address | The address of the Ray cluster. |\n| ray_config | The configuration for Ray. |\n\nLoad a large language model from VLLM.\n\n",
  "**`superduper.ext.numpy.encoder`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/ext/numpy/encoder.py)\n\n## `array` \n\n```python\narray(dtype: str,\n     shape: Sequence,\n     bytes_encoding: Optional[str] = None,\n     encodable: str = 'encodable')\n```\n| Parameter | Description |\n|-----------|-------------|\n| dtype | The dtype of the array. |\n| shape | The shape of the array. |\n| bytes_encoding | The bytes encoding to use. |\n| encodable | The encodable to use. |\n\nCreate an encoder of numpy arrays.\n\n## `DecodeArray` \n\n```python\nDecodeArray(self,\n     dtype,\n     shape)\n```\n| Parameter | Description |\n|-----------|-------------|\n| dtype | The dtype of the array. |\n| shape | The shape of the array. |\n\nDecode a numpy array from bytes.\n\n## `EncodeArray` \n\n```python\nEncodeArray(self,\n     dtype)\n```\n| Parameter | Description |\n|-----------|-------------|\n| dtype | The dtype of the array. |\n\nEncode a numpy array to bytes.\n\n",
  "**`superduper.ext.sklearn.model`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/ext/sklearn/model.py)\n\n## `Estimator` \n\n```python\nEstimator(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     trainer: Optional[superduper.ext.sklearn.model.SklearnTrainer] = None,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     signature: Literal['*args',\n     '**kwargs',\n     '*args,\n    **kwargs',\n     'singleton'] = 'singleton',\n     datatype: 'EncoderArg' = None,\n     output_schema: 't.Optional[Schema]' = None,\n     flatten: 'bool' = False,\n     model_update_kwargs: 't.Dict' = None,\n     predict_kwargs: 't.Dict' = None,\n     compute_kwargs: 't.Dict' = None,\n     validation: 't.Optional[Validation]' = None,\n     metric_values: 't.Dict' = None,\n     object: sklearn.base.BaseEstimator,\n     preprocess: Optional[Callable] = None,\n     postprocess: Optional[Callable] = None) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| signature | Model signature. |\n| datatype | DataType instance. |\n| output_schema | Output schema (mapping of encoders). |\n| flatten | Flatten the model outputs. |\n| model_update_kwargs | The kwargs to use for model update. |\n| predict_kwargs | Additional arguments to use at prediction time. |\n| compute_kwargs | Kwargs used for compute backend job submit. Example (Ray backend): compute_kwargs = dict(resources=...). |\n| validation | The validation ``Dataset`` instances to use. |\n| metric_values | The metrics to evaluate on. |\n| object | The estimator object from `sklearn`. |\n| trainer | The trainer to use. |\n| preprocess | The preprocessing function to use. |\n| postprocess | The postprocessing function to use. |\n\nEstimator model.\n\nThis is a model that can be trained and used for prediction.\n\n## `SklearnTrainer` \n\n```python\nSklearnTrainer(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     key: 'ModelInputType',\n     select: 'Query',\n     transform: 't.Optional[t.Callable]' = None,\n     metric_values: 't.Dict' = None,\n     signature: 'Signature' = '*args',\n     data_prefetch: 'bool' = False,\n     prefetch_size: 'int' = 1000,\n     prefetch_factor: 'int' = 100,\n     in_memory: 'bool' = True,\n     compute_kwargs: 't.Dict' = None,\n     fit_params: Dict = None,\n     predict_params: Dict = None,\n     y_preprocess: Optional[Callable] = None) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| key | Model input type key. |\n| select | Model select query for training. |\n| transform | (optional) transform callable. |\n| metric_values | Dictionary for metric defaults. |\n| signature | Model signature. |\n| data_prefetch | Boolean for prefetching data before forward pass. |\n| prefetch_size | Prefetch batch size. |\n| prefetch_factor | Prefetch factor for data prefetching. |\n| in_memory | If training in memory. |\n| compute_kwargs | Kwargs for compute backend. |\n| fit_params | The parameters to pass to `fit`. |\n| predict_params | The parameters to pass to `predict |\n| y_preprocess | The preprocessing function to use for the target. |\n\nA trainer for `sklearn` models.\n\n",
  "**`superduper.ext.pillow.encoder`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/ext/pillow/encoder.py)\n\n## `encode_pil_image` \n\n```python\nencode_pil_image(x,\n     info: Optional[Dict] = None)\n```\n| Parameter | Description |\n|-----------|-------------|\n| x | The image to encode. |\n| info | Additional information. |\n\nEncode a `PIL.Image` to bytes.\n\n## `image_type` \n\n```python\nimage_type(identifier: str,\n     encodable: str = 'lazy_artifact',\n     media_type: str = 'image/png',\n     db: Optional[ForwardRef('Datalayer')] = None)\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | The identifier for the data type. |\n| encodable | The encodable type. |\n| media_type | The media type. |\n| db | The datalayer instance. |\n\nCreate a `DataType` for an image.\n\n## `DecoderPILImage` \n\n```python\nDecoderPILImage(self,\n     handle_exceptions: bool = True)\n```\n| Parameter | Description |\n|-----------|-------------|\n| handle_exceptions | return a blank image if failure |\n\nDecoder to convert `bytes` back into a `PIL.Image` class.\n\n",
  "**`superduper.ext.transformers.model`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/ext/transformers/model.py)\n\n## `LLM` \n\n```python\nLLM(self,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     trainer: 't.Optional[Trainer]' = None,\n     identifier: str = '',\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     datatype: 'EncoderArg' = None,\n     output_schema: 't.Optional[Schema]' = None,\n     flatten: 'bool' = False,\n     model_update_kwargs: 't.Dict' = None,\n     predict_kwargs: 't.Dict' = None,\n     compute_kwargs: 't.Dict' = None,\n     validation: 't.Optional[Validation]' = None,\n     metric_values: 't.Dict' = None,\n     prompt: str = '{input}',\n     prompt_func: Optional[Callable] = None,\n     max_batch_size: Optional[int] = 4,\n     model_name_or_path: Optional[str] = None,\n     adapter_id: Union[str,\n     superduper.ext.transformers.training.Checkpoint,\n     NoneType] = None,\n     model_kwargs: Dict = None,\n     tokenizer_kwargs: Dict = None,\n     prompt_template: str = '{input}') -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | model identifier |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| signature | Model signature. |\n| datatype | DataType instance. |\n| output_schema | Output schema (mapping of encoders). |\n| flatten | Flatten the model outputs. |\n| model_update_kwargs | The kwargs to use for model update. |\n| predict_kwargs | Additional arguments to use at prediction time. |\n| compute_kwargs | Kwargs used for compute backend job submit. Example (Ray backend): compute_kwargs = dict(resources=...). |\n| validation | The validation ``Dataset`` instances to use. |\n| metric_values | The metrics to evaluate on. |\n| prompt | The template to use for the prompt. |\n| prompt_func | prompt function, default is None |\n| max_batch_size | The maximum batch size to use for batch generation. |\n| model_name_or_path | model name or path |\n| adapter_id | adapter id, default is None Add a adapter to the base model for inference. |\n| model_kwargs | model kwargs, all the kwargs will pass to `transformers.AutoModelForCausalLM.from_pretrained` |\n| tokenizer_kwargs | tokenizer kwargs, all the kwargs will pass to `transformers.AutoTokenizer.from_pretrained` |\n| prompt_template | prompt template, default is `\"{input}\"` |\n\nLLM model based on `transformers` library.\n\nAll the `model_kwargs` will pass to\n`transformers.AutoModelForCausalLM.from_pretrained`.\nAll the `tokenize_kwargs` will pass to\n`transformers.AutoTokenizer.from_pretrained`.\nWhen `model_name_or_path`, `bits`, `model_kwargs`, `tokenizer_kwargs` are the same,\nwill share the same base model and tokenizer cache.\n\n## `TextClassificationPipeline` \n\n```python\nTextClassificationPipeline(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     preferred_devices: 't.Sequence[str]' = ('cuda',\n     'mps',\n     'cpu'),\n     device: 't.Optional[str]' = None,\n     trainer: 't.Optional[Trainer]' = None,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     signature: Literal['*args',\n     '**kwargs',\n     '*args,\n    **kwargs',\n     'singleton'] = 'singleton',\n     datatype: 'EncoderArg' = None,\n     output_schema: 't.Optional[Schema]' = None,\n     flatten: 'bool' = False,\n     model_update_kwargs: 't.Dict' = None,\n     predict_kwargs: 't.Dict' = None,\n     compute_kwargs: 't.Dict' = None,\n     validation: 't.Optional[Validation]' = None,\n     metric_values: 't.Dict' = None,\n     tokenizer_name: Optional[str] = None,\n     tokenizer_cls: object = <class 'transformers.models.auto.tokenization_auto.AutoTokenizer'>,\n     tokenizer_kwargs: Dict = None,\n     model_name: Optional[str] = None,\n     model_cls: object = <class 'transformers.models.auto.modeling_auto.AutoModelForSequenceClassification'>,\n     model_kwargs: Dict = None,\n     pipeline: Optional[transformers.pipelines.base.Pipeline] = None,\n     task: str = 'text-classification') -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| signature | Model signature. |\n| datatype | DataType instance. |\n| output_schema | Output schema (mapping of encoders). |\n| flatten | Flatten the model outputs. |\n| model_update_kwargs | The kwargs to use for model update. |\n| predict_kwargs | Additional arguments to use at prediction time. |\n| compute_kwargs | Kwargs used for compute backend job submit. Example (Ray backend): compute_kwargs = dict(resources=...). |\n| validation | The validation ``Dataset`` instances to use. |\n| metric_values | The metrics to evaluate on. |\n| tokenizer_name | tokenizer name |\n| tokenizer_cls | tokenizer class, e.g. ``transformers.AutoTokenizer`` |\n| tokenizer_kwargs | tokenizer kwargs, will pass to ``tokenizer_cls`` |\n| model_name | model name, will pass to ``model_cls`` |\n| model_cls | model class, e.g. ``AutoModelForSequenceClassification`` |\n| model_kwargs | model kwargs, will pass to ``model_cls`` |\n| pipeline | pipeline instance, default is None, will build when None |\n| task | task of the pipeline |\n| trainer | `TransformersTrainer` instance |\n| preferred_devices | preferred devices |\n| device | device to use |\n\nA wrapper for ``transformers.Pipeline``.\n\n```python\n# Example:\n# -------\nmodel = TextClassificationPipeline(...)\n```\n\n",
  "**`superduper.ext.transformers.training`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/ext/transformers/training.py)\n\n## `create_quantization_config` \n\n```python\ncreate_quantization_config(config: superduper.ext.transformers.training.LLMTrainer)\n```\n| Parameter | Description |\n|-----------|-------------|\n| config | The configuration to use. |\n\nCreate quantization config for LLM training.\n\n## `handle_ray_results` \n\n```python\nhandle_ray_results(db,\n     llm,\n     results)\n```\n| Parameter | Description |\n|-----------|-------------|\n| db | datalayer, used for saving the checkpoint |\n| llm | llm model, used for saving the checkpoint |\n| results | the ray training results, contains the checkpoint |\n\nHandle the ray results.\n\nWill save the checkpoint to db if db and llm provided.\n\n## `prepare_lora_training` \n\n```python\nprepare_lora_training(model,\n     config: superduper.ext.transformers.training.LLMTrainer)\n```\n| Parameter | Description |\n|-----------|-------------|\n| model | The model to prepare for LoRA training. |\n| config | The configuration to use. |\n\nPrepare LoRA training for the model.\n\nGet the LoRA target modules and convert the model to peft model.\n\n## `train_func` \n\n```python\ntrain_func(training_args: superduper.ext.transformers.training.LLMTrainer,\n     train_dataset: 'Dataset',\n     eval_datasets: Union[ForwardRef('Dataset'),\n     Dict[str,\n     ForwardRef('Dataset')]],\n     model_kwargs: dict,\n     tokenizer_kwargs: dict,\n     trainer_prepare_func: Optional[Callable] = None,\n     callbacks=None,\n     **kwargs)\n```\n| Parameter | Description |\n|-----------|-------------|\n| training_args | training Arguments, see LLMTrainingArguments |\n| train_dataset | training dataset, can be huggingface datasets.Dataset or ray.data.Dataset |\n| eval_datasets | evaluation dataset, can be a dict of datasets |\n| model_kwargs | model kwargs for AutoModelForCausalLM |\n| tokenizer_kwargs | tokenizer kwargs for AutoTokenizer |\n| trainer_prepare_func | function to prepare trainer This function will be called after the trainer is created, we can add some custom settings to the trainer |\n| callbacks | list of callbacks will be added to the trainer |\n| kwargs | other kwargs for Trainer All the kwargs will be passed to Trainer, make sure the Trainer support these kwargs |\n\nBase training function for LLM model.\n\n## `tokenize` \n\n```python\ntokenize(tokenizer,\n     example,\n     X,\n     y)\n```\n| Parameter | Description |\n|-----------|-------------|\n| tokenizer | The tokenizer to use. |\n| example | The example to tokenize. |\n| X | The input key. |\n| y | The output key. |\n\nFunction to tokenize the example.\n\n## `train` \n\n```python\ntrain(training_args: superduper.ext.transformers.training.LLMTrainer,\n     train_dataset: datasets.arrow_dataset.Dataset,\n     eval_datasets: Union[datasets.arrow_dataset.Dataset,\n     Dict[str,\n     datasets.arrow_dataset.Dataset]],\n     model_kwargs: dict,\n     tokenizer_kwargs: dict,\n     db: Optional[ForwardRef('Datalayer')] = None,\n     llm: Optional[ForwardRef('LLM')] = None,\n     ray_configs: Optional[dict] = None,\n     **kwargs)\n```\n| Parameter | Description |\n|-----------|-------------|\n| training_args | training Arguments, see LLMTrainingArguments |\n| train_dataset | training dataset |\n| eval_datasets | evaluation dataset, can be a dict of datasets |\n| model_kwargs | model kwargs for AutoModelForCausalLM |\n| tokenizer_kwargs | tokenizer kwargs for AutoTokenizer |\n| db | datalayer, used for creating LLMCallback |\n| llm | llm model, used for creating LLMCallback |\n| ray_configs | ray configs, must provide if using ray |\n| kwargs | other kwargs for Trainer |\n\nTrain LLM model on specified dataset.\n\nThe training process can be run on these following modes:\n- Local node without ray, but only support single GPU\n- Local node with ray, support multi-nodes and multi-GPUs\n- Remote node with ray, support multi-nodes and multi-GPUs\n\nIf run locally, will use train_func to train the model.\nCan log the training process to db if db and llm provided.\nWill reuse the db and llm from the current process.\nIf run on ray, will use ray_train to train the model.\nCan log the training process to db if db and llm provided.\nWill rebuild the db and llm for the new process that can access to db.\nThe ray cluster must can access to db.\n\n## `Checkpoint` \n\n```python\nCheckpoint(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     path: Optional[str],\n     step: int) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| path | The path to the checkpoint. |\n| step | The step of the checkpoint. |\n\nCheckpoint component for saving the model checkpoint.\n\n## `LLMCallback` \n\n```python\nLLMCallback(self,\n     cfg: Optional[ForwardRef('Config')] = None,\n     identifier: Optional[str] = None,\n     db: Optional[ForwardRef('Datalayer')] = None,\n     llm: Optional[ForwardRef('LLM')] = None,\n     experiment_id: Optional[str] = None)\n```\n| Parameter | Description |\n|-----------|-------------|\n| cfg | The configuration to use. |\n| identifier | The identifier to use. |\n| db | The datalayer to use. |\n| llm | The LLM model to use. |\n| experiment_id | The experiment id to use. |\n\nLLM Callback for logging training process to db.\n\nThis callback will save the checkpoint to db after each epoch.\nIf the save_total_limit is set, will remove the oldest checkpoint.\n\n",
  "**`superduper.ext.anthropic.model`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/ext/anthropic/model.py)\n\n## `AnthropicCompletions` \n\n```python\nAnthropicCompletions(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     signature: 'Signature' = '*args,\n    **kwargs',\n     datatype: 'EncoderArg' = None,\n     output_schema: 't.Optional[Schema]' = None,\n     flatten: 'bool' = False,\n     model_update_kwargs: 't.Dict' = None,\n     predict_kwargs: 't.Dict' = None,\n     compute_kwargs: 't.Dict' = None,\n     validation: 't.Optional[Validation]' = None,\n     metric_values: 't.Dict' = None,\n     model: 't.Optional[str]' = None,\n     max_batch_size: 'int' = 8,\n     client_kwargs: Dict[str,\n     Any] = None,\n     prompt: str = '') -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| signature | Model signature. |\n| datatype | DataType instance. |\n| output_schema | Output schema (mapping of encoders). |\n| flatten | Flatten the model outputs. |\n| model_update_kwargs | The kwargs to use for model update. |\n| predict_kwargs | Additional arguments to use at prediction time. |\n| compute_kwargs | Kwargs used for compute backend job submit. Example (Ray backend): compute_kwargs = dict(resources=...). |\n| validation | The validation ``Dataset`` instances to use. |\n| metric_values | The metrics to evaluate on. |\n| model | The Model to use, e.g. ``'text-embedding-ada-002'`` |\n| max_batch_size | Maximum  batch size. |\n| client_kwargs | The keyword arguments to pass to the client. |\n| prompt | The prompt to use to seed the response. |\n\nCohere completions (chat) predictor.\n\n## `Anthropic` \n\n```python\nAnthropic(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     signature: 'Signature' = '*args,\n    **kwargs',\n     datatype: 'EncoderArg' = None,\n     output_schema: 't.Optional[Schema]' = None,\n     flatten: 'bool' = False,\n     model_update_kwargs: 't.Dict' = None,\n     predict_kwargs: 't.Dict' = None,\n     compute_kwargs: 't.Dict' = None,\n     validation: 't.Optional[Validation]' = None,\n     metric_values: 't.Dict' = None,\n     model: 't.Optional[str]' = None,\n     max_batch_size: 'int' = 8,\n     client_kwargs: Dict[str,\n     Any] = None) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| signature | Model signature. |\n| datatype | DataType instance. |\n| output_schema | Output schema (mapping of encoders). |\n| flatten | Flatten the model outputs. |\n| model_update_kwargs | The kwargs to use for model update. |\n| predict_kwargs | Additional arguments to use at prediction time. |\n| compute_kwargs | Kwargs used for compute backend job submit. Example (Ray backend): compute_kwargs = dict(resources=...). |\n| validation | The validation ``Dataset`` instances to use. |\n| metric_values | The metrics to evaluate on. |\n| model | The Model to use, e.g. ``'text-embedding-ada-002'`` |\n| max_batch_size | Maximum  batch size. |\n| client_kwargs | The keyword arguments to pass to the client. |\n\nAnthropic predictor.\n\n",
  "**`superduper.ext.jina.model`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/ext/jina/model.py)\n\n## `JinaEmbedding` \n\n```python\nJinaEmbedding(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     signature: str = 'singleton',\n     datatype: 'EncoderArg' = None,\n     output_schema: 't.Optional[Schema]' = None,\n     flatten: 'bool' = False,\n     model_update_kwargs: 't.Dict' = None,\n     predict_kwargs: 't.Dict' = None,\n     compute_kwargs: 't.Dict' = None,\n     validation: 't.Optional[Validation]' = None,\n     metric_values: 't.Dict' = None,\n     model: 't.Optional[str]' = None,\n     max_batch_size: 'int' = 8,\n     api_key: Optional[str] = None,\n     batch_size: int = 100,\n     shape: Optional[Sequence[int]] = None) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| signature | Model signature. |\n| datatype | DataType instance. |\n| output_schema | Output schema (mapping of encoders). |\n| flatten | Flatten the model outputs. |\n| model_update_kwargs | The kwargs to use for model update. |\n| predict_kwargs | Additional arguments to use at prediction time. |\n| compute_kwargs | Kwargs used for compute backend job submit. Example (Ray backend): compute_kwargs = dict(resources=...). |\n| validation | The validation ``Dataset`` instances to use. |\n| metric_values | The metrics to evaluate on. |\n| model | The Model to use, e.g. ``'text-embedding-ada-002'`` |\n| max_batch_size | Maximum  batch size. |\n| api_key | The API key to use for the predicto |\n| batch_size | The batch size to use for the predictor. |\n| shape | The shape of the embedding as ``tuple``. If not provided, it will be obtained by sending a simple query to the API |\n\nJina embedding predictor.\n\n## `Jina` \n\n```python\nJina(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     signature: 'Signature' = '*args,\n    **kwargs',\n     datatype: 'EncoderArg' = None,\n     output_schema: 't.Optional[Schema]' = None,\n     flatten: 'bool' = False,\n     model_update_kwargs: 't.Dict' = None,\n     predict_kwargs: 't.Dict' = None,\n     compute_kwargs: 't.Dict' = None,\n     validation: 't.Optional[Validation]' = None,\n     metric_values: 't.Dict' = None,\n     model: 't.Optional[str]' = None,\n     max_batch_size: 'int' = 8,\n     api_key: Optional[str] = None) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| signature | Model signature. |\n| datatype | DataType instance. |\n| output_schema | Output schema (mapping of encoders). |\n| flatten | Flatten the model outputs. |\n| model_update_kwargs | The kwargs to use for model update. |\n| predict_kwargs | Additional arguments to use at prediction time. |\n| compute_kwargs | Kwargs used for compute backend job submit. Example (Ray backend): compute_kwargs = dict(resources=...). |\n| validation | The validation ``Dataset`` instances to use. |\n| metric_values | The metrics to evaluate on. |\n| model | The Model to use, e.g. ``'text-embedding-ada-002'`` |\n| max_batch_size | Maximum  batch size. |\n| api_key | The API key to use for the predicto |\n\nCohere predictor.\n\n",
  "**`superduper.ext.jina.client`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/ext/jina/client.py)\n\n## `JinaAPIClient` \n\n```python\nJinaAPIClient(self,\n     api_key: Optional[str] = None,\n     model_name: str = 'jina-embeddings-v2-base-en')\n```\n| Parameter | Description |\n|-----------|-------------|\n| api_key | The Jina API key. It can be explicitly provided or automatically read from the environment variable JINA_API_KEY (recommended). |\n| model_name | The name of the Jina model to use. Check the list of available models on `https://jina.ai/embeddings/` |\n\nA client for the Jina Embedding platform.\n\nCreate a JinaAPIClient to provide an interface to encode using\nJina Embedding platform sync and async.\n\n",
  "**`superduper.ext.openai.model`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/ext/openai/model.py)\n\n## `OpenAIChatCompletion` \n\n```python\nOpenAIChatCompletion(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     signature: str = 'singleton',\n     datatype: 'EncoderArg' = None,\n     output_schema: 't.Optional[Schema]' = None,\n     flatten: 'bool' = False,\n     model_update_kwargs: 't.Dict' = None,\n     predict_kwargs: 't.Dict' = None,\n     compute_kwargs: 't.Dict' = None,\n     validation: 't.Optional[Validation]' = None,\n     metric_values: 't.Dict' = None,\n     model: 't.Optional[str]' = None,\n     max_batch_size: 'int' = 8,\n     openai_api_key: Optional[str] = None,\n     openai_api_base: Optional[str] = None,\n     client_kwargs: Optional[dict] = None,\n     batch_size: int = 1,\n     prompt: str = '') -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| signature | Model signature. |\n| datatype | DataType instance. |\n| output_schema | Output schema (mapping of encoders). |\n| flatten | Flatten the model outputs. |\n| model_update_kwargs | The kwargs to use for model update. |\n| predict_kwargs | Additional arguments to use at prediction time. |\n| compute_kwargs | Kwargs used for compute backend job submit. Example (Ray backend): compute_kwargs = dict(resources=...). |\n| validation | The validation ``Dataset`` instances to use. |\n| metric_values | The metrics to evaluate on. |\n| model | The Model to use, e.g. ``'text-embedding-ada-002'`` |\n| max_batch_size | Maximum  batch size. |\n| openai_api_key | The OpenAI API key. |\n| openai_api_base | The server to use for requests. |\n| client_kwargs | The kwargs to be passed to OpenAI |\n| batch_size | The batch size to use. |\n| prompt | The prompt to use to seed the response. |\n\nOpenAI chat completion predictor.\n\n## `OpenAIEmbedding` \n\n```python\nOpenAIEmbedding(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     signature: str = 'singleton',\n     datatype: 'EncoderArg' = None,\n     output_schema: 't.Optional[Schema]' = None,\n     flatten: 'bool' = False,\n     model_update_kwargs: 't.Dict' = None,\n     predict_kwargs: 't.Dict' = None,\n     compute_kwargs: 't.Dict' = None,\n     validation: 't.Optional[Validation]' = None,\n     metric_values: 't.Dict' = None,\n     model: 't.Optional[str]' = None,\n     max_batch_size: 'int' = 8,\n     openai_api_key: Optional[str] = None,\n     openai_api_base: Optional[str] = None,\n     client_kwargs: Optional[dict] = None,\n     shape: Optional[Sequence[int]] = None,\n     batch_size: int = 100) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| signature | Model signature. |\n| datatype | DataType instance. |\n| output_schema | Output schema (mapping of encoders). |\n| flatten | Flatten the model outputs. |\n| model_update_kwargs | The kwargs to use for model update. |\n| predict_kwargs | Additional arguments to use at prediction time. |\n| compute_kwargs | Kwargs used for compute backend job submit. Example (Ray backend): compute_kwargs = dict(resources=...). |\n| validation | The validation ``Dataset`` instances to use. |\n| metric_values | The metrics to evaluate on. |\n| model | The Model to use, e.g. ``'text-embedding-ada-002'`` |\n| max_batch_size | Maximum  batch size. |\n| openai_api_key | The OpenAI API key. |\n| openai_api_base | The server to use for requests. |\n| client_kwargs | The kwargs to be passed to OpenAI |\n| shape | The shape as ``tuple`` of the embedding. |\n| batch_size | The batch size to use. |\n\nOpenAI embedding predictor.\n\n## `OpenAIAudioTranscription` \n\n```python\nOpenAIAudioTranscription(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     signature: 'Signature' = '*args,\n    **kwargs',\n     datatype: 'EncoderArg' = None,\n     output_schema: 't.Optional[Schema]' = None,\n     flatten: 'bool' = False,\n     model_update_kwargs: 't.Dict' = None,\n     predict_kwargs: 't.Dict' = None,\n     compute_kwargs: 't.Dict' = None,\n     validation: 't.Optional[Validation]' = None,\n     metric_values: 't.Dict' = None,\n     model: 't.Optional[str]' = None,\n     max_batch_size: 'int' = 8,\n     openai_api_key: Optional[str] = None,\n     openai_api_base: Optional[str] = None,\n     client_kwargs: Optional[dict] = None,\n     takes_context: bool = True,\n     prompt: str = '') -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| signature | Model signature. |\n| datatype | DataType instance. |\n| output_schema | Output schema (mapping of encoders). |\n| flatten | Flatten the model outputs. |\n| model_update_kwargs | The kwargs to use for model update. |\n| predict_kwargs | Additional arguments to use at prediction time. |\n| compute_kwargs | Kwargs used for compute backend job submit. Example (Ray backend): compute_kwargs = dict(resources=...). |\n| validation | The validation ``Dataset`` instances to use. |\n| metric_values | The metrics to evaluate on. |\n| model | The Model to use, e.g. ``'text-embedding-ada-002'`` |\n| max_batch_size | Maximum  batch size. |\n| openai_api_key | The OpenAI API key. |\n| openai_api_base | The server to use for requests. |\n| client_kwargs | The kwargs to be passed to OpenAI |\n| takes_context | Whether the model takes context into account. |\n| prompt | The prompt to guide the model's style. |\n\nOpenAI audio transcription predictor.\n\nThe prompt should contain the `\"context\"` format variable.\n\n## `OpenAIAudioTranslation` \n\n```python\nOpenAIAudioTranslation(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     signature: str = 'singleton',\n     datatype: 'EncoderArg' = None,\n     output_schema: 't.Optional[Schema]' = None,\n     flatten: 'bool' = False,\n     model_update_kwargs: 't.Dict' = None,\n     predict_kwargs: 't.Dict' = None,\n     compute_kwargs: 't.Dict' = None,\n     validation: 't.Optional[Validation]' = None,\n     metric_values: 't.Dict' = None,\n     model: 't.Optional[str]' = None,\n     max_batch_size: 'int' = 8,\n     openai_api_key: Optional[str] = None,\n     openai_api_base: Optional[str] = None,\n     client_kwargs: Optional[dict] = None,\n     takes_context: bool = True,\n     prompt: str = '',\n     batch_size: int = 1) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| signature | Model signature. |\n| datatype | DataType instance. |\n| output_schema | Output schema (mapping of encoders). |\n| flatten | Flatten the model outputs. |\n| model_update_kwargs | The kwargs to use for model update. |\n| predict_kwargs | Additional arguments to use at prediction time. |\n| compute_kwargs | Kwargs used for compute backend job submit. Example (Ray backend): compute_kwargs = dict(resources=...). |\n| validation | The validation ``Dataset`` instances to use. |\n| metric_values | The metrics to evaluate on. |\n| model | The Model to use, e.g. ``'text-embedding-ada-002'`` |\n| max_batch_size | Maximum  batch size. |\n| openai_api_key | The OpenAI API key. |\n| openai_api_base | The server to use for requests. |\n| client_kwargs | The kwargs to be passed to OpenAI |\n| takes_context | Whether the model takes context into account. |\n| prompt | The prompt to guide the model's style. |\n| batch_size | The batch size to use. |\n\nOpenAI audio translation predictor.\n\nThe prompt should contain the `\"context\"` format variable.\n\n## `OpenAIImageCreation` \n\n```python\nOpenAIImageCreation(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     signature: str = 'singleton',\n     datatype: 'EncoderArg' = None,\n     output_schema: 't.Optional[Schema]' = None,\n     flatten: 'bool' = False,\n     model_update_kwargs: 't.Dict' = None,\n     predict_kwargs: 't.Dict' = None,\n     compute_kwargs: 't.Dict' = None,\n     validation: 't.Optional[Validation]' = None,\n     metric_values: 't.Dict' = None,\n     model: 't.Optional[str]' = None,\n     max_batch_size: 'int' = 8,\n     openai_api_key: Optional[str] = None,\n     openai_api_base: Optional[str] = None,\n     client_kwargs: Optional[dict] = None,\n     takes_context: bool = True,\n     prompt: str = '',\n     n: int = 1,\n     response_format: str = 'b64_json') -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| signature | Model signature. |\n| datatype | DataType instance. |\n| output_schema | Output schema (mapping of encoders). |\n| flatten | Flatten the model outputs. |\n| model_update_kwargs | The kwargs to use for model update. |\n| predict_kwargs | Additional arguments to use at prediction time. |\n| compute_kwargs | Kwargs used for compute backend job submit. Example (Ray backend): compute_kwargs = dict(resources=...). |\n| validation | The validation ``Dataset`` instances to use. |\n| metric_values | The metrics to evaluate on. |\n| model | The Model to use, e.g. ``'text-embedding-ada-002'`` |\n| max_batch_size | Maximum  batch size. |\n| openai_api_key | The OpenAI API key. |\n| openai_api_base | The server to use for requests. |\n| client_kwargs | The kwargs to be passed to OpenAI |\n| takes_context | Whether the model takes context into account. |\n| prompt | The prompt to use to seed the response. |\n| n | The number of images to generate. |\n| response_format | The response format to use. |\n\nOpenAI image creation predictor.\n\n## `OpenAIImageEdit` \n\n```python\nOpenAIImageEdit(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     signature: 'Signature' = '*args,\n    **kwargs',\n     datatype: 'EncoderArg' = None,\n     output_schema: 't.Optional[Schema]' = None,\n     flatten: 'bool' = False,\n     model_update_kwargs: 't.Dict' = None,\n     predict_kwargs: 't.Dict' = None,\n     compute_kwargs: 't.Dict' = None,\n     validation: 't.Optional[Validation]' = None,\n     metric_values: 't.Dict' = None,\n     model: 't.Optional[str]' = None,\n     max_batch_size: 'int' = 8,\n     openai_api_key: Optional[str] = None,\n     openai_api_base: Optional[str] = None,\n     client_kwargs: Optional[dict] = None,\n     takes_context: bool = True,\n     prompt: str = '',\n     response_format: str = 'b64_json',\n     n: int = 1) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| signature | Model signature. |\n| datatype | DataType instance. |\n| output_schema | Output schema (mapping of encoders). |\n| flatten | Flatten the model outputs. |\n| model_update_kwargs | The kwargs to use for model update. |\n| predict_kwargs | Additional arguments to use at prediction time. |\n| compute_kwargs | Kwargs used for compute backend job submit. Example (Ray backend): compute_kwargs = dict(resources=...). |\n| validation | The validation ``Dataset`` instances to use. |\n| metric_values | The metrics to evaluate on. |\n| model | The Model to use, e.g. ``'text-embedding-ada-002'`` |\n| max_batch_size | Maximum  batch size. |\n| openai_api_key | The OpenAI API key. |\n| openai_api_base | The server to use for requests. |\n| client_kwargs | The kwargs to be passed to OpenAI |\n| takes_context | Whether the model takes context into account. |\n| prompt | The prompt to use to seed the response. |\n| response_format | The response format to use. |\n| n | The number of images to generate. |\n\nOpenAI image edit predictor.\n\n",
  "**`superduper.components.stack`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper.components/stack.py)\n\n## `Stack` \n\n```python\nStack(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     components: Sequence[superduper.components.component.Component]) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| components | List of components to stack together and add to database. |\n\nA placeholder to hold list of components under a namespace.\n\n",
  "**`superduper.components.plugin`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper.components/plugin.py)\n\n## `Plugin` \n\n```python\nPlugin(self,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: None = None,\n     *,\n     identifier: str = '',\n     plugins: \"t.Optional[t.List['Plugin']]\" = None,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     path: str,\n     cache_path: str = '.superduper/plugins') -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Unique identifier for the plugin. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| plugins | A list of plugins to be used in the component. |\n| path | Path to the plugin package or module. |\n| cache_path | Path to the cache directory where the plugin will be stored. |\n\nPlugin component allows to install and use external python packages as plugins.\n\n",
  "**`superduper.components.template`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper.components/template.py)\n\n## `Template` \n\n```python\nTemplate(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     component: Union[superduper.components.component.Component,\n     Dict],\n     info: Optional[Dict] = None,\n     _component_blobs: Union[Dict,\n     bytes,\n     NoneType] = None) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| component | Template component with variables. |\n| info | Info. |\n| _component_blobs | Blobs in `Template.component` NOTE: This is only for internal use. |\n\nApplication template component.\n\n",
  "**`superduper.components.metric`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper.components/metric.py)\n\n## `Metric` \n\n```python\nMetric(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     object: Callable) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| object | Callable or an Artifact to be applied to the data. |\n\nMetric base object used to evaluate performance on a dataset.\n\nThese objects are callable and are applied row-wise to the data, and averaged.\n\n",
  "**`superduper.components.application`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper.components/application.py)\n\n## `Application` \n\n```python\nApplication(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     template: Union[superduper.components.template.Template,\n     str] = None,\n     kwargs: Dict) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| template | Template. |\n| kwargs | Keyword arguments passed to `template`. |\n\nApplication built from template.\n\n",
  "**`superduper.components.dataset`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper.components/dataset.py)\n\n## `Dataset` \n\n```python\nDataset(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: None = None,\n     *,\n     upstream: \"t.Optional[t.List['Component']]\" = None,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     select: 't.Optional[Query]' = None,\n     sample_size: 't.Optional[int]' = None,\n     random_seed: 't.Optional[int]' = None,\n     creation_date: 't.Optional[str]' = None,\n     raw_data: 't.Optional[t.Sequence[t.Any]]' = None,\n     pin: 'bool' = False) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| select | A query to select the documents for the dataset. |\n| sample_size | The number of documents to sample from the query. |\n| random_seed | The random seed to use for sampling. |\n| creation_date | The date the dataset was created. |\n| raw_data | The raw data for the dataset. |\n| pin | Whether to pin the dataset. If True, the dataset will load the datas from the database every time. If False, the dataset will cache the datas after we apply to db. |\n\nA dataset is an immutable collection of documents.\n\n## `DataInit` \n\n```python\nDataInit(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: None = None,\n     *,\n     upstream: \"t.Optional[t.List['Component']]\" = None,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     data: 't.List[t.Dict]',\n     table: 'str') -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n\nDataInit(identifier: str, db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None, uuid: None = None, *, upstream: \"t.Optional[t.List['Component']]\" = None, artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None, data: 't.List[t.Dict]', table: 'str')\n\n",
  "**`superduper.components.model`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper.components/model.py)\n\n## `codemodel` \n\n```python\ncodemodel(item: 't.Optional[t.Callable]' = None,\n     identifier: 't.Optional[str]' = None,\n     datatype=None,\n     model_update_kwargs: 't.Optional[t.Dict]' = None,\n     flatten: 'bool' = False,\n     output_schema: 't.Optional[Schema]' = None)\n```\n| Parameter | Description |\n|-----------|-------------|\n| item | Callable to wrap with `CodeModel`. |\n| identifier | Identifier for the `CodeModel`. |\n| datatype | Datatype for the model outputs. |\n| model_update_kwargs | Dictionary to define update kwargs. |\n| flatten | If `True`, flatten the outputs and save. |\n| output_schema | Schema for the model outputs. |\n\nDecorator to wrap a function with `CodeModel`.\n\nWhen a function is wrapped with this decorator,\nthe function comes out as a `CodeModel`.\n\n## `model` \n\n```python\nmodel(item: 't.Optional[t.Callable]' = None,\n     identifier: 't.Optional[str]' = None,\n     datatype=None,\n     model_update_kwargs: 't.Optional[t.Dict]' = None,\n     flatten: 'bool' = False,\n     output_schema: 't.Optional[Schema]' = None)\n```\n| Parameter | Description |\n|-----------|-------------|\n| item | Callable to wrap with `ObjectModel`. |\n| identifier | Identifier for the `ObjectModel`. |\n| datatype | Datatype for the model outputs. |\n| model_update_kwargs | Dictionary to define update kwargs. |\n| flatten | If `True`, flatten the outputs and save. |\n| output_schema | Schema for the model outputs. |\n\nDecorator to wrap a function with `ObjectModel`.\n\nWhen a function is wrapped with this decorator,\nthe function comes out as an `ObjectModel`.\n\n## `CodeModel` \n\n```python\nCodeModel(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     signature: 'Signature' = '*args,\n    **kwargs',\n     datatype: 'EncoderArg' = None,\n     output_schema: 't.Optional[Schema]' = None,\n     flatten: 'bool' = False,\n     model_update_kwargs: 't.Dict' = None,\n     predict_kwargs: 't.Dict' = None,\n     compute_kwargs: 't.Dict' = None,\n     validation: 't.Optional[Validation]' = None,\n     metric_values: 't.Dict' = None,\n     num_workers: 'int' = 0,\n     object: 'Code') -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| signature | Model signature. |\n| datatype | DataType instance. |\n| output_schema | Output schema (mapping of encoders). |\n| flatten | Flatten the model outputs. |\n| model_update_kwargs | The kwargs to use for model update. |\n| predict_kwargs | Additional arguments to use at prediction time. |\n| compute_kwargs | Kwargs used for compute backend job submit. Example (Ray backend): compute_kwargs = dict(resources=...). |\n| validation | The validation ``Dataset`` instances to use. |\n| metric_values | The metrics to evaluate on. |\n| num_workers | Number of workers to use for parallel processing |\n| object | Code object |\n\nModel component which stores a code object.\n\n## `Model` \n\n```python\nModel(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     signature: 'Signature' = '*args,\n    **kwargs',\n     datatype: 'EncoderArg' = None,\n     output_schema: 't.Optional[Schema]' = None,\n     flatten: 'bool' = False,\n     model_update_kwargs: 't.Dict' = None,\n     predict_kwargs: 't.Dict' = None,\n     compute_kwargs: 't.Dict' = None,\n     validation: 't.Optional[Validation]' = None,\n     metric_values: 't.Dict' = None) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| signature | Model signature. |\n| datatype | DataType instance. |\n| output_schema | Output schema (mapping of encoders). |\n| flatten | Flatten the model outputs. |\n| model_update_kwargs | The kwargs to use for model update. |\n| predict_kwargs | Additional arguments to use at prediction time. |\n| compute_kwargs | Kwargs used for compute backend job submit. Example (Ray backend): compute_kwargs = dict(resources=...). |\n| validation | The validation ``Dataset`` instances to use. |\n| metric_values | The metrics to evaluate on. |\n\nBase class for components which can predict.\n\n## `ObjectModel` \n\n```python\nObjectModel(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     signature: 'Signature' = '*args,\n    **kwargs',\n     datatype: 'EncoderArg' = None,\n     output_schema: 't.Optional[Schema]' = None,\n     flatten: 'bool' = False,\n     model_update_kwargs: 't.Dict' = None,\n     predict_kwargs: 't.Dict' = None,\n     compute_kwargs: 't.Dict' = None,\n     validation: 't.Optional[Validation]' = None,\n     metric_values: 't.Dict' = None,\n     num_workers: 'int' = 0,\n     object: 't.Any') -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| signature | Model signature. |\n| datatype | DataType instance. |\n| output_schema | Output schema (mapping of encoders). |\n| flatten | Flatten the model outputs. |\n| model_update_kwargs | The kwargs to use for model update. |\n| predict_kwargs | Additional arguments to use at prediction time. |\n| compute_kwargs | Kwargs used for compute backend job submit. Example (Ray backend): compute_kwargs = dict(resources=...). |\n| validation | The validation ``Dataset`` instances to use. |\n| metric_values | The metrics to evaluate on. |\n| num_workers | Number of workers to use for parallel processing |\n| object | Model/ computation object |\n\nModel component which wraps a Model to become serializable.\n\n```python\n# Example:\n# -------\nm = ObjectModel('test', lambda x: x + 2)\nm.predict(2)\n# 4\n```\n\n## `QueryModel` \n\n```python\nQueryModel(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     signature: 'Signature' = '**kwargs',\n     datatype: 'EncoderArg' = None,\n     output_schema: 't.Optional[Schema]' = None,\n     flatten: 'bool' = False,\n     model_update_kwargs: 't.Dict' = None,\n     predict_kwargs: 't.Dict' = None,\n     compute_kwargs: 't.Dict' = None,\n     validation: 't.Optional[Validation]' = None,\n     metric_values: 't.Dict' = None,\n     preprocess: 't.Optional[t.Callable]' = None,\n     postprocess: 't.Optional[t.Union[t.Callable]]' = None,\n     select: 'Query') -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| signature | Model signature. |\n| datatype | DataType instance. |\n| output_schema | Output schema (mapping of encoders). |\n| flatten | Flatten the model outputs. |\n| model_update_kwargs | The kwargs to use for model update. |\n| predict_kwargs | Additional arguments to use at prediction time. |\n| compute_kwargs | Kwargs used for compute backend job submit. Example (Ray backend): compute_kwargs = dict(resources=...). |\n| validation | The validation ``Dataset`` instances to use. |\n| metric_values | The metrics to evaluate on. |\n| preprocess | Preprocess callable |\n| postprocess | Postprocess callable |\n| select | query used to find data (can include `like`) |\n\nQueryModel component.\n\nModel which can be used to query data and return those\nprecomputed queries as Results.\n\n## `Validation` \n\n```python\nValidation(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     metrics: 't.Sequence[Metric]' = (),\n     key: 't.Optional[ModelInputType]' = None,\n     datasets: 't.Sequence[Dataset]' = ()) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| metrics | List of metrics for validation |\n| key | Model input type key |\n| datasets | Sequence of dataset. |\n\ncomponent which represents Validation definition.\n\n## `Mapping` \n\n```python\nMapping(self,\n     mapping: 'ModelInputType',\n     signature: 'Signature')\n```\n| Parameter | Description |\n|-----------|-------------|\n| mapping | Mapping that represents a collection or table map. |\n| signature | Signature for the model. |\n\nClass to represent model inputs for mapping database collections or tables.\n\n## `APIBaseModel` \n\n```python\nAPIBaseModel(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     signature: 'Signature' = '*args,\n    **kwargs',\n     datatype: 'EncoderArg' = None,\n     output_schema: 't.Optional[Schema]' = None,\n     flatten: 'bool' = False,\n     model_update_kwargs: 't.Dict' = None,\n     predict_kwargs: 't.Dict' = None,\n     compute_kwargs: 't.Dict' = None,\n     validation: 't.Optional[Validation]' = None,\n     metric_values: 't.Dict' = None,\n     model: 't.Optional[str]' = None,\n     max_batch_size: 'int' = 8) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| signature | Model signature. |\n| datatype | DataType instance. |\n| output_schema | Output schema (mapping of encoders). |\n| flatten | Flatten the model outputs. |\n| model_update_kwargs | The kwargs to use for model update. |\n| predict_kwargs | Additional arguments to use at prediction time. |\n| compute_kwargs | Kwargs used for compute backend job submit. Example (Ray backend): compute_kwargs = dict(resources=...). |\n| validation | The validation ``Dataset`` instances to use. |\n| metric_values | The metrics to evaluate on. |\n| model | The Model to use, e.g. ``'text-embedding-ada-002'`` |\n| max_batch_size | Maximum  batch size. |\n\nAPIBaseModel component which is used to make the type of API request.\n\n## `APIModel` \n\n```python\nAPIModel(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     signature: 'Signature' = '*args,\n    **kwargs',\n     datatype: 'EncoderArg' = None,\n     output_schema: 't.Optional[Schema]' = None,\n     flatten: 'bool' = False,\n     model_update_kwargs: 't.Dict' = None,\n     predict_kwargs: 't.Dict' = None,\n     compute_kwargs: 't.Dict' = None,\n     validation: 't.Optional[Validation]' = None,\n     metric_values: 't.Dict' = None,\n     model: 't.Optional[str]' = None,\n     max_batch_size: 'int' = 8,\n     url: 'str',\n     postprocess: 't.Optional[t.Callable]' = None) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| signature | Model signature. |\n| datatype | DataType instance. |\n| output_schema | Output schema (mapping of encoders). |\n| flatten | Flatten the model outputs. |\n| model_update_kwargs | The kwargs to use for model update. |\n| predict_kwargs | Additional arguments to use at prediction time. |\n| compute_kwargs | Kwargs used for compute backend job submit. Example (Ray backend): compute_kwargs = dict(resources=...). |\n| validation | The validation ``Dataset`` instances to use. |\n| metric_values | The metrics to evaluate on. |\n| model | The Model to use, e.g. ``'text-embedding-ada-002'`` |\n| max_batch_size | Maximum  batch size. |\n| url | The url to use for the API request |\n| postprocess | Postprocess function to use on the output of the API request |\n\nAPIModel component which is used to make the type of API request.\n\n## `CallableInputs` \n\n```python\nCallableInputs(self,\n     fn,\n     predict_kwargs: 't.Dict' = {})\n```\n| Parameter | Description |\n|-----------|-------------|\n| fn | Callable function |\n| predict_kwargs | (optional) predict_kwargs if provided in Model initiation |\n\nClass represents the model callable args and kwargs.\n\n## `IndexableNode` \n\n```python\nIndexableNode(self,\n     types: 't.Sequence[t.Type]') -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| types | Sequence of types |\n\nBase indexable node for `ObjectModel`.\n\n## `Inputs` \n\n```python\nInputs(self,\n     params)\n```\n| Parameter | Description |\n|-----------|-------------|\n| params | List of parameters of the Model object |\n\nBase class to represent the model args and kwargs.\n\n## `SequentialModel` \n\n```python\nSequentialModel(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     signature: 'Signature' = '*args,\n    **kwargs',\n     datatype: 'EncoderArg' = None,\n     output_schema: 't.Optional[Schema]' = None,\n     flatten: 'bool' = False,\n     model_update_kwargs: 't.Dict' = None,\n     predict_kwargs: 't.Dict' = None,\n     compute_kwargs: 't.Dict' = None,\n     validation: 't.Optional[Validation]' = None,\n     metric_values: 't.Dict' = None,\n     models: 't.List[Model]') -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| signature | Model signature. |\n| datatype | DataType instance. |\n| output_schema | Output schema (mapping of encoders). |\n| flatten | Flatten the model outputs. |\n| model_update_kwargs | The kwargs to use for model update. |\n| predict_kwargs | Additional arguments to use at prediction time. |\n| compute_kwargs | Kwargs used for compute backend job submit. Example (Ray backend): compute_kwargs = dict(resources=...). |\n| validation | The validation ``Dataset`` instances to use. |\n| metric_values | The metrics to evaluate on. |\n| models | A list of models to use |\n\nSequential model component which wraps a model to become serializable.\n\n## `Trainer` \n\n```python\nTrainer(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     key: 'ModelInputType',\n     select: 'Query',\n     transform: 't.Optional[t.Callable]' = None,\n     metric_values: 't.Dict' = None,\n     signature: 'Signature' = '*args',\n     data_prefetch: 'bool' = False,\n     prefetch_size: 'int' = 1000,\n     prefetch_factor: 'int' = 100,\n     in_memory: 'bool' = True,\n     compute_kwargs: 't.Dict' = None) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| key | Model input type key. |\n| select | Model select query for training. |\n| transform | (optional) transform callable. |\n| metric_values | Dictionary for metric defaults. |\n| signature | Model signature. |\n| data_prefetch | Boolean for prefetching data before forward pass. |\n| prefetch_size | Prefetch batch size. |\n| prefetch_factor | Prefetch factor for data prefetching. |\n| in_memory | If training in memory. |\n| compute_kwargs | Kwargs for compute backend. |\n\nTrainer component to train a model.\n\nTraining configuration object, containing all settings necessary for a particular\nlearning task use-case to be serialized and initiated. The object is ``callable``\nand returns a class which may be invoked to apply training.\n\n",
  "**`superduper.components.listener`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper.components/listener.py)\n\n## `Listener` \n\n```python\nListener(self,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     identifier: str = '',\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     key: Union[str,\n     List[str],\n     Tuple[List[str],\n     Dict[str,\n     str]]],\n     model: superduper.components.model.Model,\n     select: superduper.backends.base.query.Query,\n     active: bool = True,\n     predict_kwargs: Optional[Dict] = None) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | A string used to identify the model. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| key | Key to be bound to the model. |\n| model | Model for processing data. |\n| select | Object for selecting which data is processed. |\n| active | Toggle to ``False`` to deactivate change data triggering. |\n| predict_kwargs | Keyword arguments to self.model.predict(). |\n\nListener component.\n\nListener object which is used to process a column/key of a collection or table,\nand store the outputs.\n\n",
  "**`superduper.components.schema`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper.components/schema.py)\n\n## `get_schema` \n\n```python\nget_schema(db,\n     schema: Union[superduper.components.schema.Schema,\n     str]) -> Optional[superduper.components.schema.Schema]\n```\n| Parameter | Description |\n|-----------|-------------|\n| db | Datalayer instance. |\n| schema | Schema to get. If a string, it will be loaded from the database. |\n\nHandle schema caching and loading.\n\n## `Schema` \n\n```python\nSchema(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     fields: Mapping[str,\n     superduper.components.datatype.DataType]) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| fields | A mapping of field names to types or `Encoders` |\n\nA component carrying the `DataType` of columns.\n\n",
  "**`superduper.components.datatype`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper.components/datatype.py)\n\n## `pickle_decode` \n\n```python\npickle_decode(b: bytes,\n     info: Optional[Dict] = None) -> Any\n```\n| Parameter | Description |\n|-----------|-------------|\n| b | The bytes to decode. |\n| info | Optional information. |\n\nDecodes bytes using pickle.\n\n## `pickle_encode` \n\n```python\npickle_encode(object: Any,\n     info: Optional[Dict] = None) -> bytes\n```\n| Parameter | Description |\n|-----------|-------------|\n| object | The object to encode. |\n| info | Optional information. |\n\nEncodes an object using pickle.\n\n## `base64_to_bytes` \n\n```python\nbase64_to_bytes(encoded)\n```\n| Parameter | Description |\n|-----------|-------------|\n| encoded | The base64 encoded string. |\n\nDecodes a base64 encoded string.\n\n## `bytes_to_base64` \n\n```python\nbytes_to_base64(bytes)\n```\n| Parameter | Description |\n|-----------|-------------|\n| bytes | The bytes to convert. |\n\nConverts bytes to base64.\n\n## `dill_decode` \n\n```python\ndill_decode(b: bytes,\n     info: Optional[Dict] = None) -> Any\n```\n| Parameter | Description |\n|-----------|-------------|\n| b | The bytes to decode. |\n| info | Optional information. |\n\nDecodes bytes using dill.\n\n## `dill_encode` \n\n```python\ndill_encode(object: Any,\n     info: Optional[Dict] = None) -> bytes\n```\n| Parameter | Description |\n|-----------|-------------|\n| object | The object to encode. |\n| info | Optional information. |\n\nEncodes an object using dill.\n\n## `encode_torch_state_dict` \n\n```python\nencode_torch_state_dict(module,\n     info)\n```\n| Parameter | Description |\n|-----------|-------------|\n| module | Module. |\n| info | Information. |\n\nEncode torch state dictionary.\n\n## `file_check` \n\n```python\nfile_check(path: Any,\n     info: Optional[Dict] = None) -> str\n```\n| Parameter | Description |\n|-----------|-------------|\n| path | The file path to check. |\n| info | Optional information. |\n\nChecks if a file path exists.\n\n## `get_serializer` \n\n```python\nget_serializer(identifier: str,\n     method: str,\n     encodable: str,\n     db: Optional[ForwardRef('Datalayer')] = None)\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | The identifier of the serializer. |\n| method | The method of the serializer. |\n| encodable | The type of encodable object. |\n| db | The Datalayer instance. |\n\nGet a serializer.\n\n## `json_decode` \n\n```python\njson_decode(b: str,\n     info: Optional[Dict] = None) -> Any\n```\n| Parameter | Description |\n|-----------|-------------|\n| b | The JSON string to decode |\n| info | Optional information |\n\nDecode the JSON string to an dict.\n\n## `json_encode` \n\n```python\njson_encode(object: Any,\n     info: Optional[Dict] = None) -> str\n```\n| Parameter | Description |\n|-----------|-------------|\n| object | The object to encode |\n| info | Optional information |\n\nEncode the dict to a JSON string.\n\n## `torch_decode` \n\n```python\ntorch_decode(b: bytes,\n     info: Optional[Dict] = None) -> Any\n```\n| Parameter | Description |\n|-----------|-------------|\n| b | The bytes to decode. |\n| info | Optional information. |\n\nDecodes bytes to a torch model.\n\n## `torch_encode` \n\n```python\ntorch_encode(object: Any,\n     info: Optional[Dict] = None) -> bytes\n```\n| Parameter | Description |\n|-----------|-------------|\n| object | The object to encode. |\n| info | Optional information. |\n\nSaves an object in torch format.\n\n## `Encoder` \n\n```python\nEncoder(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     encoder: Optional[Callable] = None,\n     decoder: Optional[Callable] = None,\n     info: Optional[Dict] = None,\n     shape: Optional[Sequence] = None,\n     directory: Optional[str] = None,\n     encodable: str = 'encodable',\n     bytes_encoding: Optional[str] = <BytesEncoding.BYTES: 'Bytes'>,\n     intermediate_type: Optional[str] = 'bytes',\n     media_type: Optional[str] = None) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| encoder | A callable that converts an encodable object of this encoder to bytes. |\n| decoder | A callable that converts bytes to an encodable object of this encoder. |\n| info | An optional information dictionary. |\n| shape | The shape of the data. |\n| directory | The directory to store file types. |\n| encodable | The type of encodable object ('encodable', 'lazy_artifact', or 'file'). |\n| bytes_encoding | The encoding type for bytes ('base64' or 'bytes'). |\n| intermediate_type | Type of the intermediate data [IntermediateType.BYTES, IntermediateType.STRING] |\n| media_type | The media type. |\n\nA data type component that defines how data is encoded and decoded.\n\n## `Artifact` \n\n```python\nArtifact(self,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     x: Any = <EMPTY>,\n     *,\n     identifier: str = '',\n     file_id: Optional[str] = None,\n     datatype: superduper.components.datatype.DataType,\n     uri: Optional[str] = None,\n     sha1: Optional[str] = None) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| file_id | unique-id of the content |\n| datatype | The datatype of the content. |\n| uri | URI of the content, if any. |\n| sha1 | SHA1 hash of the content. |\n| x | The artifact object. |\n\nClass for representing data to be saved on disk or in the artifact-store.\n\n## `DecodeTorchStateDict` \n\n```python\nDecodeTorchStateDict(self,\n     cls)\n```\n| Parameter | Description |\n|-----------|-------------|\n| cls | Torch state cls |\n\nTorch state dictionary decoder.\n\n## `Encodable` \n\n```python\nEncodable(self,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     x: Any = <EMPTY>,\n     blob: dataclasses.InitVar[typing.Optional[bytearray]] = None,\n     *,\n     identifier: str = '',\n     file_id: Optional[str] = None,\n     datatype: superduper.components.datatype.DataType,\n     uri: Optional[str] = None,\n     sha1: Optional[str] = None) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| file_id | unique-id of the content |\n| datatype | The datatype of the content. |\n| uri | URI of the content, if any. |\n| sha1 | SHA1 hash of the content. |\n| x | The encodable object. |\n| blob | The blob data. |\n\nClass for encoding non-Python datatypes to the database.\n\n## `File` \n\n```python\nFile(self,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     x: Any = <EMPTY>,\n     file_name: Optional[str] = None,\n     *,\n     identifier: str = '',\n     file_id: Optional[str] = None,\n     datatype: superduper.components.datatype.DataType,\n     uri: Optional[str] = None,\n     sha1: Optional[str] = None) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| file_id | unique-id of the content |\n| datatype | The datatype of the content. |\n| uri | URI of the content, if any. |\n| sha1 | SHA1 hash of the content. |\n| x | path to the file |\n| file_name | File name |\n\nData to be saved on disk and passed as a file reference.\n\n## `LazyArtifact` \n\n```python\nLazyArtifact(self,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     x: Any = <EMPTY>,\n     *,\n     identifier: str = '',\n     file_id: Optional[str] = None,\n     datatype: superduper.components.datatype.DataType,\n     uri: Optional[str] = None,\n     sha1: Optional[str] = None) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| file_id | unique-id of the content |\n| datatype | The datatype of the content. |\n| uri | URI of the content, if any. |\n| sha1 | SHA1 hash of the content. |\n| x | The artifact object. |\n\nData to be saved and loaded only when needed.\n\n## `LazyFile` \n\n```python\nLazyFile(self,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     x: Any = <EMPTY>,\n     file_name: Optional[str] = None,\n     *,\n     identifier: str = '',\n     file_id: Optional[str] = None,\n     datatype: superduper.components.datatype.DataType,\n     uri: Optional[str] = None,\n     sha1: Optional[str] = None) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| file_id | unique-id of the content |\n| datatype | The datatype of the content. |\n| uri | URI of the content, if any. |\n| sha1 | SHA1 hash of the content. |\n| x | path to the file |\n| file_name | File name |\n\nClass is used to load a file only when needed.\n\n## `Native` \n\n```python\nNative(self,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     x: Optional[Any] = None,\n     *,\n     identifier: str = '',\n     file_id: Optional[str] = None,\n     datatype: superduper.components.datatype.DataType,\n     uri: Optional[str] = None,\n     sha1: Optional[str] = None) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| file_id | unique-id of the content |\n| datatype | The datatype of the content. |\n| uri | URI of the content, if any. |\n| sha1 | SHA1 hash of the content. |\n| x | The encodable object. |\n\nClass for representing native data supported by the underlying database.\n\n",
  "**`superduper.components.table`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper.components/table.py)\n\n## `Table` \n\n```python\nTable(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     schema: superduper.components.schema.Schema,\n     primary_id: str = 'id') -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| schema | The schema of the table |\n| primary_id | The primary id of the table |\n\nA component that represents a table in a database.\n\n",
  "**`superduper.components.component`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper.components/component.py)\n\n## `ensure_initialized` \n\n```python\nensure_initialized(func)\n```\n| Parameter | Description |\n|-----------|-------------|\n| func | Decorator function. |\n\nDecorator to ensure that the model is initialized before calling the function.\n\n## `getdeepattr` \n\n```python\ngetdeepattr(obj,\n     attr)\n```\n| Parameter | Description |\n|-----------|-------------|\n| obj | Object. |\n| attr | Attribute. |\n\nGet nested attribute with dot notation.\n\n## `import_` \n\n```python\nimport_(r=None,\n     path=None,\n     db=None)\n```\n| Parameter | Description |\n|-----------|-------------|\n| r | Object to be imported. |\n| path | Components directory. |\n| db | Datalayer instance. |\n\nHelper function for importing component JSONs, YAMLs, etc.\n\n## `Component` \n\n```python\nComponent(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n\nBase class for all components in `superduper`.\n\nClass to represent `superduper` serializable entities\nthat can be saved into a database.\n\n",
  "**`superduper.components.vector_index`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper.components/vector_index.py)\n\n## `sqlvector` \n\n```python\nsqlvector(shape)\n```\n| Parameter | Description |\n|-----------|-------------|\n| shape | The shape of the vector |\n\nCreate an encoder for a vector (list of ints/ floats) of a given shape.\n\nThis is used for compatibility with SQL databases, as the default vector\n\n## `vector` \n\n```python\nvector(shape,\n     identifier: Optional[str] = None)\n```\n| Parameter | Description |\n|-----------|-------------|\n| shape | The shape of the vector |\n| identifier | The identifier of the vector |\n\nCreate an encoder for a vector (list of ints/ floats) of a given shape.\n\n## `VectorIndex` \n\n```python\nVectorIndex(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     artifacts: 'dc.InitVar[t.Optional[t.Dict]]' = None,\n     indexing_listener: superduper.components.listener.Listener,\n     compatible_listener: Optional[superduper.components.listener.Listener] = None,\n     measure: superduper.vector_search.base.VectorIndexMeasureType = <VectorIndexMeasureType.cosine: 'cosine'>,\n     metric_values: Optional[Dict] = None) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| artifacts | A dictionary of artifacts paths and `DataType` objects |\n| indexing_listener | Listener which is applied to created vectors |\n| compatible_listener | Listener which is applied to vectors to be compared |\n| measure | Measure to use for comparison |\n| metric_values | Metric values for this index |\n\nA component carrying the information to apply a vector index.\n\n## `DecodeArray` \n\n```python\nDecodeArray(self,\n     dtype)\n```\n| Parameter | Description |\n|-----------|-------------|\n| dtype | Datatype of array |\n\nClass to decode an array.\n\n## `EncodeArray` \n\n```python\nEncodeArray(self,\n     dtype)\n```\n| Parameter | Description |\n|-----------|-------------|\n| dtype | Datatype of array |\n\nClass to encode an array.\n\n",
  "**`superduper.misc`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/misc.py)\n\n## `border_msg` \n\n```python\nborder_msg(msg,\n     indent=1,\n     width=None,\n     title=None)\n```\n| Parameter | Description |\n|-----------|-------------|\n| msg | Message to print |\n| indent | Indentation of the box |\n| width | Width of the box |\n| title | Title of the box |\n\nPrint message-box with optional title.\n\n",
  "**`superduper.jobs.task_workflow`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/jobs/task_workflow.py)\n\n## `TaskWorkflow` \n\n```python\nTaskWorkflow(self,\n     database: 'Datalayer',\n     G: 'DiGraph' = None) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| database | ``DB`` instance to use |\n| G | ``networkx.DiGraph`` to use as the graph |\n\nTask workflow class.\n\nKeep a graph of jobs that need to be performed and their dependencies,\nand perform them when called.\n\n",
  "**`superduper.jobs.tasks`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/jobs/tasks.py)\n\n## `callable_job` \n\n```python\ncallable_job(cfg,\n     function_to_call,\n     args,\n     kwargs,\n     job_id,\n     dependencies=(),\n     db: Optional[ForwardRef('Datalayer')] = None)\n```\n| Parameter | Description |\n|-----------|-------------|\n| cfg | configuration |\n| function_to_call | function to call |\n| args | positional arguments to pass to the function |\n| kwargs | keyword arguments to pass to the function |\n| job_id | unique identifier for this job |\n| dependencies | other jobs that this job depends on |\n| db | datalayer to use |\n\nRun a function in the database.\n\n## `method_job` \n\n```python\nmethod_job(cfg,\n     type_id,\n     identifier,\n     method_name,\n     args,\n     kwargs,\n     job_id,\n     dependencies=(),\n     db: Optional[ForwardRef('Datalayer')] = None)\n```\n| Parameter | Description |\n|-----------|-------------|\n| cfg | user config |\n| type_id | type of component |\n| identifier | identifier of component |\n| method_name | name of method to run |\n| args | positional arguments to pass to the method |\n| kwargs | keyword arguments to pass to the method |\n| job_id | unique identifier for this job |\n| dependencies | other jobs that this job depends on |\n| db | datalayer to use |\n\nRun a method on a component in the database.\n\n",
  "**`superduper.jobs.job`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/jobs/job.py)\n\n## `job` \n\n```python\njob(f)\n```\n| Parameter | Description |\n|-----------|-------------|\n| f | function to be decorated |\n\nDecorator to create a job from a function.\n\n## `ComponentJob` \n\n```python\nComponentJob(self,\n     component_identifier: str,\n     type_id: str,\n     method_name: str,\n     args: Optional[Sequence] = None,\n     kwargs: Optional[Dict] = None,\n     compute_kwargs: Dict = {})\n```\n| Parameter | Description |\n|-----------|-------------|\n| component_identifier | unique identifier of the component |\n| type_id | type of the component |\n| method_name | name of the method to be called |\n| args | positional arguments to be passed to the method |\n| kwargs | keyword arguments to be passed to the method |\n| compute_kwargs | Arguments to use for model predict computation |\n\nJob for running a class method of a component.\n\n## `FunctionJob` \n\n```python\nFunctionJob(self,\n     callable: Callable,\n     args: Optional[Sequence] = None,\n     kwargs: Optional[Dict] = None,\n     compute_kwargs: Dict = {})\n```\n| Parameter | Description |\n|-----------|-------------|\n| callable | function to be called |\n| args | positional arguments to be passed to the function |\n| kwargs | keyword arguments to be passed to the function |\n| compute_kwargs | Arguments to use for model predict computation |\n\nJob for running a function.\n\n## `Job` \n\n```python\nJob(self,\n     args: Optional[Sequence] = None,\n     kwargs: Optional[Dict] = None,\n     compute_kwargs: Dict = {})\n```\n| Parameter | Description |\n|-----------|-------------|\n| args | positional arguments to be passed to the function or method |\n| kwargs | keyword arguments to be passed to the function or method |\n| compute_kwargs | Arguments to use for model predict computation |\n\nBase class for jobs. Jobs are used to run functions or methods on.\n\n",
  "**`superduper.vector_search.in_memory`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/vector_search/in_memory.py)\n\n## `InMemoryVectorSearcher` \n\n```python\nInMemoryVectorSearcher(self,\n     identifier: str,\n     dimensions: int,\n     h: Optional[numpy.ndarray] = None,\n     index: Optional[List[str]] = None,\n     measure: Union[str,\n     Callable] = 'cosine')\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Unique string identifier of index |\n| dimensions | Dimension of the vector embeddings |\n| h | array/ tensor of vectors |\n| index | list of IDs |\n| measure | measure to assess similarity |\n\nSimple hash-set for looking up with vector similarity.\n\n",
  "**`superduper.vector_search.base`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/vector_search/base.py)\n\n## `cosine` \n\n```python\ncosine(x,\n     y)\n```\n| Parameter | Description |\n|-----------|-------------|\n| x | numpy.ndarray |\n| y | numpy.ndarray, y should be normalized! |\n\nCosine similarity function for vector search.\n\n## `dot` \n\n```python\ndot(x,\n     y)\n```\n| Parameter | Description |\n|-----------|-------------|\n| x | numpy.ndarray |\n| y | numpy.ndarray |\n\nDot function for vector similarity search.\n\n## `l2` \n\n```python\nl2(x,\n     y)\n```\n| Parameter | Description |\n|-----------|-------------|\n| x | numpy.ndarray |\n| y | numpy.ndarray |\n\nL2 function for vector similarity search.\n\n## `BaseVectorSearcher` \n\n```python\nBaseVectorSearcher(self,\n     identifier: 'str',\n     dimensions: 'int',\n     h: 't.Optional[numpy.ndarray]' = None,\n     index: 't.Optional[t.List[str]]' = None,\n     measure: 't.Optional[str]' = None)\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Unique string identifier of index |\n| dimensions | Dimension of the vector embeddings |\n| h | Seed vectors ``numpy.ndarray`` |\n| index | list of IDs |\n| measure | measure to assess similarity |\n\nBase class for vector searchers.\n\n## `VectorItem` \n\n```python\nVectorItem(self,\n     id: 'str',\n     vector: 'numpy.ndarray') -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| id | ID of the vector |\n| vector | Vector of the item |\n\nClass for representing a vector in vector search with id and vector.\n\n",
  "**`superduper.vector_search.atlas`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/vector_search/atlas.py)\n\n## `MongoAtlasVectorSearcher` \n\n```python\nMongoAtlasVectorSearcher(self,\n     identifier: str,\n     collection: str,\n     dimensions: Optional[int] = None,\n     measure: Optional[str] = None,\n     output_path: Optional[str] = None)\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Unique string identifier of index |\n| collection | Collection name |\n| dimensions | Dimension of the vector embeddings |\n| measure | measure to assess similarity |\n| output_path | Path to the output |\n\nVector searcher implementation of atlas vector search.\n\n",
  "**`superduper.vector_search.lance`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/vector_search/lance.py)\n\n## `LanceVectorSearcher` \n\n```python\nLanceVectorSearcher(self,\n     identifier: str,\n     dimensions: int,\n     h: Optional[numpy.ndarray] = None,\n     index: Optional[List[str]] = None,\n     measure: Optional[str] = None)\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Unique string identifier of index |\n| dimensions | Dimension of the vector embeddings in the Lance dataset |\n| h | Seed vectors ``numpy.ndarray`` |\n| index | list of IDs |\n| measure | measure to assess similarity |\n\nImplementation of a vector index using the ``lance`` library.\n\n",
  "**`superduper.vector_search.update_tasks`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/vector_search/update_tasks.py)\n\n## `copy_vectors` \n\n```python\ncopy_vectors(vector_index: str,\n     query: Union[Dict,\n     superduper.backends.base.query.Query],\n     ids: Sequence[str],\n     db=typing.Optional[ForwardRef('Datalayer')])\n```\n| Parameter | Description |\n|-----------|-------------|\n| vector_index | A identifier of the vector-index. |\n| query | A query which was used by `db._build_task_workflow` method |\n| ids | List of ids which were observed as added/updated documents. |\n| db | Datalayer instance. |\n\nCopy vectors of a ``VectorIndex`` component from the databackend to the fast_vector_search backend.\n\n## `delete_vectors` \n\n```python\ndelete_vectors(vector_index: str,\n     ids: Sequence[str],\n     db=typing.Optional[ForwardRef('Datalayer')])\n```\n| Parameter | Description |\n|-----------|-------------|\n| vector_index | A identifier of vector-index. |\n| ids | List of ids which were observed as deleted documents. |\n| db | Datalayer instance. |\n\nDelete vectors of a ``VectorIndex`` component in the fast_vector_search backend.\n\n",
  "**`superduper.vector_search.interface`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/vector_search/interface.py)\n\n## `FastVectorSearcher` \n\n```python\nFastVectorSearcher(self,\n     db: 'Datalayer',\n     vector_searcher,\n     vector_index: str)\n```\n| Parameter | Description |\n|-----------|-------------|\n| db | Datalayer instance |\n| vector_searcher | Vector searcher instance |\n| vector_index | Vector index name |\n\nFast vector searcher implementation using the server.\n\n",
  "**`superduper.base.document`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/base/document.py)\n\n## `Document` \n\n```python\nDocument(self,\n     *args,\n     schema: Optional[ForwardRef('Schema')] = None,\n     db: Optional[ForwardRef('Datalayer')] = None,\n     **kwargs)\n```\n| Parameter | Description |\n|-----------|-------------|\n| args | *args for `dict` |\n| schema | The schema to use. |\n| db | The datalayer to use. |\n| kwargs | **kwargs for `dict` |\n\nA wrapper around an instance of dict or a Encodable.\n\nThe document data is used to dump that resource to\na mix of json-able content, ids and `bytes`\n\n## `QueryUpdateDocument` \n\n```python\nQueryUpdateDocument(self,\n     *args,\n     schema: Optional[ForwardRef('Schema')] = None,\n     db: Optional[ForwardRef('Datalayer')] = None,\n     **kwargs)\n```\n| Parameter | Description |\n|-----------|-------------|\n| args | *args for `dict` |\n| schema | The schema to use. |\n| db | The datalayer to use. |\n| kwargs | **kwargs for `dict` |\n\nA document that is used to update a document in a database.\n\nThis document is used to update a document in a database.\nIt is a subclass of Document.\n\n",
  "**`superduper.base.exceptions`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/base/exceptions.py)\n\n## `DatabackendException` \n\n```python\nDatabackendException(self,\n     msg)\n```\n| Parameter | Description |\n|-----------|-------------|\n| msg | msg for BaseException |\n\nDatabackendException.\n\n## `BaseException` \n\n```python\nBaseException(self,\n     msg)\n```\n| Parameter | Description |\n|-----------|-------------|\n| msg | msg for Exception |\n\nBaseException which logs a message after exception.\n\n## `ComponentException` \n\n```python\nComponentException(self,\n     msg)\n```\n| Parameter | Description |\n|-----------|-------------|\n| msg | msg for BaseException |\n\nComponentException.\n\n## `ComponentInUseError` \n\n```python\nComponentInUseError(self,\n     /,\n     *args,\n     **kwargs)\n```\n| Parameter | Description |\n|-----------|-------------|\n| args | *args for Exception |\n| kwargs | **kwargs for Exception |\n\nException raised when a component is already in use.\n\n## `ComponentInUseWarning` \n\n```python\nComponentInUseWarning(self,\n     /,\n     *args,\n     **kwargs)\n```\n| Parameter | Description |\n|-----------|-------------|\n| args | *args for Exception |\n| kwargs | **kwargs for Exception |\n\nWarning raised when a component is already in use.\n\n## `MetadataException` \n\n```python\nMetadataException(self,\n     msg)\n```\n| Parameter | Description |\n|-----------|-------------|\n| msg | msg for BaseException |\n\nMetadataException.\n\n## `QueryException` \n\n```python\nQueryException(self,\n     msg)\n```\n| Parameter | Description |\n|-----------|-------------|\n| msg | msg for BaseException |\n\nQueryException.\n\n## `RequiredPackageVersionsNotFound` \n\n```python\nRequiredPackageVersionsNotFound(self,\n     /,\n     *args,\n     **kwargs)\n```\n| Parameter | Description |\n|-----------|-------------|\n| args | *args for ImportError |\n| kwargs | **kwargs for ImportError |\n\nException raised when one or more required packages are not found.\n\n## `RequiredPackageVersionsWarning` \n\n```python\nRequiredPackageVersionsWarning(self,\n     /,\n     *args,\n     **kwargs)\n```\n| Parameter | Description |\n|-----------|-------------|\n| args | *args for ImportWarning |\n| kwargs | **kwargs for ImportWarning |\n\nException raised when one or more required packages are not found.\n\n## `ServiceRequestException` \n\n```python\nServiceRequestException(self,\n     msg)\n```\n| Parameter | Description |\n|-----------|-------------|\n| msg | msg for BaseException |\n\nServiceRequestException.\n\n## `UnsupportedDatatype` \n\n```python\nUnsupportedDatatype(self,\n     msg)\n```\n| Parameter | Description |\n|-----------|-------------|\n| msg | msg for BaseException |\n\nUnsupportedDatatype.\n\n",
  "**`superduper.base.config_dicts`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/base/config_dicts.py)\n\n## `combine_configs` \n\n```python\ncombine_configs(dicts: Sequence[Dict[str,\n     object]]) -> Dict[str,\n     object]\n```\n| Parameter | Description |\n|-----------|-------------|\n| dicts | The dictionaries to combine. |\n\nCombine a sequence of dictionaries into a single dictionary.\n\n## `environ_to_config_dict` \n\n```python\nenviron_to_config_dict(prefix: str,\n     parent: Dict[str,\n     str],\n     environ: Optional[Dict[str,\n     str]] = None,\n     err: Optional[TextIO] = <_io.TextIOWrapper name='<stderr>' mode='w' encoding='utf-8'>,\n     fail: bool = False)\n```\n| Parameter | Description |\n|-----------|-------------|\n| prefix | The prefix to use for environment variables. |\n| parent | The parent dictionary to use as a basis. |\n| environ | The environment variables to read from. |\n| err | The file to write errors to. |\n| fail | Whether to raise an exception on error. |\n\nConvert environment variables to a configuration dictionary.\n\n",
  "**`superduper.base.decorators`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/base/decorators.py)\n\n## `code` \n\n```python\ncode(my_callable)\n```\n| Parameter | Description |\n|-----------|-------------|\n| my_callable | The callable to mark as remote code. |\n\nDecorator to mark a function as remote code.\n\n",
  "**`superduper.base.cursor`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/base/cursor.py)\n\n## `SelectResult` \n\n```python\nSelectResult(self,\n     raw_cursor: Any,\n     id_field: str,\n     db: Optional[ForwardRef('Datalayer')] = None,\n     scores: Optional[Dict[str,\n     float]] = None,\n     schema: Optional[ForwardRef('Schema')] = None,\n     _it: int = 0) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| raw_cursor | the cursor to wrap |\n| id_field | the field to use as the document id |\n| db | the datalayer to use to decode the documents |\n| scores | a dict of scores to add to the documents |\n| schema | the schema to use to decode the documents |\n| _it | an iterator to keep track of the current position in the cursor, Default is 0. |\n\nA wrapper around a raw cursor that adds some extra functionality.\n\nA cursor that wraps a cursor and returns ``Document`` wrapping\na dict including ``Encodable`` objects.\n\n",
  "**`superduper.base.config`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/base/config.py)\n\n## `BaseConfig` \n\n```python\nBaseConfig(self) -> None\n```\nA base class for configuration dataclasses.\n\nThis class allows for easy updating of configuration dataclasses\nwith a dictionary of parameters.\n\n## `CDCConfig` \n\n```python\nCDCConfig(self,\n     uri: Optional[str] = None,\n     strategy: Union[superduper.base.config.PollingStrategy,\n     superduper.base.config.LogBasedStrategy,\n     NoneType] = None) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| uri | The URI for the CDC service |\n| strategy | The strategy to use for CDC |\n\nDescribes the configuration for change data capture.\n\n## `CDCStrategy` \n\n```python\nCDCStrategy(self,\n     type: str) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| type | The type of CDC strategy |\n\nBase CDC strategy dataclass.\n\n## `Cluster` \n\n```python\nCluster(self,\n     compute: superduper.base.config.Compute = None,\n     vector_search: superduper.base.config.VectorSearch = None,\n     rest: superduper.base.config.Rest = None,\n     cdc: superduper.base.config.CDCConfig = None) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| compute | The URI for compute - None: run all jobs in local mode i.e. simple function call - \"ray://host:port\": Run all jobs on a remote ray cluster |\n| vector_search | The URI for the vector search service - None: Run vector search on local - `f\"http://{host}:{port}\"`: Connect a remote vector search service |\n| rest | The URI for the REST service - `f\"http://{host}:{port}\"`: Connect a remote vector search service |\n| cdc | The URI for the change data capture service (if \"None\" then no cdc assumed) None: Run cdc on local as a thread. - `f\"{http://{host}:{port}\"`: Connect a remote cdc service |\n\nDescribes a connection to distributed work via Ray.\n\n## `Compute` \n\n```python\nCompute(self,\n     uri: Optional[str] = None,\n     compute_kwargs: Dict = None) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| uri | The URI for the compute service |\n| compute_kwargs | The keyword arguments to pass to the compute service |\n\nDescribes the configuration for distributed computing.\n\n## `Config` \n\n```python\nConfig(self,\n     envs: dataclasses.InitVar[typing.Optional[typing.Dict[str,\n     str]]] = None,\n     data_backend: str = 'mongodb://localhost:27017/test_db',\n     lance_home: str = '.superduper/vector_indices',\n     artifact_store: Optional[str] = None,\n     metadata_store: Optional[str] = None,\n     cluster: superduper.base.config.Cluster = None,\n     retries: superduper.base.config.Retry = None,\n     downloads: superduper.base.config.Downloads = None,\n     fold_probability: float = 0.05,\n     log_level: superduper.base.config.LogLevel = <LogLevel.INFO: 'INFO'>,\n     logging_type: superduper.base.config.LogType = <LogType.SYSTEM: 'SYSTEM'>,\n     bytes_encoding: superduper.base.config.BytesEncoding = <BytesEncoding.BYTES: 'Bytes'>,\n     auto_schema: bool = True) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| envs | The envs datas |\n| data_backend | The URI for the data backend |\n| lance_home | The home directory for the Lance vector indices, Default: .superduper/vector_indices |\n| artifact_store | The URI for the artifact store |\n| metadata_store | The URI for the metadata store |\n| cluster | Settings distributed computing and change data capture |\n| retries | Settings for retrying failed operations |\n| downloads | Settings for downloading files |\n| fold_probability | The probability of validation fold |\n| log_level | The severity level of the logs |\n| logging_type | The type of logging to use |\n| bytes_encoding | The encoding of bytes in the data backend |\n| auto_schema | Whether to automatically create the schema. If True, the schema will be created if it does not exist. |\n\nThe data class containing all configurable superduper values.\n\n## `Downloads` \n\n```python\nDownloads(self,\n     folder: Optional[str] = None,\n     n_workers: int = 0,\n     headers: Dict = None,\n     timeout: Optional[int] = None) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| folder | The folder to download files to |\n| n_workers | The number of workers to use for downloading |\n| headers | The headers to use for downloading |\n| timeout | The timeout for downloading |\n\nDescribes the configuration for downloading files.\n\n## `LogBasedStrategy` \n\n```python\nLogBasedStrategy(self,\n     type: str = 'logbased',\n     resume_token: Optional[Dict[str,\n     str]] = None) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| resume_token | The resume token to use for log-based CDC |\n| type | The type of CDC strategy |\n\nDescribes a log-based strategy for change data capture.\n\n## `PollingStrategy` \n\n```python\nPollingStrategy(self,\n     type: 'str' = 'incremental',\n     auto_increment_field: Optional[str] = None,\n     frequency: float = 3600) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| auto_increment_field | The field to use for auto-incrementing |\n| frequency | The frequency to poll for changes |\n| type | The type of CDC strategy |\n\nDescribes a polling strategy for change data capture.\n\n## `Rest` \n\n```python\nRest(self,\n     uri: Optional[str] = None,\n     config: Optional[str] = None) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| uri | The URI for the REST service |\n| config | The path to the config yaml file for the REST service |\n\nDescribes the configuration for the REST service.\n\n## `Retry` \n\n```python\nRetry(self,\n     stop_after_attempt: int = 2,\n     wait_max: float = 10.0,\n     wait_min: float = 4.0,\n     wait_multiplier: float = 1.0) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| stop_after_attempt | The number of attempts to make |\n| wait_max | The maximum time to wait between attempts |\n| wait_min | The minimum time to wait between attempts |\n| wait_multiplier | The multiplier for the wait time between attempts |\n\nDescribes how to retry using the `tenacity` library.\n\n## `VectorSearch` \n\n```python\nVectorSearch(self,\n     uri: Optional[str] = None,\n     type: str = 'in_memory',\n     backfill_batch_size: int = 100) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| uri | The URI for the vector search service |\n| type | The type of vector search service |\n| backfill_batch_size | The size of the backfill batch |\n\nDescribes the configuration for vector search.\n\n",
  "**`superduper.base.configs`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/base/configs.py)\n\n## `ConfigError` \n\n```python\nConfigError(self,\n     /,\n     *args,\n     **kwargs)\n```\n| Parameter | Description |\n|-----------|-------------|\n| args | *args for `Exception` |\n| kwargs | **kwargs for `Exception` |\n\nAn exception raised when there is an error in the configuration.\n\n## `ConfigSettings` \n\n```python\nConfigSettings(self,\n     cls: Type,\n     environ: Optional[Dict] = None) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| cls | The Pydantic class to read. |\n| environ | The environment variables to read from. |\n\nHelper class to read a configuration from a dataclass.\n\nReads a dataclass class from a configuration file and environment variables.\n\n",
  "**`superduper.base.code`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/base/code.py)\n\n## `Code` \n\n```python\nCode(self,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None,\n     *,\n     identifier: str = '',\n     code: str) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n| code | The code to store. |\n\nA class to store remote code.\n\nThis class stores remote code that can be executed on a remote server.\n\n",
  "**`superduper.base.variables`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/base/variables.py)\n\n## `Variable` \n\n```python\nVariable(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n\nMechanism for allowing \"free variables\" in a leaf object.\n\nThe idea is to allow a variable to be set at runtime, rather than\nat object creation time.\n\n## `VariableError` \n\n```python\nVariableError(self,\n     /,\n     *args,\n     **kwargs)\n```\n| Parameter | Description |\n|-----------|-------------|\n| args | *args for `Exception`. |\n| kwargs | **kwargs for `Exception`. |\n\nVariable error.\n\n",
  "**`superduper.base.leaf`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/base/leaf.py)\n\n## `find_leaf_cls` \n\n```python\nfind_leaf_cls(full_import_path) -> Type[superduper.base.leaf.Leaf]\n```\n| Parameter | Description |\n|-----------|-------------|\n| full_import_path | Full import path of the class. |\n\nFind leaf class by class full import path.\n\n## `Leaf` \n\n```python\nLeaf(self,\n     identifier: str,\n     db: dataclasses.InitVar[typing.Optional[ForwardRef('Datalayer')]] = None,\n     uuid: str = None) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| identifier | Identifier of the leaf. |\n| db | Datalayer instance. |\n| uuid | UUID of the leaf. |\n\nBase class for all leaf classes.\n\n",
  "**`superduper.base.superduper`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/base/superduper.py)\n\n## `superduper` \n\n```python\nsuperduper(item: Optional[Any] = None,\n     **kwargs) -> Any\n```\n| Parameter | Description |\n|-----------|-------------|\n| item | A database or model |\n| kwargs | Additional keyword arguments to pass to the component |\n\n`superduper` API to automatically wrap an object to a db or a component.\n\nAttempts to automatically wrap an item in a superduper.ioponent by\nusing duck typing to recognize it.\n\n",
  "**`superduper.base.datalayer`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/base/datalayer.py)\n\n## `Datalayer` \n\n```python\nDatalayer(self,\n     databackend: superduper.backends.base.data_backend.BaseDataBackend,\n     metadata: superduper.backends.base.metadata.MetaDataStore,\n     artifact_store: superduper.backends.base.artifacts.ArtifactStore,\n     compute: superduper.backends.base.compute.ComputeBackend = <superduper.backends.local.compute.LocalComputeBackend object at 0x291ee3510>)\n```\n| Parameter | Description |\n|-----------|-------------|\n| databackend | Object containing connection to Datastore. |\n| metadata | Object containing connection to Metadatastore. |\n| artifact_store | Object containing connection to Artifactstore. |\n| compute | Object containing connection to ComputeBackend. |\n\nBase database connector for superduper.\n\n## `LoadDict` \n\n```python\nLoadDict(self,\n     database: superduper.base.datalayer.Datalayer,\n     field: Optional[str] = None,\n     callable: Optional[Callable] = None) -> None\n```\n| Parameter | Description |\n|-----------|-------------|\n| database | Instance of Datalayer. |\n| field | (optional) Component type identifier. |\n| callable | (optional) Callable function on key. |\n\nHelper class to load component identifiers with on-demand loading from the database.\n\n",
  "**`superduper.rest.utils`** \n\n[Source code](https://github.com/superduper/superduper/blob/main/superduper/rest/utils.py)\n\n## `parse_query` \n\n```python\nparse_query(query,\n     documents,\n     db)\n```\n| Parameter | Description |\n|-----------|-------------|\n| query | query string to parse |\n| documents | documents to use in the query |\n| db | datalayer instance |\n\nParse a query string into a query object.\n\n## `strip_artifacts` \n\n```python\nstrip_artifacts(r: Any)\n```\n| Parameter | Description |\n|-----------|-------------|\n| r | the data to strip artifacts from |\n\nStrip artifacts for the data.\n\n",
  "---\nsidebar_label: Multimodal vector search - Image\nfilename: build.md\n---\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n\n<!-- TABS -->\n# Multimodal vector search - Image\n\n<!-- TABS -->\n## Connect to superduper\n\n:::note\nNote that this is only relevant if you are running superduper in development mode.\nOtherwise refer to \"Configuring your production system\".\n:::\n\n```python\nfrom superduper import superduper\n\ndb = superduper('mongomock:///test_db')\n```\n\n<!-- TABS -->\n## Get useful sample data\n\n```python\n!curl -O https://superduperdb-public-demo.s3.amazonaws.com/images.zip && unzip images.zip\nimport os\nfrom PIL import Image\n\ndata = [f'images/{x}' for x in os.listdir('./images') if x.endswith(\".png\")][:200]\ndata = [ Image.open(path) for path in data]\n```\n\n```python\ndata = [{'img': d} for d in data[:100]]\n```\n\n## Build multimodal embedding models\n\nWe define the output data type of a model as a vector for vector transformation.\n\n\n<Tabs>\n    <TabItem value=\"MongoDB\" label=\"MongoDB\" default>\n        ```python\n        from superduper.components.vector_index import vector\n        output_datatpye = vector(shape=(1024,))        \n        ```\n    </TabItem>\n    <TabItem value=\"SQL\" label=\"SQL\" default>\n        ```python\n        from superduper.components.vector_index import sqlvector\n        output_datatpye = sqlvector(shape=(1024,))        \n        ```\n    </TabItem>\n</Tabs>\nThen define two models, one for text embedding and one for image embedding.\n\n```python\n!pip install git+https://github.com/openai/CLIP.git\n!pip install ../../plugins/torch\nimport clip\nfrom superduper import vector\nfrom superduper_torch import TorchModel\n\n# Load the CLIP model and obtain the preprocessing function\nmodel, preprocess = clip.load(\"RN50\", device='cpu')\n\n# Create a TorchModel for text encoding\ncompatible_model = TorchModel(\n    identifier='clip_text', # Unique identifier for the model\n    object=model, # CLIP model\n    preprocess=lambda x: clip.tokenize(x)[0],  # Model input preprocessing using CLIP \n    postprocess=lambda x: x.tolist(), # Convert the model output to a list\n    datatype=output_datatpye,  # Vector encoder with shape (1024,)\n    forward_method='encode_text', # Use the 'encode_text' method for forward pass \n)\n\n# Create a TorchModel for visual encoding\nembedding_model = TorchModel(\n    identifier='clip_image',  # Unique identifier for the model\n    object=model.visual,  # Visual part of the CLIP model    \n    preprocess=preprocess, # Visual preprocessing using CLIP\n    postprocess=lambda x: x.tolist(), # Convert the output to a list \n    datatype=output_datatpye, # Vector encoder with shape (1024,)\n)\n```\n\nBecause we use multimodal models, we define different keys to specify which model to use for embedding calculations in the vector_index.\n\n```python\nindexing_key = 'img' # we use img key for img embedding\ncompatible_key = 'text' # we use text key for text embedding\n```\n\n## Create vector-index\n\n```python\nvector_index_name = 'my-vector-index'\n```\n\n```python\nfrom superduper import VectorIndex, Listener\n\nvector_index = VectorIndex(\n    vector_index_name,\n    indexing_listener=Listener(\n        key=indexing_key,                 # the `Document` key `model` should ingest to create embedding\n        select=db['docs'].select(),       # a `Select` query telling which data to search over\n        model=embedding_model,            # a `_Predictor` how to convert data to embeddings\n        identifier='indexing-listener',\n    ),\n    compatible_listener=Listener(\n        key=compatible_key,               # the `Document` key `model` should ingest to create embedding\n        model=compatible_model,           # a `_Predictor` how to convert data to embeddings\n        select=None,\n        identifier='compatible-listener',\n    )\n)\n```\n\n```python\nfrom superduper import Application\n\napplication = Application(\n    'image-vector-search',\n    components=[vector_index],\n)\n\ndb.apply(application)\n```\n\n## Add the data\n\nThe order in which data is added is not important. *However* if your data requires a custom `Schema` in order to work, it's easier to add the `Application` first, and the data later. The advantage of this flexibility, is that once the `Application` is installed, it's waiting for incoming data, so that the `Application` is always up-to-date. This comes in particular handy with AI scenarios which need to respond to changing news.\n\n```python\nfrom superduper import Document\n\ntable_or_collection = db['docs']\n\nids = db.execute(table_or_collection.insert([Document(r) for r in data]))\n```\n\n## Perform a vector search\n\nWe can perform the vector searches using two types of data:\n\n- Text: By text description, we can find images similar to the text description.\n- Image: By using an image, we can find images similar to the provided image.\n\n\n<Tabs>\n    <TabItem value=\"Text\" label=\"Text\" default>\n        ```python\n        item = Document({compatible_key: \"Find a black dog\"})        \n        ```\n    </TabItem>\n    <TabItem value=\"Image\" label=\"Image\" default>\n        ```python\n        from IPython.display import display\n        search_image = data[0]\n        display(search_image)\n        item = Document({indexing_key: search_image})        \n        ```\n    </TabItem>\n</Tabs>\nOnce we have this search target, we can execute a search as follows.\n\n```python\nselect = db['docs'].like(item, vector_index=vector_index_name, n=5).select()\nresults = list(db.execute(select))\n```\n\n## Visualize Results\n\n```python\nfrom IPython.display import display\nfor result in results:\n    display(result[indexing_key])\n```\n\n## Create a `Template`\n\n```python\nfrom superduper import Template\n\ntemplate = Template(\n    'image-vector-search',\n    template=application,\n    substitutions={'docs': 'table'},\n)\n\ntemplate.export('.')\n```\n\n",
  "---\nsidebar_label: Fine tune LLM on database\nfilename: build.md\n---\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n\n<!-- TABS -->\n# Fine tune LLM on database\n\n<!-- TABS -->\n## Connect to superduper\n\n:::note\nNote that this is only relevant if you are running superduper in development mode.\nOtherwise refer to \"Configuring your production system\".\n:::\n\n```python\nfrom superduper import superduper\n\ndb = superduper('mongomock:///test_db')\n```\n\n<!-- TABS -->\n## Get LLM Finetuning Data\n\nThe following are examples of training data in different formats.\n\n\n<Tabs>\n    <TabItem value=\"Text\" label=\"Text\" default>\n        ```python\n        from datasets import load_dataset\n        from superduper.base.document import Document\n        dataset_name = \"timdettmers/openassistant-guanaco\"\n        dataset = load_dataset(dataset_name)\n        \n        train_dataset = dataset[\"train\"]\n        eval_dataset = dataset[\"test\"]\n        \n        train_documents = [\n            Document({**example, \"_fold\": \"train\"})\n            for example in train_dataset\n        ]\n        eval_documents = [\n            Document({**example, \"_fold\": \"valid\"})\n            for example in eval_dataset\n        ]\n        \n        datas = train_documents + eval_documents        \n        ```\n    </TabItem>\n    <TabItem value=\"Prompt-Response\" label=\"Prompt-Response\" default>\n        ```python\n        from datasets import load_dataset\n        \n        from superduper.base.document import Document\n        dataset_name = \"mosaicml/instruct-v3\"\n        dataset = load_dataset(dataset_name)\n        \n        train_dataset = dataset[\"train\"]\n        eval_dataset = dataset[\"test\"]\n        \n        train_documents = [\n            Document({**example, \"_fold\": \"train\"})\n            for example in train_dataset\n        ]\n        eval_documents = [\n            Document({**example, \"_fold\": \"valid\"})\n            for example in eval_dataset\n        ]\n        \n        datas = train_documents + eval_documents        \n        ```\n    </TabItem>\n    <TabItem value=\"Chat\" label=\"Chat\" default>\n        ```python\n        from datasets import load_dataset\n        from superduper.base.document import Document\n        dataset_name = \"philschmid/dolly-15k-oai-style\"\n        dataset = load_dataset(dataset_name)['train'].train_test_split(0.9)\n        \n        train_dataset = dataset[\"train\"]\n        eval_dataset = dataset[\"test\"]\n        \n        train_documents = [\n            Document({**example, \"_fold\": \"train\"})\n            for example in train_dataset\n        ]\n        eval_documents = [\n            Document({**example, \"_fold\": \"valid\"})\n            for example in eval_dataset\n        ]\n        \n        datas = train_documents + eval_documents        \n        ```\n    </TabItem>\n</Tabs>\nWe can define different training parameters to handle this type of data.\n\n\n<Tabs>\n    <TabItem value=\"Text\" label=\"Text\" default>\n        ```python\n        # Function for transformation after extracting data from the database\n        transform = None\n        key = ('text')\n        training_kwargs=dict(dataset_text_field=\"text\")        \n        ```\n    </TabItem>\n    <TabItem value=\"Prompt-Response\" label=\"Prompt-Response\" default>\n        ```python\n        # Function for transformation after extracting data from the database\n        def transform(prompt, response):\n            return {'text': prompt + response + \"</s>\"}\n        \n        key = ('prompt', 'response')\n        training_kwargs=dict(dataset_text_field=\"text\")        \n        ```\n    </TabItem>\n    <TabItem value=\"Chat\" label=\"Chat\" default>\n        ```python\n        # Function for transformation after extracting data from the database\n        transform = None\n        \n        key = ('messages')\n        training_kwargs=None        \n        ```\n    </TabItem>\n</Tabs>\nExample input_text and output_text\n\n\n<Tabs>\n    <TabItem value=\"Text\" label=\"Text\" default>\n        ```python\n        data = datas[0]\n        input_text, output_text = data[\"text\"].rsplit(\"### Assistant: \", maxsplit=1)\n        input_text += \"### Assistant: \"\n        output_text = output_text.rsplit(\"### Human:\")[0]\n        print(\"Input: --------------\")\n        print(input_text)\n        print(\"Response: --------------\")\n        print(output_text)        \n        ```\n    </TabItem>\n    <TabItem value=\"Prompt-Response\" label=\"Prompt-Response\" default>\n        ```python\n        data = datas[0]\n        input_text = data[\"prompt\"]\n        output_text = data[\"response\"]\n        print(\"Input: --------------\")\n        print(input_text)\n        print(\"Response: --------------\")\n        print(output_text)        \n        ```\n    </TabItem>\n    <TabItem value=\"Chat\" label=\"Chat\" default>\n        ```python\n        data = datas[0]\n        messages = data[\"messages\"]\n        input_text = messages[:-1]\n        output_text = messages[-1][\"content\"]\n        print(\"Input: --------------\")\n        print(input_text)\n        print(\"Response: --------------\")\n        print(output_text)        \n        ```\n    </TabItem>\n</Tabs>\n<!-- TABS -->\n## Insert simple data\n\nAfter turning on auto_schema, we can directly insert data, and superduper will automatically analyze the data type, and match the construction of the table and datatype.\n\n```python\nfrom superduper import Document\n\ntable_or_collection = db['docs']\n\nids = db.execute(table_or_collection.insert([Document(data) for data in datas]))\nselect = table_or_collection.select()\n```\n\n## Select a Model\n\n```python\nmodel_name = \"facebook/opt-125m\"\nmodel_kwargs = dict()\ntokenizer_kwargs = dict()\n\n# or \n# model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n# token = \"hf_xxxx\"\n# model_kwargs = dict(token=token)\n# tokenizer_kwargs = dict(token=token)\n```\n\n<!-- TABS -->\n## Build A Trainable LLM\n\n**Create an LLM Trainer for training**\n\nThe parameters of this LLM Trainer are basically the same as `transformers.TrainingArguments`, but some additional parameters have been added for easier training setup.\n\n```python\nfrom superduper_transformers import LLM, LLMTrainer\n\ntrainer = LLMTrainer(\n    identifier=\"llm-finetune-trainer\",\n    output_dir=\"output/finetune\",\n    overwrite_output_dir=True,\n    num_train_epochs=3,\n    save_total_limit=3,\n    logging_steps=10,\n    evaluation_strategy=\"steps\",\n    save_steps=100,\n    eval_steps=100,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    max_seq_length=512,\n    key=key,\n    select=select,\n    transform=transform,\n    training_kwargs=training_kwargs,\n)\n```\n\n\n<Tabs>\n    <TabItem value=\"Lora\" label=\"Lora\" default>\n        ```python\n        trainer.use_lora = True        \n        ```\n    </TabItem>\n    <TabItem value=\"QLora\" label=\"QLora\" default>\n        ```python\n        trainer.use_lora = True\n        trainer.bits = 4        \n        ```\n    </TabItem>\n    <TabItem value=\"Deepspeed\" label=\"Deepspeed\" default>\n        ```python\n        !pip install deepspeed\n        deepspeed = {\n            \"train_batch_size\": \"auto\",\n            \"train_micro_batch_size_per_gpu\": \"auto\",\n            \"gradient_accumulation_steps\": \"auto\",\n            \"zero_optimization\": {\n                \"stage\": 2,\n            },\n        }\n        trainer.use_lora = True\n        trainer.bits = 4\n        trainer.deepspeed = deepspeed        \n        ```\n    </TabItem>\n    <TabItem value=\"Multi-GPUS\" label=\"Multi-GPUS\" default>\n        ```python\n        trainer.use_lora = True\n        trainer.bits = 4\n        trainer.num_gpus = 2        \n        ```\n    </TabItem>\n</Tabs>\nCreate a trainable LLM model and add it to the database, then the training task will run automatically.\n\n```python\nllm = LLM(\n    identifier=\"llm\",\n    model_name_or_path=model_name,\n    trainer=trainer,\n    model_kwargs=model_kwargs,\n    tokenizer_kwargs=tokenizer_kwargs,\n)\n\ndb.apply(llm)\n```\n\n## Load the trained model\nThere are two methods to load a trained model:\n\n- **Load the model directly**: This will load the model with the best metrics (if the transformers' best model save strategy is set) or the last version of the model.\n- **Use a specified checkpoint**: This method downloads the specified checkpoint, then initializes the base model, and finally merges the checkpoint with the base model. This approach supports custom operations such as resetting flash_attentions, model quantization, etc., during initialization.\n\n\n<Tabs>\n    <TabItem value=\"Load Trained Model Directly\" label=\"Load Trained Model Directly\" default>\n        ```python\n        llm = db.load(\"model\", \"llm\")        \n        ```\n    </TabItem>\n    <TabItem value=\"Use a specified checkpoint\" label=\"Use a specified checkpoint\" default>\n        ```python\n        from superduper_transformers import LLM\n        \n        experiment_id = db.show(\"checkpoint\")[-1]\n        version = None # None means the last checkpoint\n        checkpoint = db.load(\"checkpoint\", experiment_id, version=version)\n        llm = LLM(\n            identifier=\"llm\",\n            model_name_or_path=model_name,\n            adapter_id=checkpoint,\n            model_kwargs=dict(load_in_4bit=True)\n        )        \n        ```\n    </TabItem>\n</Tabs>\n```python\nllm.predict(input_text, max_new_tokens=200)\n```\n\n```python\nfrom superduper import Template\n\nt = Template('llm-finetune', template=llm, substitutions={'docs': 'collection', model_name: 'model_name'})\n```\n\n```python\nt.export('.')\n```\n\n",
  "---\nsidebar_label: Text Vector Search\nfilename: build.md\n---\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n\n<!-- TABS -->\n# Text Vector Search\n\nYou'll find this example as well as the saved template in the main repository of `superduper`.\nSee [here](https://github.com/superduper-io/superduper/tree/main/templates/text_vector_search).\n\nIf you'd like to modify the template, or practice building it yourself, then you can rerun the `build.ipynb` notebook\nin the template directory\n\n<!-- TABS -->\n## Connect to superduper\n\n```python\nfrom superduper import superduper\n\ndb = superduper('mongomock://test_db')\n```\n\n<!-- TABS -->\n## Get useful sample data\n\n\n<Tabs>\n    <TabItem value=\"Text\" label=\"Text\" default>\n        ```python\n        !curl -O https://superduperdb-public-demo.s3.amazonaws.com/text.json\n        import json\n        \n        with open('text.json', 'r') as f:\n            data = json.load(f)        \n        ```\n    </TabItem>\n    <TabItem value=\"PDF\" label=\"PDF\" default>\n        ```python\n        !curl -O https://superduperdb-public-demo.s3.amazonaws.com/pdfs.zip && unzip -o pdfs.zip\n        import os\n        \n        data = [f'pdfs/{x}' for x in os.listdir('./pdfs') if x.endswith('.pdf')]        \n        ```\n    </TabItem>\n</Tabs>\n```python\ndatas = [{'x': d} for d in data]\n```\n\n<!-- TABS -->\n## Create datatype\n\nSuperduperDB supports automatic data conversion, so users don’t need to worry about the compatibility of different data formats (`PIL.Image`, `numpy.array`, `pandas.DataFrame`, etc.) with the database.\n\nIt also supports custom data conversion methods for transforming data, such as defining the following Datatype.\n\n\n<Tabs>\n    <TabItem value=\"Text\" label=\"Text\" default>\n        ```python\n        datatype = 'str'        \n        ```\n    </TabItem>\n    <TabItem value=\"PDF\" label=\"PDF\" default>\n        ```python\n        from superduper import DataType\n        \n        # By creating a datatype and setting its encodable attribute to “file” for saving PDF files, \n        # all datatypes encoded as “file” will have their corresponding files uploaded to the artifact store. \n        # References will be recorded in the database, and the files will be downloaded locally when needed. \n        \n        datatype = DataType('pdf', encodable='file')        \n        ```\n    </TabItem>\n</Tabs>\n<!-- TABS -->\n## Setup tables or collections\n\n```python\nfrom superduper.components.table import Table\nfrom superduper import Schema\n\nschema = Schema(identifier=\"schema\", fields={\"x\": datatype})\ntable = Table(\"docs\", schema=schema)\nselect = db['docs'].select()\n```\n\n<!-- TABS -->\n## Apply a chunker for search\n\n:::note\nNote that applying a chunker is ***not*** mandatory for search.\nIf your data is already chunked (e.g. short text snippets or audio) or if you\nare searching through something like images, which can't be chunked, then this\nwon't be necessary.\n:::\n\n\n<Tabs>\n    <TabItem value=\"Text\" label=\"Text\" default>\n        ```python\n        from superduper import model\n        \n        CHUNK_SIZE = 200\n        \n        @model(flatten=True, model_update_kwargs={'document_embedded': False})\n        def chunker(text):\n            text = text.split()\n            chunks = [' '.join(text[i:i + CHUNK_SIZE]) for i in range(0, len(text), CHUNK_SIZE)]\n            return chunks        \n        ```\n    </TabItem>\n    <TabItem value=\"PDF\" label=\"PDF\" default>\n        ```python\n        !pip install -q \"unstructured[pdf]\"\n        from superduper import model\n        from unstructured.partition.pdf import partition_pdf\n        \n        CHUNK_SIZE = 500\n        \n        @model(flatten=True)\n        def chunker(pdf_file):\n            elements = partition_pdf(pdf_file)\n            text = '\\n'.join([e.text for e in elements])\n            chunks = [text[i:i + CHUNK_SIZE] for i in range(0, len(text), CHUNK_SIZE)]\n            return chunks        \n        ```\n    </TabItem>\n</Tabs>\nNow we wrap this chunker as a `Listener`, so that it processes incoming data\n\n```python\nfrom superduper import Listener\n\nupstream_listener = Listener(\n    model=chunker,\n    select=db['docs'].select(),\n    key='x',\n    uuid=\"chunk\",\n    identifier='chunker',\n)\n```\n\n## Select outputs of upstream listener\n\n:::note\nThis is useful if you have performed a first step, such as pre-computing \nfeatures, or chunking your data. You can use this query to \noperate on those outputs.\n:::\n\n```python\nindexing_key = upstream_listener.outputs\nindexing_key\n```\n\n<!-- TABS -->\n## Build text embedding model\n\n\n<Tabs>\n    <TabItem value=\"OpenAI\" label=\"OpenAI\" default>\n        ```python\n        from superduper_openai import OpenAIEmbedding\n        import os\n        \n        os.environ['OPENAI_API_KEY'] = 'sk-<secret>'\n        \n        embedding_model = OpenAIEmbedding(identifier='text-embedding-ada-002')        \n        ```\n    </TabItem>\n    <TabItem value=\"JinaAI\" label=\"JinaAI\" default>\n        ```python\n        import os\n        from superduper_jina import JinaEmbedding\n        \n        os.environ[\"JINA_API_KEY\"] = \"jina_xxxx\"\n         \n        # define the model\n        embedding_model = JinaEmbedding(identifier='jina-embeddings-v2-base-en')        \n        ```\n    </TabItem>\n    <TabItem value=\"Sentence-Transformers\" label=\"Sentence-Transformers\" default>\n        ```python\n        !pip install sentence-transformers\n        from superduper import vector\n        import sentence_transformers\n        from superduper_sentence_transformers import SentenceTransformer\n        \n        embedding_model = SentenceTransformer(\n            identifier=\"embedding\",\n            object=sentence_transformers.SentenceTransformer(\"BAAI/bge-small-en\"),\n            datatype=vector(shape=(1024,)),\n            postprocess=lambda x: x.tolist(),\n            predict_kwargs={\"show_progress_bar\": True},\n        )        \n        ```\n    </TabItem>\n</Tabs>\n```python\nprint(len(embedding_model.predict(\"What is superduper\")))\n```\n\n## Create vector-index\n\n```python\nvector_index_name = 'my-vector-index'\n```\n\n```python\nfrom superduper import VectorIndex, Listener\n\nvector_index = VectorIndex(\n    vector_index_name,\n    indexing_listener=Listener(\n        key=indexing_key,              # the `Document` key `model` should ingest to create embedding\n        select=db[indexing_key].select(),                 # a `Select` query telling which data to search over\n        model=embedding_model,         # a `_Predictor` how to convert data to embeddings\n        identifier=f'{embedding_model.identifier}-listener',\n        upstream=[table, upstream_listener],              # this makes sure that the table is already set up when the other components are triggered\n    )\n)\n```\n\n```python\nfrom superduper import Application\n\napplication = Application(\n    'text-vector-search', \n    components=[\n        table,\n        upstream_listener,\n        vector_index,\n    ]\n)\n```\n\n```python\ndb.apply(application)\n```\n\n```python\napplication.info(verbosity=2)\n```\n\n```python\ndb['docs'].insert(datas).execute()\nselect = db['docs'].select()\n```\n\n```python\ndb.databackend.db.list_collection_names()\n```\n\n## Perform a vector search\n\n```python\nfrom superduper import Document\n# Perform the vector search based on the query\nitem = Document({indexing_key: \"Tell me about vector-search\"})\n```\n\n```python\nresults = db[indexing_key].like(item, vector_index=vector_index_name, n=10).select().execute()\n```\n\n```python\nfor result in results:\n    print(\"\\n\", '-' * 20, '\\n')\n    print(Document(result.unpack())[indexing_key])\n```\n\n```python\nfrom superduper import Template\n\nt = Template(\n    'vector-search',\n    template=application,\n    substitutions={'docs': 'table_name'},\n)\n```\n\n```python\nt.export('.')\n```\n\n```python\n!cat component.json | jq .\n```\n\n",
  "---\nsidebar_label: Transfer learning\nfilename: build.md\n---\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n\n<!-- TABS -->\n# Transfer learning\n\n<!-- TABS -->\n## Connect to superduper\n\n```python\nfrom superduper import superduper\n\ndb = superduper('mongomock:///test_db')\n```\n\n<!-- TABS -->\n## Get useful sample data\n\n\n<Tabs>\n    <TabItem value=\"Text-Classification\" label=\"Text-Classification\" default>\n        ```python\n        !curl -O https://superduperdb-public-demo.s3.amazonaws.com/text_classification.json\n        import json\n        \n        with open(\"text_classification.json\", \"r\") as f:\n            data = json.load(f)\n        num_classes = 2        \n        ```\n    </TabItem>\n    <TabItem value=\"Image-Classification\" label=\"Image-Classification\" default>\n        ```python\n        !curl -O https://superduperdb-public-demo.s3.amazonaws.com/images_classification.zip && unzip images_classification.zip\n        import json\n        from PIL import Image\n        \n        with open('images/images.json', 'r') as f:\n            data = json.load(f)\n            \n        data = [{'x': Image.open(d['image_path']), 'y': d['label']} for d in data]\n        num_classes = 2        \n        ```\n    </TabItem>\n</Tabs>\nAfter obtaining the data, we insert it into the database.\n\n\n<Tabs>\n    <TabItem value=\"Text-Classification\" label=\"Text-Classification\" default>\n        ```python\n        datas = [{'txt': d['x'], 'label': d['y']} for d in data]        \n        ```\n    </TabItem>\n    <TabItem value=\"Image-Classification\" label=\"Image-Classification\" default>\n        ```python\n        datas = [{'image': d['x'], 'label': d['y']} for d in data]        \n        ```\n    </TabItem>\n</Tabs>\n<!-- TABS -->\n## Insert simple data\n\nAfter turning on auto_schema, we can directly insert data, and superduper will automatically analyze the data type, and match the construction of the table and datatype.\n\n```python\nfrom superduper import Document\n\ntable_or_collection = db['docs']\n\nids = db.execute(table_or_collection.insert([Document(data) for data in datas]))\nselect = table_or_collection.select()\n```\n\n<!-- TABS -->\n## Compute features\n\n\n<Tabs>\n    <TabItem value=\"Text\" label=\"Text\" default>\n        ```python\n        key = 'txt'\n        import sentence_transformers\n        from superduper import vector, Listener\n        from superduper_sentence_transformers import SentenceTransformer\n        \n        superdupermodel = SentenceTransformer(\n            identifier=\"embedding\",\n            object=sentence_transformers.SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\"),\n            postprocess=lambda x: x.tolist(),\n        )\n        \n        jobs, listener = db.apply(\n            Listener(\n                model=superdupermodel,\n                select=select,\n                key=key,\n                identifier=\"features\"\n            )\n        )        \n        ```\n    </TabItem>\n    <TabItem value=\"Image\" label=\"Image\" default>\n        ```python\n        key = 'image'\n        import torchvision.models as models\n        from torchvision import transforms\n        from superduper_torch import TorchModel\n        from superduper import Listener\n        from PIL import Image\n        \n        class TorchVisionEmbedding:\n            def __init__(self):\n                # Load the pre-trained ResNet-18 model\n                self.resnet = models.resnet18(pretrained=True)\n                \n                # Set the model to evaluation mode\n                self.resnet.eval()\n                \n            def preprocess(self, image):\n                # Preprocess the image\n                preprocess = preprocess = transforms.Compose([\n                    transforms.Resize(256),\n                    transforms.CenterCrop(224),\n                    transforms.ToTensor(),\n                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                ])\n                tensor_image = preprocess(image)\n                return tensor_image\n                \n        model = TorchVisionEmbedding()\n        superdupermodel = TorchModel(identifier='my-vision-model-torch', object=model.resnet, preprocess=model.preprocess, postprocess=lambda x: x.numpy().tolist())\n        \n        jobs, listener = db.apply(\n            Listener(\n                model=superdupermodel,\n                select=select,\n                key=key,\n                identifier=\"features\"\n            )\n        )        \n        ```\n    </TabItem>\n</Tabs>\n## Choose features key from feature listener\n\n```python\ninput_key = listener.outputs\ntraining_select = select.outputs(listener.predict_id)\n```\n\nWe can find the calculated feature data from the database.\n\n```python\nfeature = list(training_select.limit(1).execute())[0][input_key]\nfeature_size = len(feature)\n```\n\n<!-- TABS -->\n## Build and train classifier\n\n\n<Tabs>\n    <TabItem value=\"Scikit-Learn\" label=\"Scikit-Learn\" default>\n        ```python\n        from superduper_sklearn import Estimator, SklearnTrainer\n        from sklearn.svm import SVC\n        \n        model = Estimator(\n            identifier=\"my-model\",\n            object=SVC(),\n            trainer=SklearnTrainer(\n                \"my-trainer\",\n                key=(input_key, \"label\"),\n                select=training_select,\n            ),\n        )        \n        ```\n    </TabItem>\n    <TabItem value=\"Torch\" label=\"Torch\" default>\n        ```python\n        import torch\n        from torch import nn\n        from superduper_torch.model import TorchModel\n        from superduper_torch.training import TorchTrainer\n        from torch.nn.functional import cross_entropy\n        \n        \n        class SimpleModel(nn.Module):\n            def __init__(self, input_size=16, hidden_size=32, num_classes=3):\n                super(SimpleModel, self).__init__()\n                self.fc1 = nn.Linear(input_size, hidden_size)\n                self.relu = nn.ReLU()\n                self.fc2 = nn.Linear(hidden_size, num_classes)\n        \n            def forward(self, x):\n                out = self.fc1(x)\n                out = self.relu(out)\n                out = self.fc2(out)\n                return out\n        \n        preprocess = lambda x: torch.tensor(x)\n        \n        # Postprocess function for the model output    \n        def postprocess(x):\n            return int(x.topk(1)[1].item())\n        \n        def data_transform(features, label):\n            return torch.tensor(features), label\n        \n        # Create a Logistic Regression model\n        # feature_length is the input feature size\n        model = SimpleModel(feature_size, num_classes=num_classes)\n        model = TorchModel(\n            identifier='my-model',\n            object=model,         \n            preprocess=preprocess,\n            postprocess=postprocess,\n            trainer=TorchTrainer(\n                key=(input_key, 'label'),\n                identifier='my_trainer',\n                objective=cross_entropy,\n                loader_kwargs={'batch_size': 10},\n                max_iterations=1000,\n                validation_interval=100,\n                select=select,\n                transform=data_transform,\n            ),\n        )        \n        ```\n    </TabItem>\n</Tabs>\nDefine a validation for evaluating the effect after training.\n\n```python\nfrom superduper import Dataset, Metric, Validation\n\n\ndef acc(x, y):\n    return sum([xx == yy for xx, yy in zip(x, y)]) / len(x)\n\n\naccuracy = Metric(identifier=\"acc\", object=acc)\nvalidation = Validation(\n    \"transfer_learning_performance\",\n    key=(input_key, \"label\"),\n    datasets=[\n        Dataset(identifier=\"my-valid\", select=training_select.add_fold('valid'))\n    ],\n    metrics=[accuracy],\n)\nmodel.validation = validation\n```\n\nIf we execute the apply function, then the model will be added to the database, and because the model has a Trainer, it will perform training tasks.\n\n```python\ndb.apply(model)\n```\n\n```python\nmodel.encode()\n```\n\nGet the training metrics\n\n```python\nmodel = db.load('model', model.identifier)\nmodel.metric_values\n```\n\n```python\nfrom superduper import Template\n\nt = Template('transfer-learner', template=model, substitutions={'docs': 'table'})\n```\n\n```python\nt.export('.')\n```\n\n",
  "---\nsidebar_label: Multimodal vector search - Video\nfilename: build.md\n---\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n\n<!-- TABS -->\n# Multimodal vector search - Video\n\n<!-- TABS -->\n## Connect to superduper\n\n```python\nfrom superduper import superduper\n  \ndb = superduper('mongomock://test_db')\n```\n\n<!-- TABS -->\n## Get useful sample data\n\n```python\n!curl -O https://superduperdb-public-demo.s3.amazonaws.com/videos.zip && unzip videos.zip\nimport os\nfrom superduper.ext.pillow import pil_image\n\ndata = [f'videos/{x}' for x in os.listdir('./videos')]\nsample_datapoint = data[-1]\n\nchunked_model_datatype = pil_image\n\ndatas = [{'x': d} for d in data[:3]]\n```\n\n<!-- TABS -->\n## Create datatype\n\nSuperduperDB supports automatic data conversion, so users don’t need to worry about the compatibility of different data formats (`PIL.Image`, `numpy.array`, `pandas.DataFrame`, etc.) with the database.\n\nIt also supports custom data conversion methods for transforming data, such as defining the following Datatype.\n\n```python\nfrom superduper import DataType\n\n# Create an instance of the Encoder with the identifier 'video_on_file' and load_hybrid set to False\ndatatype = DataType(\n    identifier='video_on_file',\n    encodable='file',\n)\n```\n\n<!-- TABS -->\n## Setup tables or collections\n\n```python\nfrom superduper.components.table import Table\nfrom superduper import Schema\n\nschema = Schema(identifier=\"schema\", fields={\"x\": datatype})\ntable = Table(\"docs\", schema=schema)\n```\n\n```python\ndb.apply(table)\n```\n\n```python\ndb['docs'].insert(datas).execute()\n```\n\n<!-- TABS -->\n## Apply a chunker for search\n\n:::note\nNote that applying a chunker is ***not*** mandatory for search.\nIf your data is already chunked (e.g. short text snippets or audio) or if you\nare searching through something like images, which can't be chunked, then this\nwon't be necessary.\n:::\n\n```python\n# !pip install opencv-python\nimport cv2\nimport tqdm\nfrom PIL import Image\nfrom superduper.ext.pillow import pil_image\nfrom superduper import model, Schema\n\n\n@model(\n    flatten=True,\n    model_update_kwargs={},\n)\ndef chunker(video_file):\n    # Set the sampling frequency for frames\n    sample_freq = 10\n    \n    # Open the video file using OpenCV\n    cap = cv2.VideoCapture(video_file)\n    \n    # Initialize variables\n    frame_count = 0\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    extracted_frames = []\n    progress = tqdm.tqdm()\n\n    # Iterate through video frames\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        # Get the current timestamp based on frame count and FPS\n        current_timestamp = frame_count // fps\n        \n        # Sample frames based on the specified frequency\n        if frame_count % sample_freq == 0:\n            extracted_frames.append({\n                'image': Image.fromarray(frame[:,:,::-1]),  # Convert BGR to RGB\n                'current_timestamp': current_timestamp,\n            })\n        frame_count += 1\n        progress.update(1)\n    \n    # Release resources\n    cap.release()\n    cv2.destroyAllWindows()\n    \n    # Return the list of extracted frames\n    return extracted_frames\n```\n\nNow we apply this chunker to the data by wrapping the chunker in `Listener`:\n\n```python\nfrom superduper import Listener\n\nupstream_listener = Listener(\n    model=chunker,\n    select=db['docs'].select(),\n    key='x',\n    uuid='chunker',\n    identifier='chunker',\n    upstream=[table]\n)\n```\n\n```python\ndb.apply(upstream_listener)\n```\n\n## Build multimodal embedding models\n\nWe define the output data type of a model as a vector for vector transformation.\n\n\n<Tabs>\n    <TabItem value=\"MongoDB\" label=\"MongoDB\" default>\n        ```python\n        from superduper.components.vector_index import vector\n        output_datatype = vector(shape=(1024,))        \n        ```\n    </TabItem>\n    <TabItem value=\"SQL\" label=\"SQL\" default>\n        ```python\n        from superduper.components.vector_index import sqlvector\n        output_datatype = sqlvector(shape=(1024,))        \n        ```\n    </TabItem>\n</Tabs>\nThen define two models, one for text embedding and one for image embedding.\n\n```python\n# !pip install git+https://github.com/openai/CLIP.git\nimport clip\nfrom superduper import vector\nfrom superduper_torch import TorchModel\n\n# Load the CLIP model and obtain the preprocessing function\nmodel, preprocess = clip.load(\"ViT-B/32\", device='cpu')\n\n# Create a TorchModel for text encoding\ncompatible_model = TorchModel(\n    identifier='clip_text', # Unique identifier for the model\n    object=model, # CLIP model\n    preprocess=lambda x: clip.tokenize(x)[0],  # Model input preprocessing using CLIP \n    postprocess=lambda x: x.tolist(), # Convert the model output to a list\n    datatype=output_datatype,  # Vector encoder with shape (1024,)\n    forward_method='encode_text', # Use the 'encode_text' method for forward pass \n)\n\n# Create a TorchModel for visual encoding\nmodel = TorchModel(\n    identifier='clip_image',  # Unique identifier for the model\n    object=model.visual,  # Visual part of the CLIP model    \n    preprocess=preprocess, # Visual preprocessing using CLIP\n    postprocess=lambda x: x.tolist(), # Convert the output to a list \n    datatype=output_datatype, # Vector encoder with shape (1024,)\n)\n```\n\nBecause we use multimodal models, we define different keys to specify which model to use for embedding calculations in the vector_index.\n\n## Create vector-index\n\n```python\nfrom superduper import VectorIndex, Listener\n\nvector_index = VectorIndex(\n    'my-vector-index',\n    indexing_listener=Listener(\n        key=upstream_listener.outputs + '.image',      # the `Document` key `model` should ingest to create embedding\n        select=db[upstream_listener.outputs].select(),       # a `Select` query telling which data to search over\n        model=model,         # a `_Predictor` how to convert data to embeddings\n        identifier=f'{model.identifier}-listener'\n    ),\n    compatible_listener=Listener(\n        key='text',      # the `Document` key `model` should ingest to create embedding\n        model=compatible_model,         # a `_Predictor` how to convert data to embeddings\n        select=None,\n        identifier='compatible-listener',\n    ),\n    upstream=[upstream_listener],\n)\n```\n\n```python\ndb.apply(vector_index)\n```\n\n```python\nfrom superduper import Application\n\napp = Application(\n    'video-search',\n    components=[\n        upstream_listener,\n        vector_index,\n    ]\n)\n```\n\n```python\ndb.apply(app)\n```\n\n## Perform a vector search\n\nWe can perform the vector searches using text description:\n\n```python\nfrom superduper import Document\nitem = Document({'text': \"A single red and a blue player battle for the ball\"})\n```\n\n```python\nfrom superduper import Document\nitem = Document({'text': \"Some monkeys playing\"})\n```\n\nOnce we have this search target, we can execute a search as follows.\n\n```python\nselect = db[upstream_listener.outputs].like(item, vector_index='my-vector-index', n=5).select()\nresults = list(db.execute(select))\n```\n\n## Visualize Results\n\n```python\nfrom IPython.display import display\nfor result in results:\n    display(Document(result.unpack())[upstream_listener.outputs + '.image'])\n```\n\n## Check the system stays updated\n\nYou can add new data; once the data is added, all related models will perform calculations according to the underlying constructed model and listener, simultaneously updating the vector index to ensure that each query uses the latest data.\n\n```python\nnew_datas = [{'x': data[-1]}]\nids = db['docs'].insert(new_datas).execute()\n```\n\n```python\nfrom superduper import Template\n\nt = Template('video-search-template', template=app, substitutions={'docs': 'content_table'})\n```\n\n```python\nt.export('.')\n```\n\n",
  "---\nsidebar_label: Retrieval augmented generation\nfilename: build.md\n---\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\n\n\n<!-- TABS -->\n# Retrieval augmented generation\n\n<!-- TABS -->\n## Connect to superduper\n\n:::note\nNote that this is only relevant if you are running superduper in development mode.\nOtherwise refer to \"Configuring your production system\".\n:::\n\n```python\nfrom superduper import superduper\n\ndb = superduper('mongomock:///test_db')\n```\n\n<!-- TABS -->\n## Get useful sample data\n\n\n<Tabs>\n    <TabItem value=\"Text\" label=\"Text\" default>\n        ```python\n        # !curl -O https://superduperdb-public-demo.s3.amazonaws.com/text.json\n        import json\n        \n        with open('text.json', 'r') as f:\n            data = json.load(f)        \n        ```\n    </TabItem>\n    <TabItem value=\"PDF\" label=\"PDF\" default>\n        ```python\n        !curl -O https://superduperdb-public-demo.s3.amazonaws.com/pdfs.zip && unzip -o pdfs.zip\n        import os\n        \n        data = [f'pdfs/{x}' for x in os.listdir('./pdfs') if x.endswith('.pdf')]        \n        ```\n    </TabItem>\n</Tabs>\n```python\ndatas = [{'x': d} for d in data]\n```\n\n<!-- TABS -->\n## Insert simple data\n\nAfter turning on auto_schema, we can directly insert data, and superduper will automatically analyze the data type, and match the construction of the table and datatype.\n\n```python\nfrom superduper import Document\n\nids = db.execute(db['docs'].insert([Document(data) for data in datas]))\n```\n\n<!-- TABS -->\n## Apply a chunker for search\n\n:::note\nNote that applying a chunker is ***not*** mandatory for search.\nIf your data is already chunked (e.g. short text snippets or audio) or if you\nare searching through something like images, which can't be chunked, then this\nwon't be necessary.\n:::\n\n\n<Tabs>\n    <TabItem value=\"Text\" label=\"Text\" default>\n        ```python\n        from superduper import model\n        \n        CHUNK_SIZE = 200\n        \n        @model(flatten=True, model_update_kwargs={})\n        def chunker(text):\n            text = text.split()\n            chunks = [' '.join(text[i:i + CHUNK_SIZE]) for i in range(0, len(text), CHUNK_SIZE)]\n            return chunks        \n        ```\n    </TabItem>\n    <TabItem value=\"PDF\" label=\"PDF\" default>\n        ```python\n        !pip install -q \"unstructured[pdf]\"\n        from superduper import model\n        from unstructured.partition.pdf import partition_pdf\n        \n        CHUNK_SIZE = 500\n        \n        @model(flatten=True)\n        def chunker(pdf_file):\n            elements = partition_pdf(pdf_file)\n            text = '\\n'.join([e.text for e in elements])\n            chunks = [text[i:i + CHUNK_SIZE] for i in range(0, len(text), CHUNK_SIZE)]\n            return chunks        \n        ```\n    </TabItem>\n</Tabs>\nNow we apply this chunker to the data by wrapping the chunker in `Listener`:\n\n```python\nfrom superduper import Listener\n\nupstream_listener = Listener(\n    model=chunker,\n    select=db['docs'].select(),\n    key='x',\n    uuid=\"chunker\",\n    identifier='chunker',\n)\n```\n\n## Select outputs of upstream listener\n\n:::note\nThis is useful if you have performed a first step, such as pre-computing \nfeatures, or chunking your data. You can use this query to \noperate on those outputs.\n:::\n\n<!-- TABS -->\n## Build text embedding model\n\n\n<Tabs>\n    <TabItem value=\"OpenAI\" label=\"OpenAI\" default>\n        ```python\n        import os\n        os.environ['OPENAI_API_KEY'] = 'sk-<secret>'\n        from superduper_openai import OpenAIEmbedding\n        \n        embedding_model = OpenAIEmbedding(identifier='text-embedding-ada-002')        \n        ```\n    </TabItem>\n    <TabItem value=\"JinaAI\" label=\"JinaAI\" default>\n        ```python\n        import os\n        from superduper_jina import JinaEmbedding\n        \n        os.environ[\"JINA_API_KEY\"] = \"jina_xxxx\"\n         \n        # define the model\n        embedding_model = JinaEmbedding(identifier='jina-embeddings-v2-base-en')        \n        ```\n    </TabItem>\n    <TabItem value=\"Sentence-Transformers\" label=\"Sentence-Transformers\" default>\n        ```python\n        !pip install sentence-transformers\n        from superduper import vector\n        import sentence_transformers\n        from superduper_sentence_transformers import SentenceTransformer\n        \n        embedding_model = SentenceTransformer(\n            identifier=\"embedding\",\n            object=sentence_transformers.SentenceTransformer(\"BAAI/bge-small-en\"),\n            datatype=vector(shape=(1024,)),\n            postprocess=lambda x: x.tolist(),\n            predict_kwargs={\"show_progress_bar\": True},\n        )        \n        ```\n    </TabItem>\n</Tabs>\n## Create vector-index\n\n```python\nfrom superduper import VectorIndex, Listener\n\nvector_index_name = 'vector-index'\n\nvector_index = \\\n    VectorIndex(\n        vector_index_name,\n        indexing_listener=Listener(\n            key=upstream_listener.outputs,      # the `Document` key `model` should ingest to create embedding\n            select=db[upstream_listener.outputs].select(),       # a `Select` query telling which data to search over\n            model=embedding_model,         # a `_Predictor` how to convert data to embeddings\n            uuid=\"embedding-listener\",\n            identifier='embedding-listener',\n            upstream=[upstream_listener],\n        )\n    )\n```\n\n<!-- TABS -->\n## Create Vector Search Model\n\n```python\nitem = {'_outputs__chunker': '<var:query>'}\n```\n\n```python\nfrom superduper.components.model import QueryModel\n\nvector_search_model = QueryModel(\n    identifier=\"VectorSearch\",\n    select=db[upstream_listener.outputs].like(item, vector_index=vector_index_name, n=5).select(),\n    # The _source is the identifier of the upstream data, which can be used to locate the data from upstream sources using `_source`.\n    postprocess=lambda docs: [{\"text\": doc['_outputs__chunker'], \"_source\": doc[\"_source\"]} for doc in docs],\n    db=db\n)\n```\n\n<!-- TABS -->\n## Build LLM\n\n\n<Tabs>\n    <TabItem value=\"OpenAI\" label=\"OpenAI\" default>\n        ```python\n        from superduper_openai import OpenAIChatCompletion\n        \n        llm = OpenAIChatCompletion(identifier='llm', model='gpt-3.5-turbo')        \n        ```\n    </TabItem>\n    <TabItem value=\"Anthropic\" label=\"Anthropic\" default>\n        ```python\n        from superduper_anthropic import AnthropicCompletions\n        import os\n        \n        os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-xxx\"\n        \n        predict_kwargs = {\n            \"max_tokens\": 1024,\n            \"temperature\": 0.8,\n        }\n        \n        llm = AnthropicCompletions(identifier='llm', model='claude-2.1', predict_kwargs=predict_kwargs)        \n        ```\n    </TabItem>\n    <TabItem value=\"vLLM\" label=\"vLLM\" default>\n        ```python\n        from superduper_vllm import VllmModel\n        \n        predict_kwargs = {\n            \"max_tokens\": 1024,\n            \"temperature\": 0.8,\n        }\n        \n        \n        llm = VllmModel(\n            identifier=\"llm\",\n            model_name=\"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\",\n            vllm_kwargs={\n                \"gpu_memory_utilization\": 0.7,\n                \"max_model_len\": 1024,\n                \"quantization\": \"awq\",\n            },\n            predict_kwargs=predict_kwargs,\n        )        \n        ```\n    </TabItem>\n    <TabItem value=\"Transformers\" label=\"Transformers\" default>\n        ```python\n        from superduper_transformers import LLM\n        \n        llm = LLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", load_in_8bit=True, device_map=\"cuda\", identifier=\"llm\", predict_kwargs=dict(max_new_tokens=128))        \n        ```\n    </TabItem>\n    <TabItem value=\"Llama.cpp\" label=\"Llama.cpp\" default>\n        ```python\n        !huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.2-GGUF mistral-7b-instruct-v0.2.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\n        \n        from superduper_llama_cpp.model import LlamaCpp\n        llm = LlamaCpp(identifier=\"llm\", model_name_or_path=\"mistral-7b-instruct-v0.2.Q4_K_M.gguf\")        \n        ```\n    </TabItem>\n</Tabs>\n## Answer question with LLM\n\n```python\nfrom superduper import model\nfrom superduper.components.graph import Graph, input_node\n\nprompt_template = (\n    \"Use the following context snippets, these snippets are not ordered!, Answer the question based on this context.\\n\"\n    \"{context}\\n\\n\"\n    \"Here's the question: {query}\"\n)\n\n@model\ndef build_prompt(query, docs):\n    chunks = [doc[\"text\"] for doc in docs]\n    context = \"\\n\\n\".join(chunks)\n    prompt = prompt_template.format(context=context, query=query)\n    return prompt\n\n# We build a graph to handle the entire pipeline\n\n# create a input node, only have one input parameter `query`\nin_ = input_node('query')\n# pass the query to the vector search model\nvector_search_results = vector_search_model(query=in_)\n# pass the query and the search results to the prompt builder\nprompt = build_prompt(query=in_, docs=vector_search_results)\n# pass the prompt to the llm model\nanswer = llm(prompt)\n# create a graph, and the graph output is the answer\nrag = answer.to_graph(\"rag\")\n```\n\nBy applying the RAG model to the database, it will subsequently be accessible for use in other services.\n\n```python\nfrom superduper import Application\n\napp = Application(\n    'rag-app',\n    components=[\n        upstream_listener,\n        vector_index,\n        vector_search_model,\n        rag,\n    ]\n)\n\ndb.apply(app)\n```\n\nYou can now load the model elsewhere and make predictions using the following command.\n\n```python\nrag = db.load(\"model\", 'rag')\nprint(rag.predict(\"Tell me about superduper\")[0])\n```\n\n## Create template\n\n```python\nfrom superduper import Template\n\ntemplate = Template('rag-template', template=app, substitutions={'docs': 'collection'})\n```\n\n```python\ntemplate.export('.')\n```\n\n",
  "# Basic RAG tutorial\n\n:::info\nIn this tutorial we show you how to do retrieval augmented generation (RAG) with Superduper.\nNote that this is just an example of the flexibility and power which Superduper gives \nto developers. Superduper is about much more than RAG and LLMs. \n:::\n\nAs in the vector-search tutorial we'll use Superduper documentation for the tutorial.\nWe'll add this to a testing database by downloading the data snapshot:\n\n\n```python\n!curl -O https://superduper-public-demo.s3.amazonaws.com/text.json\n```\n\n\n```python\nimport json\n\nfrom superduper import superduper, Document\n\ndb = superduper('mongomock://test')\n\nwith open('text.json') as f:\n    data = json.load(f)\n\n_ = db['docu'].insert_many([{'txt': r} for r in data]).execute()\n```\n\nLet's verify the data in the `db` by querying one datapoint:\n\n\n```python\ndb['docu'].find_one().execute()\n```\n\nThe first step in a RAG application is to create a `VectorIndex`. The results of searching \nwith this index will be used as input to the LLM for answering questions.\n\nRead about `VectorIndex` [here](../apply_api/vector_index.md) and follow along the tutorial on \nvector-search [here](./vector_search.md).\n\n\n```python\nimport requests \n\nfrom superduper import Application, Document, VectorIndex, Listener, vector\nfrom superduper.ext.sentence_transformers.model import SentenceTransformer\nfrom superduper.base.code import Code\n\ndef postprocess(x):\n    return x.tolist()\n\ndatatype = vector(shape=384, identifier=\"my-vec\")\n    \nmodel = SentenceTransformer(\n    identifier=\"my-embedding\",\n    datatype=datatype,\n    predict_kwargs={\"show_progress_bar\": True},\n    signature=\"*args,**kwargs\",\n    model=\"all-MiniLM-L6-v2\",      \n    device=\"cpu\",\n    postprocess=Code.from_object(postprocess),\n)\n\nlistener = Listener(\n    identifier=\"my-listener\",\n    model=model,\n    key='txt',\n    select=db['docu'].find(),\n    predict_kwargs={'max_chunk_size': 50},\n)\n\nvector_index = VectorIndex(\n    identifier=\"my-index\",\n    indexing_listener=listener,\n    measure=\"cosine\"\n)\n\ndb.apply(vector_index)\n```\n\nNow that we've set up a `VectorIndex`, we can connect this index with an LLM in a number of ways.\nA simple way to do that is with the `SequentialModel`. The first part of the `SequentialModel`\nexecutes a query and provides the results to the LLM in the second part. \n\nThe `RetrievalPrompt` component takes a query with a \"free\" variable as input, signified with `<var:???>`. \nThis gives users great flexibility with regard to how they fetch the context\nfor their downstream models.\n\nWe're using OpenAI, but you can use any type of LLM with Superduper. We have several \nnative integrations (see [here](../ai_integraitons/)) but you can also [bring your own model](../models/bring_your_own_model.md).\n\n\n```python\nfrom superduper.ext.llm.prompter import *\nfrom superduper import Document\nfrom superduper.components.model import SequentialModel\nfrom superduper.ext.openai import OpenAIChatCompletion\n\nq = db['docu'].like(Document({'txt': '<var:prompt>'}), vector_index='my-index', n=5).find().limit(10)\n\ndef get_output(c):\n    return [r['txt'] for r in c]\n\nprompt_template = RetrievalPrompt('my-prompt', select=q, postprocess=Code.from_object(get_output))\n\nllm = OpenAIChatCompletion('gpt-3.5-turbo')\nseq = SequentialModel('rag', models=[prompt_template, llm])\n\ndb.apply(seq)\n```\n\nNow we can test the `SequentialModel` with a sample question:\n\n\n```python\nseq.predict('Tell be about vector-indexes')\n```\n\n:::tip\nDid you know you can use any tools from the Python ecosystem with Superduper.\nThat includes `langchain` and `llamaindex` which can be very useful for RAG applications.\n:::\n\n\n```python\nfrom superduper import Application\n\napp = Application('rag-app', components=[vector_index, seq, plugin_1, plugin_2])\n```\n\n\n```python\napp.encode()\n```\n\n\n```python\napp.export('rag-app')\n```\n\n\n```python\n!cat rag-app/requirements.txt\n```\n\n\n```python\nfrom superduper import *\n\napp = Component.read('rag-app')\n```\n\n\n```python\napp.info()\n```\n",
  "# Vector search\n\n:::note\nSince vector-search is all-the-rage right now, \nhere is the simplest possible iteration of semantic \ntext-search with a `sentence_transformers` model, \nas an entrypoint to `superduper`.\n\nNote that `superduper` is much-much more than vector-search\non text. Explore the docs to read about classical machine learning, \ncomputer vision, LLMs, fine-tuning and much much more!\n:::\n\n\nFirst let's get some data. These data are the markdown files \nof the very same documentation you are reading!\nYou can download other sample data-sets for testing `superduper`\nby following [these lines of code](../reusable_snippets/get_useful_sample_data).\n\n\n```python\nimport json\nimport requests \nr = requests.get('https://superduper-public-demo.s3.amazonaws.com/text.json')\n\nwith open('text.json', 'wb') as f:\n    f.write(r.content)\n\nwith open('text.json', 'r') as f:\n    data = json.load(f)        \n\nprint(data[0])\n```\n\nNow we connect to superduper, using MongoMock as a databackend.\nRead more about connecting to superduper [here](../core_api/connect) and\na semi-exhaustive list of supported data-backends for connecting [here](../reusable_snippets/connect_to_superduper).\n\n\n```python\nfrom superduper import superduper, Document\n\ndb = superduper('mongomock://test')\n\n_ = db['documents'].insert_many([Document({'txt': txt}) for txt in data]).execute()\n```\n\n\n```python\ndb.show()\n```\n\nWe are going to make these data searchable by activating a [`Model`](../apply_api/model) instance \nto compute vectors for each item inserted to the `\"documents\"` collection.\nFor that we'll use the [sentence-transformers](https://sbert.net/) integration to `superduper`.\nRead more about the `sentence_transformers` integration [here](../ai_integrations/sentence_transformers)\nand [here](../../api/ext/sentence_transformers/).\n\n\n```python\nfrom superduper.ext.sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\n    identifier=\"test\",\n    predict_kwargs={\"show_progress_bar\": True},\n    model=\"all-MiniLM-L6-v2\",\n    device=\"cpu\",\n    postprocess=lambda x: x.tolist(),\n)\n```\n\nWe can check that this model gives us what we want by evaluating an output \non a single data-point. (Learn more about the various aspects of `Model` [here](../models/).)\n\n\n```python\nmodel.predict(data[0])\n```\n\nNow that we've verified that this model works, we can \"activate\" it for \nvector-search by creating a [`VectorIndex`](../apply_api/vector_index).\n\n\n```python\nimport pprint\n\nvector_index = model.to_vector_index(select=db['documents'].find(), key='txt')\n\npprint.pprint(vector_index)\n```\n\nYou will see that the `VectorIndex` contains a [`Listener`](../apply_api/listener) instance.\nThis instance wraps the model, and configures it to compute outputs \non data inserted to the `\"documents\"` collection with the key `\"txt\"`.\n\nTo activate this index, we now do:\n\n\n```python\ndb.apply(vector_index)\n```\n\nThe `db.apply` command is a universal command for activating AI components in superduper.\n\nYou will now see lots of output - the model-outputs/ vectors are computed \nand the various parts of the `VectorIndex` are registered in the system.\n\nYou can verify this with:\n\n\n```python\ndb.show()\n```\n\n\n```python\ndb['documents'].find_one().execute().unpack()\n```\n\nTo \"use\" the `VectorIndex` we can execute a vector-search query:\n\n\n```python\nquery = db['documents'].like({'txt': 'Tell me about vector-search'}, vector_index=vector_index.identifier, n=3).find()\ncursor = query.execute()\n```\n\nThis query will return a cursor of [`Document`](../fundamentals/document) instances.\nTo obtain the raw dictionaries, call the `.unpack()` command:\n\n\n```python\nfor r in cursor:\n    print('=' * 100)\n    print(r.unpack()['txt'])\n    print('=' * 100)\n```\n\nYou should see that the documents returned are relevant to the `like` part of the \nquery.\n\nLearn more about building queries with `superduper` [here](../execute_api/overview.md).\n",
  "# Custom serialization\n\nIn this tutorial, we demonstrate how developers can flexibily and portably define\ntheir own classes in Superduper. These may be exported with `Component.export` \nand transported to other Superduper deployments with `db.apply`.\n\nTo make our lives difficult, we'll include a data blob in the model, which should be serialized with the \nexported class:\n\n\n```python\n!curl -O https://superduper-public-demo.s3.amazonaws.com/text.json\nimport json\n\nwith open('text.json') as f:\n    data = json.load(f)\n```\n\nWe'll define our own `Model` descendant, with a custom `.predict` method. \nWe are free to define any of our own parameters to this class with a simple annotation in the header, since `Model` \nis a `dataclasses.dataclass`:\n\n\n```python\nfrom superduper import *\n\n\nrequires_packages(['openai', None, None])\n\n\nclass NewModel(Model):\n    a: int = 2\n    b: list\n\n    def predict(self, x):\n        return x * self.a\n```\n\nIf we want `b` to be saved as a blob in the `db.artifact_store` we can simply\nannotate this in the `artifacts=...` parameter, supplying the serializer we would like to use:\n\n\n```python\nm = NewModel('test-hg', a=2, b=data, artifacts={'b': pickle_serializer})\n```\n\nNow we can export the model:\n\n\n```python\nm.export('test-hg')\n```\n\n\n```python\n!cat test-hg/component.json\n```\n\n\n```python\n!ls test-hg/blobs/\n```\n\nThe following cell works even after restarting the kernel.\nThat means the exported component is now completely portable!\n\n\n```python\nfrom superduper import *\n\nc = Component.read('test-hg')\n\nc.predict(2)\n```\n",
  "# Training and Managing MNIST Predictions with superduper\n\n:::note\nThis tutorial guides you through the implementation of a classic machine learning task: MNIST handwritten digit recognition. The twist? We perform the task directly on data hosted in a database using superduper.\n:::\n\nThis example makes it easy to connect any of your image recognition models directly to your database in real-time. \n\n\n```python\n!pip install torch torchvision\n```\n\nFirst, we need to establish a connection to a MongoDB datastore via superduper. \n\n\n```python\nfrom superduper import superduper\n    \ndb = superduper('mongomock://')\n```\n\nAfter establishing a connection to MongoDB, the next step is to load the MNIST dataset. superduper's strength lies in handling diverse data types, especially those that are not supported by standard databases. To achieve this, we use an `Encoder` in conjunction with `Document` wrappers. These components allow Python dictionaries containing non-JSONable or bytes objects to be seamlessly inserted into the underlying data infrastructure.\n\n\n```python\nimport torchvision\nfrom superduper import Document\n\nimport random\n\n# Load MNIST images as Python objects using the Python Imaging Library.\n# Each MNIST item is a tuple (image, label)\nmnist_data = list(torchvision.datasets.MNIST(root='./data', download=True))\n\ndocument_list = [Document({'img': x[0], 'class': x[1]}) for x in mnist_data]\n\n# Shuffle the data and select a subset of 1000 documents\nrandom.shuffle(document_list)\ndata = document_list[:1000]\n\n# Insert the selected data into the mnist_collection which we mentioned before like: mnist_collection = Collection('mnist')\ndb['mnist'].insert_many(data[:-100]).execute()\n```\n\nNow that the images and their classes are inserted into the database, we can query the data in its original format. Particularly, we can use the `PIL.Image` instances to inspect the data.\n\n\n```python\n# Get and display one of the images\nr = db['mnist'].find_one().execute()\nr.unpack()['img'].resize((300, 300))\n```\n\nFollowing that, we build our machine learning model. superduper conveniently supports various frameworks, and for this example, we opt for PyTorch, a suitable choice for computer vision tasks. In this instance, we combine `torch` with `torchvision`.\n\nTo facilitate communication with the superduper `Datalayer`, we design `postprocess` and `preprocess` functions. These functions are then wrapped with the `TorchModel` wrapper to create a native superduper object.\n\n\n```python\nfrom superduper.ext.torch import TorchModel\n\nimport torch\n\n# Define the LeNet-5 architecture for image classification\nclass LeNet5(torch.nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        # Layer 1\n        self.layer1 = torch.nn.Sequential(\n            torch.nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n            torch.nn.BatchNorm2d(6),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n        # Layer 2\n        self.layer2 = torch.nn.Sequential(\n            torch.nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n            torch.nn.BatchNorm2d(16),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n        # Fully connected layers\n        self.fc = torch.nn.Linear(400, 120)\n        self.relu = torch.nn.ReLU()\n        self.fc1 = torch.nn.Linear(120, 84)\n        self.relu1 = torch.nn.ReLU()\n        self.fc2 = torch.nn.Linear(84, num_classes)\n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.reshape(out.size(0), -1)\n        out = self.fc(out)\n        out = self.relu(out)\n        out = self.fc1(out)\n        out = self.relu1(out)\n        out = self.fc2(out)\n        return out\n\n# Postprocess function for the model output    \ndef postprocess(x):\n    return int(x.topk(1)[1].item())\n\n# Preprocess function for input data\ndef preprocess(x):\n    return torchvision.transforms.Compose([\n        torchvision.transforms.Resize((32, 32)),\n        torchvision.transforms.ToTensor(),\n        torchvision.transforms.Normalize(mean=(0.1307,), std=(0.3081,))]\n    )(x)\n\n# Create an instance of the LeNet-5 model\nlenet_model = LeNet5(10)\n\n\nmodel = TorchModel(\n    identifier='my-model',\n    object=lenet_model,\n    preprocess=preprocess,\n    postprocess=postprocess, \n    preferred_devices=('cpu',),\n)\n\n# Check that the model successfully creates predictions over single data-points\nmodel.predict(data[0]['img'])\n```\n\nNow we are ready to \"train\" or \"fit\" the model. Trainable models in superduper come with a sklearn-like `.fit` method,\nwhich developers may implement for their specific model class. `torch` models come with a pre-configured\n`TorchTrainer` class and `.fit` method. These may be invoked simply by \"applying\" the model to `db`:\n\n\n```python\nfrom torch.nn.functional import cross_entropy\n\nfrom superduper import Metric, Validation, Dataset\nfrom superduper.ext.torch import TorchTrainer\n\nacc = lambda x, y: (sum([xx == yy for xx, yy in zip(x, y)]) / len(x))\n\naccuracy = Metric(identifier='acc', object=acc)\n\nmodel.validation = Validation(\n    'mnist_performance',\n    datasets=[\n        Dataset(\n            identifier='my-valid',\n            select=db['mnist'].find({'_fold': 'valid'})\n        )\n    ],\n    metrics=[accuracy],\n)\n\nmodel.trainer = TorchTrainer(\n    identifier='my-trainer',\n    objective=cross_entropy,\n    loader_kwargs={'batch_size': 10},\n    max_iterations=1000,\n    validation_interval=5,\n    select=db['mnist'].find(),\n    key=('img', 'class'),\n    transform=lambda x, y: (preprocess(x), y),\n)\n\n_ = db.apply(model)\n```\n\nThe trained model is now available via `db.load` - the `model.trainer` object contains the metric traces\nlogged during training.\n\n\n```python\nfrom matplotlib import pyplot as plt\n\n# Load the model from the database\nmodel = db.load('model', model.identifier)\n\n# Plot the accuracy values\nplt.plot(model.trainer.metric_values['my-valid/acc'])\nplt.show()\n```\n",
  "# Tutorials\n\nIn this section we guide newcomers through the most \nbasic usage pattern in Superduper.\n\nFor more detailed, flexible and realistic use-cases, \nrefer to the [use-cases section](/use_cases).\n\n```mdx-code-block\nimport DocCardList from '@theme/DocCardList';\n\n<DocCardList />\n```",
  "# Eager Mode (Alpha) \n\nEager Mode is an interactive way to build superduper applications. \n\nUsers can input data as usual, continuously call models, and view results.\nOnce the interactive debugging and construction are complete, \nthe corresponding data pipeline can be built directly through `apply`, \neliminating the need for direct debugging between AI application models and databases.\n\n:::note\nThis feature is in alpha.\n:::\n\nConnect the database and insert data.\n\n\n```python\nfrom superduper import superduper\n\ndb = superduper('mongomock://test')\n```\n\n\n```python\nimport numpy as np\ndata = [\n    {\"x\": 1, \"y\": \"2\", \"z\": np.array([1, 2, 3])},\n    {\"x\": 2, \"y\": \"3\", \"z\": np.array([4, 5, 6])},\n    {\"x\": 3, \"y\": \"4\", \"z\": np.array([7, 8, 9])},\n]\n\ndb[\"documents\"].insert(data).execute()\n```\n\nWhen using `select.execute(eager_mode=True)`, all returned data will enter eager mode, which can be used for interactive model pipeline construction.\n\n\n```python\ndata = list(db[\"documents\"].select().execute(eager_mode=True))[0]\ndata\n```\n\nDefine the first model and make predictions.\n\n\n```python\nfrom superduper import ObjectModel\ndef func_a(x):\n    return {\"x\": x, \"model\": \"a\"}\n\nmodel_a = ObjectModel(identifier=\"a\", object=func_a)\noutput_a = model_a(data[\"x\"])\noutput_a\n```\n\nDefine the second model and make predictions.\n\n\n```python\ndef func_b(x, y, o_a):\n    return {\n        \"x\": x,\n        \"y\": y,\n        \"o_a\": o_a,\n        \"model\": \"b\"\n    }\n\nmodel_b = ObjectModel(identifier=\"b\", object=func_b)\noutput_b = model_b(data[\"x\"], data[\"y\"], output_a)\noutput_b\n```\n\nDefine the third model and make predictions.\n\n\n```python\ndef func_c(x, y, z, o_a, o_b):\n    return {\n        \"x\": x,\n        \"y\": y,\n        \"z\": z,\n        \"o_a\": o_a,\n        \"o_b\": o_b,\n        \"model\": \"c\",\n    }\n\nmodel_c = ObjectModel(identifier=\"c\", object=func_c)\noutput_c = model_c(data[\"x\"], data[\"y\"], data[\"z\"], output_a, output_b)\noutput_c\n```\n\nApply all models to the data to start monitoring the data and making predictions.\nWhen adding a model result, not only the current model but also the recursively dependent upstream models will be added.\n\n\n```python\noutput_c.apply()\n```\n\n\n```python\nlist(db[\"documents\"].select().outputs(\"a\", \"b\", \"c\").select().execute())\n```\n\nIf you want to modify the predict_id of a specific model, \nyou can use `output.predict_id = \"your_predict_id\"` to set it.\n\n\n```python\nmodel_predict_id = ObjectModel(identifier=\"c\", object=func_c)\noutput_predict_id = model_predict_id(data[\"x\"], data[\"y\"], data[\"z\"], output_a, output_b)\noutput_predict_id.predict_id = \"new_predict_id\"\noutput_predict_id.apply()\n```\n\nView the prediction results of all data in the database.\n\n\n```python\nlist(db[\"_outputs.new_predict_id\"].select().execute())\n```\n\nIf you want to perform if-like conditional operations to route data using different models, you can use `set_condition` to handle it. Currently, only equals and not equals conditions are supported.\n\n\n```python\nmodel_condition = ObjectModel(identifier=\"condition\", object=func_a)\noutput_condition = model_condition(data[\"x\"])\noutput_condition.set_condition(data[\"x\"] == 1)\noutput_condition.apply()\noutput_condition\n```\n\n\n```python\ndb['documents'].find({}, {'x': 1, '_builds': 1, '_files': 1, '_blobs': 1, '_schema': 1}).filter({'x': 1}).execute()\n```\n\n\n```python\nlist(db[\"_outputs.condition\"].select().execute())\n```\n",
  "# Listening for new data\n\n:::note\nIn Superduper, AI models may be configured to listen for newly inserted data.\nOutputs will be computed over that data and saved back to the data-backend.\n:::\n\nIn this example we show how to configure 3 models to interact when new data is added.\n\n1. A featurizing computer vision model (images `->` vectors).\n1. 2 models evaluating image-2-text similarity to a set of key-words.\n\nWe use an open-source model \"CLIP\" which we install via `pip` directly from GitHub.\nYou can read more about installing requirements on our docs [here](../get_started/environment).\n\n\n```python\n!pip install git+https://github.com/openai/CLIP.git\n```\n\nWe apply our setup to images from the \n[cats and dogs dataset](https://www.kaggle.com/c/dogs-vs-cats). We've prepared a subset especially \nfor quick experimentation.\n\n\n```python\n!curl -O https://superduper-public-demo.s3.amazonaws.com/images.zip && unzip images.zip\nfrom PIL import Image\nimport os\n\ndata = [f'images/{x}' for x in os.listdir('./images') if x.endswith('png')]\ndata = [{'img': Image.open(path)} for path in data]\n```\n\nNow that we've prepared these records we can insert this data \"directly\" into the database with \na standard insert statement. (Notice however the difference from `pymongo` with the `.execute()` call.)\nThe same pattern may be applied to other database types.\n\n\n```python\nfrom superduper import superduper, Document\n\ndb = superduper('mongomock://')\n\ndb['images'].insert_many([Document(r) for r in data[:-1]]).execute()\n```\n\nWe can verify that the images are correctly saved by retrieved a single record:\n\n\n```python\nr = db['images'].find_one().execute()\nr\n```\n\nThe contents of the `Document` may be accessed by calling `.unpack()`. You can see that the images were saved and retrieved correctly.\n\n\n```python\nr.unpack()['img']\n```\n\nWe now build a `torch` model for text-2-image similarity using the `clip` library. In order to \nsave the outputs correctly in the system, we add the `tensor` datatype to the model:\n\n\n```python\nimport clip\nimport torch\nfrom superduper.ext.torch import TorchModel, tensor\n\n\nmodel, preprocess = clip.load(\"ViT-B/32\", \"cpu\")\n\nclass ImageModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = model\n\n    def forward(self, image_tensors):\n        return self.model.encode_image(image_tensors)\n\n\ndt = tensor(dtype='float', shape=(512,))\n\n\nimage_model = TorchModel(\n    identifier='clip_image',\n    object=ImageModel(),\n    preprocess=preprocess,\n    datatype=dt,\n    loader_kwargs={'batch_size': 5},\n)\n```\n\nWe can verify that this model gives us the correct outputs on the added data with the `.predict` method:\n\n\n```python\nimage_model.predict(data[0]['img'])\n```\n\nNow we'd like to set up this model to compute outputs on the `'img'` key of each record. \nTo do that we create a `Listener` (see [here](../apply_api/listener) for more information) which \n\"listens\" for incoming and existing data, and computes outputs on that data.\n\nWhen new data is inserted, the model automatically will create outputs on that data. This is a very handy \nfeature for productionizing AI and ML, since a data deployment needs to be keep up-to-date as far as possible.\n\n\n```python\nlistener = image_model.to_listener(\n    select=db['images'].find(),\n    key='img',\n    identifier='image_predictions',\n)\n\n_ = db.apply(listener)\n```\n\nWe can verify that the outputs are correctly inserted into the documents with this query. \nThe outputs are saved in the `listener.outputs` field:\n\n\n```python\nlist(listener.outputs_select.limit(1).execute())[0][listener.outputs].unpack()\n```\n\nDownstream of this first model, we now can add another smaller model, to classify images with configurable terms. \nSince the dataset is concerned with cats and dogs we create 2 downstream models classifying the images in 2 different ways.\n\n\n```python\nfrom superduper import ObjectModel\n\n\nclass Comparer:\n    def __init__(self, words, text_features):\n        self.targets = {w: text_features[i] for i, w in enumerate(words)}\n        self.lookup = list(self.targets.keys())\n        self.matrix = torch.stack(list(self.targets.values()))\n\n    def __call__(self, vector):\n        best = (self.matrix @ vector).topk(1)[1].item()\n        return self.lookup[best]\n\n\ncats_vs_dogs = ObjectModel(\n    'cats_vs_dogs',\n    object=Comparer(['cat', 'dog'], model.encode_text(clip.tokenize(['cat', 'dog']))),\n).to_listener(\n    select=db['images'].find(),\n    key=listener.outputs,\n)\n\n            \nfelines_vs_canines = ObjectModel(\n    'felines_vs_canines',\n    object=Comparer(['feline', 'canine'], model.encode_text(clip.tokenize(['feline', 'canine']))),\n).to_listener(\n    select=db['images'].find(),\n    key=listener.outputs,\n)\n\n\ndb.apply(cats_vs_dogs)\ndb.apply(felines_vs_canines)\n```\n\nWe can verify that both downstream models have written their outputs to the database by querying a document:\n\n\n```python\nr = db['images'].find_one().execute()\n\nprint(r[cats_vs_dogs.outputs])\nprint(r[felines_vs_canines.outputs])\n```\n\nLet's check that the predictions make sense for the inserted images:\n\n\n```python\ndb['images'].find_one({cats_vs_dogs.outputs: 'cat'}).execute()['img']\n```\n\n\n```python\ndb['images'].find_one({felines_vs_canines.outputs: 'feline'}).execute()['img']\n```\n\n\n```python\ndb['images'].find_one({cats_vs_dogs.outputs: 'dog'}).execute()['img']\n```\n\n\n```python\ndb['images'].find_one({felines_vs_canines.outputs: 'canine'}).execute()['img']\n```\n\nNow that we have installed the models using `Listener`, we can insert new data. This \ndata should be automatically processed by the installed models:\n\n\n```python\ndb['images'].insert_one(Document({**data[-1], 'new': True})).execute()\n```\n\nWe can verify this by querying the data again:\n\n\n```python\nr = db['images'].find_one({'new': True}).execute().unpack()\nr['img']\n```\n\nYou see here that the models have been called in the correct order on the newly added data and the outputs saved \nto the new record:\n\n\n```python\nr['_outputs']\n```\n",
  "# Core superduper usage\n\nIn this section we walk through how to perform the key operations with superduper.\nThere are three key patterns C-A-E:\n\n***Connect***\n\n```python\nfrom superduper import superduper\ndb = superduper('<data-connection>')\n```\n\n***Apply***\n\n```python\ndb.apply(<ai_component>)\n```\n\n***Execute***\n\n```python\nto_execute = <build_your_database_query_or_model_prediction>\ndb.execute(to_execute)\n```\n",
  "# Apply\n\nIn superduper there are three fundamental base components which you'll use for the majority of your functionality:\n\n- [`Model`](../apply_api/model)\n- [`Listener`](../apply_api/listener)\n- [`VectorIndex`](../apply_api/vector_index)\n\nIn addition there is an overarching component:\n\n- [`Application`](../apply_api/Application)\n\nwhich in some sense \"rules them all\"\n\nWhenever you wish to apply AI to your data, you will instantiate one of more of these, and \"apply\" these to \nyour connection:\n\n```python\ndb.apply(component)\n```\n\n## Base components\n\n### `Model`\n\nA `Model` is a wrapper around a standard ML/ AI model. It may contain additional functionality, such as \npre- and post-processing, and encoding/ decoding data into/ from the correct type required by the database.\n\n`db.apply(model)` tells `superduper` to store the model and its metadata in the system.\n\nIf additional configurations, such as training parameters, are added to the `Model` then the `db.apply` command\nwill also train the component on data in superduper.\n\nRead more about `Model` [here](../apply_api/model).\n\n### `Listener`\n\nA `Listener` wraps a `Model`. The `db.apply(listener)` tells `superduper` to \"listen\" for incoming data and to compute outputs on those data, saving them back in `superduper`.\n\nRead more about `Listener` [here](../apply_api/listener).\n\n### `VectorIndex`\n\nA `VectorIndex` wraps one or two `Listener` components, and tells `superduper` that the outputs computed, should\nbe made searchable via vector-search queries.\n\nRead more about `VectorIndex` [here](../apply_api/vector_index).\n\n## Connecting component: `Stack`\n\nA `Stack` of AI functionality is a combination of multiple `Model`, `Listener`, and `VectorIndex` components which may be \"applied\" in \none pass to your data via superduper. \n\nOn `db.add(stack)` superduper performs the heavy lifting of deciding which components need to be applied \nfirst, which need to be modified on incoming data, and which outputs need to be made searchable.\n\nRead more about `Stack` [here](../apply_api/stack).\n\n## View applied components\n\nUse `db.show` to view components.\n\nView all components:\n\n```python\n>>> db.show()\n[\n  {'type_id': 'model', 'identifier': 'my-model'},\n  {'type_id': 'model', 'identifier': 'my-other-model'}\n]\n```\n\nView all components of a certain type:\n\n```python\n>>> db.show('<type_id>')\n['my-model', 'my-other-model']\n```\n\nView all versions of a particular component:\n\n```python\n>>> db.show('<type_id>', '<component_identifier>')\n[0, 1, 2, 3]\n```\n\n## Reloading applied components\n\nWhen components are applied with `db.apply(component)`, the component is provided with a version, which may be optionally used to reload the component.\nBy default the latest version is reloaded:\n\n```python\nreloaded = db.load('<type_id>', '<component_identifier>')\n```\n\n```python\nreloaded = db.load('<type_id>', '<component_identifier>', <version>)\n```\n\nFor example to reload a model, identified by 'my-model', the first version added:\n\n```python\nreloaded_model = db.load('model', 'my-model', 0)\n```\n\n## Read more\n\nRead more about the \"apply\" API [here](../apply_api/component.md).",
  "# Connect\n\nThe standard way to connect to Superduper is via the `superduper` decorator:\n\n## Development mode\n\nIn [development mode](../get_started/environment#development-mode), you may provide the URI/ connection details of your data deployment directly\n\n```python\ndb = superduper('<database-uri>')\n```\n\nFor example if you are running a (not secure) MongoDB deployment locally, and you want to connect to the `\"documents\"` database, you might write:\n\n```python\nfrom superduper import superduper\ndb = superduper('mongodb://localhost:27017/documents')\n```\n\n### Complete connection guide\n\nFor a semi-exhaustive list of possible connections see [here](../reusable_snippets/connect_to_superduper).\n\n### Fine grained configuration\n\n`superduper` chooses default `artifact_store` (file blob storage) and `metadata_store` (AI metadata) values for your connection. These defaults may be overridden directly:\n\n```python\ndb = superduper(\n    '<database-uri>',\n    artifact_store='<artifact-store-uri>,\n    metadata_store='<metadata-store-uri>,\n)\n```\n\n## Cluster mode\n\nIn [cluster mode](../get_started/environment#cluster-mode), the connection string needs to be provided in a configuration \nfile or via environment variable so that all services can connect correctly:\n\nAdd these lines to your configuration:\n\n```yaml\ndata_backend: mongodb://localhost:27018/documents\n```\n\nRead more about configuration [here](../get_started/configuration).\n\nAfter doing this, you may connect directly with the `superduper` decorator:\n\n```python\ndb = superduper()\n```\n\n### Fine grained configuration\n\nFor more granular configuration add these lines:\n\n\n```yaml\ndata_backend: <database-uri>,\nartifact_store: <artifact-store-uri>\nmetadata_store: <metadata-store-uri>\n```\n\n## Next steps\n\n`db` is now your connection to your data, models, and model meta-data.\nNow that you have established this connection you are ready to build, deploy and manage AI with Superduper.\n",
  "# Execute\n\n`db.execute` is superduper's wrapper around standard database queries:\n\n- Inserts\n- Selects\n- Updates\n- Deletes\n\nAs well as model predictions:\n\n- Prediction on single data points (streaming)\n- Predictions on multiple data points (batch prediction)\n\nAnd also queries which consist of a combination of model computations and data operations:\n\n- Vector-search queries\n- Complex model predictions which include database queries (e.g. \"RAG\")\n\nStandard database queries are built using a compositional syntax similar to that found in Python database clients \nsuch as `pymongo` and `ibis`. The API also includes extensions of this paradigm to cover model predictions\nand vector-searches.\n\nRead more about the differences and approaches to document stores/ SQL data-backends [here](docs/data_integrations).\n\n## Building queries/ predictions\n\nAll queries consist of a \"chain\" of methods executed over a base object. The base object \ncan refer to a table/ collection or a model:\n\n```python\nq = base_object.method_1(*args_1, **kwargs_1).method_2(*args_2, **kwargs_2)....\n```\n\n### Selects\n\n***MongoDB***\n\nA MongoDB `find` query can be built like this:\n\n```python\nq = db['collection'].find().limit(5).skip(2)\n```\n\n***SQL***\n\nA query with on an SQL data-backend can be built with `ibis` syntax like this:\n\n```python\nq = db['documents'].filter(t.brand == 'Nike').limit(5)\n```\n\n### Inserts\n\n***MongoDB***\n\nTypically insert queries wrap `Document` instances and call the `insert` method on a table or collection:\n\n```python\nq = db['documents'].insert_many([Document(r) for r in data])\n```\n\n***SQL***\n\nThe `ibis` insert is slightly different:\n\n```python\nq = db['documents'].insert([Document(r) for r in data])\n```\n\n## Executing the query\n\n\n```python\nresults = q.execute()\n```\n\n***Multiple results***\n\nIterables of results are sent wrapped in a cursor\n\n***Indiviudal results***\n\nIndividual results are sent wrapped in a `Document`\n\nRead more about `.execute` [here](../execute_api/overview).",
  "# Anthropic\n\n## Installation\n\n```bash\npip install superduper_anthropic\n```\n\n## API\n\n`superduper` allows users to work with `anthropic` API models. The key integration is the integration \nof high-quality API-hosted LLM services.\n\n| Class | Description | GitHub | API-docs |\n| --- | --- | --- | --- |\n| `superduper.ext.anthropic.AnthropicCompletions` | Completes a prompt with natural language (LLM) | [Code](https://github.com/superduper/superduper/blob/main/superduper/ext/anthropic/model.py) | [Docs](/docs/api/ext/anthropic/model#anthropiccompletions) |",
  "# OpenAI\n\n## Installation\n\n```bash\npip install superduper_openai\n```\n\n## API\n\n`superduper` allows users to work with `openai` API models.\n\n| Class | Description | GitHub | API-docs |\n| --- | --- | --- | --- |\n| `superduper.ext.openai.OpenAIChatCompletion` | Completes a prompt with natural language (LLM) | [Code](https://github.com/superduper/superduper/blob/main/superduper/ext/openai/model.py) | [Docs](/docs/api/ext/openai/model#openaichatcompletion) |\n| `superduper.ext.openai.OpenAIEmbedding` | Embeds a piece of text | [Code](https://github.com/superduper/superduper/blob/main/superduper/ext/openai/model.py) | [Docs](/docs/api/ext/openai/model#openaiembedding) |\n| `superduper.ext.openai.OpenAIImageCreation` | Creates an image dependent on a prompt | [Code](https://github.com/superduper/superduper/blob/main/superduper/ext/openai/model.py) | [Docs](/docs/api/ext/openai/model#openaiimagecreation) |\n| `superduper.ext.openai.OpenAIAudioTranscription` | Transcribes audio to text | [Code](https://github.com/superduper/superduper/blob/main/superduper/ext/openai/model.py) | [Docs](/docs/api/ext/openai/model#openaiaudiotranscription) |\n| `superduper.ext.openai.OpenAIAudioTranslation` | Translates audio | [Code](https://github.com/superduper/superduper/blob/main/superduper/ext/openai/model.py) | [Docs](/docs/api/ext/openai/model#openaiaudiotranslation) |",
  "---\nsidebar_position: 3\n---\n\n# PyTorch\n\n## Installation\n\n```bash\npip install superduper_torch\n```\n\n## API\n\n`superduper` allows users to work with arbitrary `torch` models, with custom pre-, post-processing and input/ output data-types,\nas well as offering training with `superduper`\n\n\n| Class | Description | GitHub | API-docs |\n| --- | --- | --- | --- |\n| `superduper.ext.torch.model.TorchModel` | Wraps a PyTorch model | [Code](https://github.com/superduper/superduper/blob/main/superduper/ext/torch/model.py) | [Docs](/docs/api/ext/torch/model#torchmodel-1) |\n| `superduper.ext.torch.model.TorchTrainer` | May be attached to a `TorchModel` for training | [Code](https://github.com/superduper/superduper/blob/main/superduper/ext/torch/training.py) | [Docs](/docs/api/ext/torch/training#torchtrainer)",
  "# Cohere\n\n## Installation\n\n```bash\npip install superduper_cohere\n```\n\n## API\n\n`superduper` allows users to work with `cohere` API models.\n\n\n| Class | Description | GitHub | API-docs |\n| --- | --- | --- | --- |\n| `superduper.ext.cohere.CohereEmbed` | Embeds a piece of text as a vector | [Code](https://github.com/superduper/superduper/blob/main/superduper/ext/cohere/model.py) | [Docs](/docs/api/ext/cohere/model#cohereembed) |\n| `superduper.ext.cohere.CohereGenerate` | Completes a piece of text with most likely completion | [Code](https://github.com/superduper/superduper/blob/main/superduper/ext/cohere/model.py) | [Docs](/docs/api/ext/cohere/model#coheregenerate) |",
  "---\nsidebar_position: 1\n---\n\n# Community Support\n\nThe primary way in which developers will integrate and implement functionality from popular AI frameworks, is via\nthe `Predictor` and `Model` abstractions.\n\nThe `Predictor` mixin class, interfaces with all AI frameworks and API providers, which provide `self.predict` functionality,\nand is subclassed by:\n\n| class | framework |\n| --- | --- |\n| `superduper.ext.sklearn.Estimator` | [Scikit-Learn](https://scikit-learn.org/stable/) |\n| `superduper.ext.transformers.Pipeline` | [Hugging Face's `transformers`](https://huggingface.co/docs/transformers/index) |\n| `superduper.ext.torch.TorchModel` | [PyTorch](https://pytorch.org/) |\n| `superduper.ext.openai.OpenAI` | [OpenAI](https://api.openai.com) |\n| `superduper.ext.cohere.Cohere` | [Cohere](https://cohere.com) |\n| `superduper.ext.anthropic.Anthropic` | [Anthropic](https://anthropic.com) |\n| `superduper.ext.jina.Jina` | [Jina](https://jina.ai/embeddings) |\n\nThe `Model` class is subclassed by:\n\n| class | framework |\n| --- | --- |\n| `superduper.ext.sklearn.Estimator` | [Scikit-Learn](https://scikit-learn.org/stable/) |\n| `superduper.ext.transformers.Pipeline` | [Hugging Face's `transformers`](https://huggingface.co/docs/transformers/index) |\n| `superduper.ext.torch.TorchModel` | [PyTorch](https://pytorch.org/) |\n\n`Model` instances implement `self.predict`, but also hold import data, such as model weights, parameters or hyperparameters.\nIn addition, `Model` may implement `self.fit` functionality - model training and calibration.",
  "# Sentence-Transformers\n\n## Installation\n\n```bash\npip install superduper_sentence_transformers\n```\n\n## API\n\n`superduper` allows users to work with self-hosted embedding models via \"[Sentence-Transformers](https://sbert.net/)\".\n\n| Class | Description | GitHub | API-docs |\n| --- | --- | --- | --- |\n| `superduper.ext.sentence_transformers.model.SentenceTransformer` | Embeds a piece of text with a model hosted locally | [Code](https://github.com/superduper/superduper/blob/main/superduper/ext/sentence_transformers/model.py) | [Docs](/docs/api/ext/sentence_transformers/model#sentencetransformer) |",
  "# Scikit-learn\n\n## Installation\n\n```bash\npip install superduper_sklearn\n```\n\n## API\n\n`superduper` allows users to work with arbitrary `sklearn` estimators, with additional support for pre-, post-processing and input/ output data-types.\n\nRead more about this [here](/docs/docs/walkthrough/ai_models#scikit-learn).\n\n| Class | Description | GitHub | API-docs |\n| --- | --- | --- | --- |\n| `superduper.ext.sklearn.model.Estimator` | Wraps a scikit-learn estimator | [Code](https://github.com/superduper/superduper/blob/main/superduper/ext/sklearn/model.py) | [Docs](/docs/api/ext/sklearn/model#estimator) |\n| `superduper.ext.sklearn.model.SklearnTrainer` | May be attached to an `Estimator` for training | [Code](https://github.com/superduper/superduper/blob/main/superduper/ext/sklearn/model.py) | [Docs](/docs/api/ext/sklearn/model#sklearntrainer) |",
  "# vLLM\n\n## Installation\n\n```bash\npip install superduper_vllm\n```\n\n## API\n\n`superduper` allows users to work with self-hosted LLM models via \"[vLLM](https://github.com/vllm-project/vllm)\".\n\n| Class | Description | GitHub | API-docs |\n| --- | --- | --- | --- |\n| `superduper.ext.vllm.VllmModel` | Completes a prompt with natural language (LLM) based on a self hosted LLM | [Code](https://github.com/superduper/superduper/blob/main/superduper/ext/vllm/model.py) | [Docs](/docs/api/ext/vllm/model#vllmmodel) |\n| `superduper.ext.vllm.VllmAPI` | Completes a prompt with natural language (LLM) based on a self hosted LLM behind the vLLM API server | [Code](https://github.com/superduper/superduper/blob/main/superduper/ext/vllm/model.py) | [Docs](/docs/api/ext/vllm/model#vllmapi) |\n\n",
  "# Transformers\n\n## Installation\n\n```bash\npip install superduper_transformers\n```\n\n## API\n\n[Transformers](https://huggingface.co/docs/transformers/index) is a popular AI framework, and we have incorporated native support for Transformers to provide essential Large Language Model (LLM) capabilities.\n`superduper` allows users to work with arbitrary `transformers` pipelines, with custom input/ output data-types.\n\n| Class | Description | GitHub | API-docs |\n| --- | --- | --- | --- |\n| `superduper.ext.transformers.model.TextClassification` | A pipeline for classifying text. | [Code](https://github.com/superduper/superduper/blob/main/superduper/transformers/model.py) | [Docs](/docs/api/ext/transformers/model#textclassificationpipeline) |\n| `superduper.ext.transformers.model.LLM` | Work locally with the `transformers` implementations of LLM. | [Code](https://github.com/superduper/superduper/blob/main/superduper/ext/transformers/model.py) | [Docs](/docs/api/ext/transformers/model#llm) |\n\n\n### `TextClassification`\n\nOne of the most commonly used pipelines in `transformers` is the `TextClassificationPipeline`.\nYou may apply and train these pipelines with `superduper`.\nRead more in the [API documentation](/docs/api/ext/transformers/model#textclassificationpipeline).\n\n\n### `LLM`\n\nYou can quickly utilize LLM capabilities using the following Python function:\n\n```python\nfrom superduper.ext.transformers import LLM\nllm = LLM(model_name_or_path=\"facebook/opt-350m\")\nllm.predict(\"What are we having for dinner?\")\n```\n\nOr use a method similar to transformers’ from_pretrained, just need to supplement the identifier parameter.\n\n```python\nfrom superduper.ext.transformers import LLM\nllm = LLM.from_pretrained(\n    \"facebook/opt-350m\", \n    load_in_8bit=True, \n    device_map=\"cuda\", \n    identifier=\"llm\",\n)\n```\n\nThe model can be configured with the following parameters:\n\n- adapter_id: Add an adapter to the base model for inference.\n- model_kwargs: a dictionary; all the model_kwargs will be passed to transformers.AutoModelForCausalLM.from_pretrained. You can provide parameters such as trust_remote_code=True.\n- tokenizer_kwargs: a dictionary; all the tokenizer_kwargs will be passed to transformers.AutoTokenizer.from_pretrained.\n\n## Training\n\nFor a fully worked out training/ fine-tuning use-case refer to the [use-cases section](../use_cases/fine_tune_llm_on_database.md).\n\n### LLM fine-tuning\n\n`superduper` provides a convenient fine-tuning method based on the [trl](https://huggingface.co/docs/trl/index) framework to help you train data in the database.\n\n### Supported Features\n\n**Training Methods**:\n\n- Full fine-tuning\n- LoRA fine-tuning\n\n**Parallel Training**:\n\nParallel training is supported using Ray, with data parallelism as the default strategy. You can also pass DeepSpeed parameters to configure parallelism through [DeepSpeed configuration](https://huggingface.co/docs/transformers/main_classes/deepspeed#zero).\n\n- Multi-GPUs fine-tuning\n- Multi-nodes fine-tuning\n\n**Training on Ray**:\n\nWe can use Ray to train models. When using Ray as the compute backend, tasks will automatically run in Ray and the program will no longer be blocked.\n",
  "---\nsidebar_position: 2\n---\n\n# Custom Models\n\n`superduper` provides fully flexible support for AI models from across the \nopen-source ecosystem.\n\nCustom AI integrations may be achieved by building on to of the base `superduper.Model` class.\n\nRead more [here](../models/bring_your_own_models).",
  "# Llama.cpp\n\n## Installation\n\n```bash\npip install superduper_llama_cpp\n```\n\n## API\n\n`superduper` allows users to work with self-hosted LLM models via \"[Llama.cpp](https://github.com/ggerganov/llama.cpp)\".\n\n| Class | Description | GitHub | API-docs |\n| --- | --- | --- | --- |\n| `superduper.ext.llamacpp.LlamaCpp` | Completes a prompt with natural language (LLM) | [Code](https://github.com/superduper/superduper/blob/main/superduper/ext/llamacpp/model.py) | [Docs](/docs/api/ext/llamacpp/model#llamacpp) |\n| `superduper.ext.llamacpp.LlamaCppEmbedding` | Embeds text | [Code](https://github.com/superduper/superduper/blob/main/superduper/ext/llamacpp/model.py) | [Docs](/docs/api/ext/llamacpp/model#llamacppembedding) |",
  "# Jina\n\n## Installation\n\n```bash\npip install superduper_jina\n```\n\n## API\n\n`superduper` allows users to work with `Jina Embeddings models` through the Jina Embedding API.\n\nRead more about this [here](/docs/docs/walkthrough/ai_apis#jina).\n\n| Class | Description | GitHub | API-docs |\n| --- | --- | --- | --- |\n| `superduper.ext.jina.JinaEmbeddings` | Embeds a piece of text with a very long maximum context length | [Code](https://github.com/superduper/superduper/blob/main/superduper/ext/jina/model.py) | [Docs](/docs/api/ext/jina/model#jinaembedding) |"
]
