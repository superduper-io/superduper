{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50891727",
   "metadata": {},
   "source": [
    "# Run computations on Dask\n",
    "\n",
    "In this example, we show how to run computations on a Dask cluster, rather than in the same process as \n",
    "data is submitted from. This allows compute to be scaled horizontally, and also submitted to \n",
    "workers, which may utilize specialized hardware, including GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbd6eee",
   "metadata": {},
   "source": [
    "To do this, we need to override the default configuration. To do this, we only need specify the \n",
    "configurations which diverge from the defaults. In particular, to use a Dask cluster, we specify \n",
    "`CFG.distributed = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d5a9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo '{\"distributed\": true}' > configs.json\n",
    "!cat configs.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d840df3",
   "metadata": {},
   "source": [
    "We can now confirm, by importing the loaded configuration `CFG`, that `CFG.distribute == True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34229af6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'apis': {'providers': {},\n",
      "          'retry': {'stop_after_attempt': 2,\n",
      "                    'wait_max': 10.0,\n",
      "                    'wait_min': 4.0,\n",
      "                    'wait_multiplier': 1.0}},\n",
      " 'cdc': False,\n",
      " 'dask': {'deserializers': [],\n",
      "          'ip': 'localhost',\n",
      "          'local': True,\n",
      "          'password': '',\n",
      "          'port': 8786,\n",
      "          'serializers': [],\n",
      "          'username': ''},\n",
      " 'data_layers': {'artifact': {'cls': 'mongodb',\n",
      "                              'connection': 'pymongo',\n",
      "                              'kwargs': {'host': 'localhost',\n",
      "                                         'password': 'testmongodbpassword',\n",
      "                                         'port': 27018,\n",
      "                                         'username': 'testmongodbuser'},\n",
      "                              'name': '_filesystem:test_db'},\n",
      "                 'data_backend': {'cls': 'mongodb',\n",
      "                                  'connection': 'pymongo',\n",
      "                                  'kwargs': {'host': 'localhost',\n",
      "                                             'password': 'testmongodbpassword',\n",
      "                                             'port': 27018,\n",
      "                                             'username': 'testmongodbuser'},\n",
      "                                  'name': 'test_db'},\n",
      "                 'metadata': {'cls': 'mongodb',\n",
      "                              'connection': 'pymongo',\n",
      "                              'kwargs': {'host': 'localhost',\n",
      "                                         'password': 'testmongodbpassword',\n",
      "                                         'port': 27018,\n",
      "                                         'username': 'testmongodbuser'},\n",
      "                              'name': 'test_db'}},\n",
      " 'distributed': True,\n",
      " 'logging': {'kwargs': {},\n",
      "             'level': <LogLevel.INFO: 'INFO'>,\n",
      "             'type': <LogType.STDERR: 'STDERR'>},\n",
      " 'model_server': {'host': '127.0.0.1',\n",
      "                  'password': '',\n",
      "                  'port': 5001,\n",
      "                  'username': ''},\n",
      " 'notebook': {'ip': '0.0.0.0', 'password': '', 'port': 8888, 'token': ''},\n",
      " 'server': {'host': '127.0.0.1', 'port': 3223, 'protocol': 'http'},\n",
      " 'vector_search': {'host': 'localhost',\n",
      "                   'password': '',\n",
      "                   'port': 19530,\n",
      "                   'type': {'backfill_batch_size': 100, 'inmemory': True},\n",
      "                   'username': ''}}\n"
     ]
    }
   ],
   "source": [
    "from superduperdb import CFG\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(CFG.dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69847fe",
   "metadata": {},
   "source": [
    "Now that we've set up the environment to use a Dask cluster, we can add some data to the `Datalayer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d334809",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:faiss.loader:Loading faiss.\n",
      "INFO:faiss.loader:Successfully loaded faiss.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "INFO:distributed.http.proxy:To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "INFO:distributed.scheduler:State start\n",
      "INFO:distributed.diskutils:Found stale lock file and directory '/var/folders/y9/b74b9yj906s_wtj0rrh2lf7c0000gn/T/dask-scratch-space/scheduler-zr4pij_d', purging\n",
      "INFO:distributed.scheduler:  Scheduler at:     tcp://127.0.0.1:50788\n",
      "INFO:distributed.scheduler:  dashboard at:  http://127.0.0.1:8787/status\n",
      "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:50791'\n",
      "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:50792'\n",
      "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:50793'\n",
      "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:50794'\n",
      "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:50799', name: 0, status: init, memory: 0, processing: 0>\n",
      "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:50799\n",
      "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:50802\n",
      "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:50800', name: 2, status: init, memory: 0, processing: 0>\n",
      "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:50800\n",
      "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:50805\n",
      "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:50803', name: 1, status: init, memory: 0, processing: 0>\n",
      "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:50803\n",
      "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:50808\n",
      "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:50806', name: 3, status: init, memory: 0, processing: 0>\n",
      "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:50806\n",
      "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:50810\n",
      "INFO:distributed.scheduler:Receive client connection: Client-127d5058-306b-11ee-8888-1e00f226d551\n",
      "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:50811\n"
     ]
    }
   ],
   "source": [
    "from superduperdb.db.base.build import build_datalayer\n",
    "\n",
    "db = build_datalayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b3ae27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.db.client.drop_database('test_db')\n",
    "db.db.client.drop_database('_filesystem:test_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d364e04",
   "metadata": {},
   "source": [
    "As in the previous tutorials, we can wrap models from a range of AI frameworks to interoperate with the data set, \n",
    "as well as inserting data with, for instances, tensors of a specific data type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71a538ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Could not get an identifier from submitted function, creating one!\n",
      "INFO:faiss.loader:Loading faiss.\n",
      "INFO:faiss.loader:Successfully loaded faiss.\n",
      "INFO:root:found 0 uris\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import torch\n",
    "\n",
    "from superduperdb import superduper\n",
    "from superduperdb.container.document import Document as D\n",
    "from superduperdb.ext.torch.tensor import tensor\n",
    "from superduperdb.db.mongodb.query import Collection\n",
    "\n",
    "m = superduper(\n",
    "    torch.nn.Linear(128, 7),\n",
    "    encoder=tensor(torch.float, shape=(7,))\n",
    ")\n",
    "\n",
    "t32 = tensor(torch.float, shape=(128,))\n",
    "\n",
    "output = db.execute(\n",
    "    Collection('localcluster').insert_many(\n",
    "        [D({'x': t32(torch.randn(128))}) for _ in range(1000)], \n",
    "        encoders=(t32,)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2488d20",
   "metadata": {},
   "source": [
    "Now when we instruct the model to make predictions based on the `Datalayer`, the computations run on the Dask cluster. The `.predict` method returns a `Job` instance, which can be used to monitor the progress of the computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a653d0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:faiss.loader:Loading faiss.\n",
      "INFO:faiss.loader:Successfully loaded faiss.\n",
      "WARNING:root:model/linear/0 already exists - doing nothing\n",
      "WARNING:root:model/linear/0 already exists - doing nothing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/dodo/SuperDuperDB/superduperdb/superduperdb/ext/torch/tensor.py:26: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:212.)\n",
      "  return torch.from_numpy(array)\n",
      "\r",
      "  0%|          | 0/1000 [00:00<?, ?it/s]\r",
      "100%|##########| 1000/1000 [00:00<00:00, 19485.55it/s]\n"
     ]
    }
   ],
   "source": [
    "job = m.predict(\n",
    "    X='x',\n",
    "    db=db,\n",
    "    select=Collection('localcluster').find(),\n",
    ")\n",
    "\n",
    "job.listen()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050c0252",
   "metadata": {},
   "source": [
    "To check that the `Datalayer` has been populated with outputs, we can check the `\"_outputs\"` field of a record:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d111789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('64c9016978525ecbb9f364ee'),\n",
       " 'x': tensor([-0.4790,  1.6279,  0.6850, -1.5799, -0.4191,  1.2377, -1.3383,  1.1423,\n",
       "          0.8592,  0.3720,  0.6008,  0.5610,  0.5444,  0.5510, -1.6476,  0.5723,\n",
       "         -1.7961,  1.3233, -1.8662,  1.5770,  0.6966, -0.6335, -0.0390, -0.6632,\n",
       "         -1.1577, -0.3995,  0.1329,  0.7714,  1.7876,  0.0969, -0.6961,  0.7991,\n",
       "         -0.5688,  0.2482,  0.9115,  1.1281, -0.6572, -0.4827, -0.1045,  1.2745,\n",
       "          1.3540, -0.5822,  1.1297,  1.2904,  0.6449, -0.2306,  1.7910, -0.0457,\n",
       "         -0.2778, -0.0755, -1.3594,  1.0392, -0.3065, -0.4266,  1.0180,  0.4784,\n",
       "         -2.0998,  0.5551,  0.0905,  0.8975, -0.8218, -0.5843,  0.6310, -0.3477,\n",
       "         -0.9706,  2.0773, -0.2294,  0.5185,  1.0339, -0.5785,  0.1760,  0.6875,\n",
       "         -0.6930,  0.8977, -0.2261,  0.9070, -0.8581,  1.0713, -0.8523,  2.1395,\n",
       "         -1.5788,  1.8719, -0.2903, -0.0486, -1.5856, -1.5521,  1.1228,  0.5800,\n",
       "         -0.0910, -0.3383, -0.6915, -0.1575,  0.9333,  0.8694,  1.0298,  1.0466,\n",
       "          1.3531, -0.9910, -0.5684,  0.1287,  0.1838, -0.4840, -0.5953, -1.6719,\n",
       "         -1.6483,  0.4128,  0.7672, -0.2162,  1.1338,  0.2761,  0.0877, -1.4852,\n",
       "          1.5328,  0.5380,  0.1083,  0.9587, -0.9339, -1.2192, -2.2069,  0.0636,\n",
       "          0.1208, -1.2256,  0.8015,  0.8166, -0.1822, -2.1465, -0.3624,  1.8288]),\n",
       " '_fold': 'train',\n",
       " '_outputs': {'x': {'linear': tensor([-0.0519,  0.0409,  0.2666,  0.2636, -0.4667, -0.3497, -0.2458])}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.execute(Collection('localcluster').find_one()).unpack()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
